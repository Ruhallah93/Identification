{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qnEf299z3qfE"
   },
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J8vbzmkeI4_i"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "from datetime import timedelta\n",
    "import shutil\n",
    "from scipy.stats.mstats import gmean\n",
    "from scipy.stats import entropy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import preprocessing\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import math\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.calibration import calibration_curve\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from scipy.signal import savgol_filter\n",
    "import glob\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.metrics import pairwise_distances\n",
    "# from dtaidistance import dtw\n",
    "from sklearn.manifold import TSNE\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "r95SYeFplinj",
    "outputId": "3d09c430-7e6d-4913-aa07-154b3c5cec0f"
   },
   "outputs": [],
   "source": [
    "print(tf.__version__)\n",
    "tf.keras.backend.floatx()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9pGWzwdfOxs3"
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "import sklearn.exceptions\n",
    "warnings.filterwarnings(\"ignore\", category=sklearn.exceptions.UndefinedMetricWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J3yxSrGp3u-E"
   },
   "source": [
    "# Data Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VwtoWYsW3xOt"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "class Transformer:\n",
    "\n",
    "    def __init__(self, decision_size, decision_overlap, segments_size=90, segments_overlap=45, sampling=2):\n",
    "        self.segments_size = segments_size\n",
    "        self.segments_overlap = segments_overlap\n",
    "        self.sampling = sampling\n",
    "        self.decision_size = decision_size\n",
    "        self.decision_overlap = decision_overlap\n",
    "\n",
    "    def transfer(self, dataset, features, method):\n",
    "        print(\"segmenting data with \" + str(len(dataset)) + \" points\")\n",
    "        segments, labels = self.__segment_signal(dataset, features)\n",
    "        print(\"making \" + str(len(segments)) + \" segments\")\n",
    "        if method == \"table\":\n",
    "            segments_dataset = self.__transfer_table(segments, features)\n",
    "        elif method == \"1d\":\n",
    "            segments_dataset = self.__transfer_1d(segments, features)\n",
    "        elif method == \"2d\":\n",
    "            segments_dataset = self.__transfer_2d(segments, features)\n",
    "        elif method == \"3d_1ch\":\n",
    "          segments_dataset = self.__transfer_2d_1ch(segments, features)\n",
    "        elif method == \"3d\":\n",
    "            segments_dataset = self.__transfer_3d(segments, features)\n",
    "        elif method == \"4d\":\n",
    "            segments_dataset = self.__transfer_4d(segments, features)\n",
    "        elif method == \"rnn_2d\":\n",
    "            segments_dataset, labels =  self.__transfer_rnn_2d(segments, labels, features)\n",
    "        elif method == \"rnn_3d_1ch\":\n",
    "            segments_dataset, labels =  self.__transfer_rnn_3d_1ch(segments, labels, features)\n",
    "        return segments_dataset, labels\n",
    "\n",
    "    @staticmethod\n",
    "    def data_shape(method, n_features, segments_size, segments_overlap=None, decision_size=None):\n",
    "        if method == \"table\":\n",
    "            return (None, n_features * segments_size)\n",
    "        elif method == \"1d\":\n",
    "            return (None, 1, n_features * segments_size, 1)\n",
    "        elif method == \"2d\":\n",
    "            return (None, n_features, segments_size)\n",
    "        elif method == \"3d_1ch\":\n",
    "            return (None, n_features, segments_size, 1)\n",
    "        elif method == \"3d\":\n",
    "            return (None, 1, segments_size, n_features)\n",
    "        elif method == \"4d\":\n",
    "            return (n_features, None, 1, segments_size, 1)\n",
    "        elif method == \"rnn_2d\":\n",
    "            s_b = Transformer.get_segments_a_decision_window(segments_size,\n",
    "                                                             int(segments_size * segments_overlap),\n",
    "                                                             decision_size)\n",
    "            return (None, s_b, segments_size*n_features)\n",
    "        elif method == \"rnn_3d_1ch\":\n",
    "            s_b = Transformer.get_segments_a_decision_window(segments_size,\n",
    "                                                             int(segments_size * segments_overlap),\n",
    "                                                             decision_size)\n",
    "            return (None, s_b, 1, segments_size*n_features)\n",
    "        return ()\n",
    "\n",
    "    @staticmethod\n",
    "    def get_segments_a_decision_window(segment_size, segment_overlap_size, decision_size):\n",
    "        return int((decision_size - segment_size) / (segment_size - segment_overlap_size) + 1)\n",
    "\n",
    "    def __transfer_rnn_2d(self, segments, labels, features):\n",
    "        y_output = []\n",
    "        x_output = []\n",
    "        c = len(np.unique(labels))\n",
    "        s = Transformer.get_segments_a_decision_window(self.segments_size,\n",
    "                                                       self.segments_overlap,\n",
    "                                                       self.decision_size)\n",
    "        r = int(np.floor(s * self.decision_overlap))\n",
    "        for _id in np.unique(labels):\n",
    "          subset = segments[np.where(labels == _id)]\n",
    "          n = subset.shape[0]\n",
    "          o = int(np.floor((n - r) / (s - r)))\n",
    "          for i in range(o):\n",
    "            row = []\n",
    "            for j in range(s):\n",
    "              A = subset[i * (s-r) + j]\n",
    "              A = A.reshape(A.shape[0]*A.shape[1])\n",
    "              row.append(A)\n",
    "            y_output.append(_id)\n",
    "            x_output.append(row)\n",
    "        x_output = np.array(x_output)\n",
    "        y_output = np.array(y_output)\n",
    "        return x_output, y_output\n",
    "\n",
    "    def __transfer_rnn_3d_1ch(self, segments, labels, features):\n",
    "        # (samples, time, channels=1, rows)\n",
    "        y_output = []\n",
    "        x_output = []\n",
    "        c = len(np.unique(labels))\n",
    "        s = Transformer.get_segments_a_decision_window(self.segments_size,\n",
    "                                                       self.segments_overlap,\n",
    "                                                       self.decision_size)\n",
    "        r = int(np.floor(s * self.decision_overlap))\n",
    "        for _id in np.unique(labels):\n",
    "          subset = segments[np.where(labels == _id)]\n",
    "          n = subset.shape[0]\n",
    "          o = int(np.floor((n - r) / (s - r)))\n",
    "          for i in range(o):\n",
    "            row = []\n",
    "            for j in range(s):\n",
    "              A = subset[i * (s-r) + j]\n",
    "              A = A.reshape(A.shape[0]*A.shape[1])\n",
    "              row.append([A])\n",
    "            y_output.append(_id)\n",
    "            x_output.append(row)\n",
    "        x_output = np.array(x_output)\n",
    "        y_output = np.array(y_output)\n",
    "        return x_output, y_output\n",
    "\n",
    "    def __transfer_table(self, segments, features):\n",
    "        new_dataset = []\n",
    "        for segment in segments:\n",
    "            row = []\n",
    "            for feature_i in range(len(features)):\n",
    "                for i in range(len(segment[feature_i])):\n",
    "                    row.append(segment[feature_i][i])\n",
    "            new_dataset.append(row)\n",
    "\n",
    "        new_dataset = np.array(new_dataset)\n",
    "        return new_dataset\n",
    "\n",
    "    def __transfer_1d(self, segments, features):\n",
    "        new_dataset = []\n",
    "        for segment in segments:\n",
    "            row = []\n",
    "            for feature_i in range(len(features)):\n",
    "                for i in range(len(segment[feature_i])):\n",
    "                    row.append(segment[feature_i][i])\n",
    "            new_dataset.append([row])\n",
    "\n",
    "        new_dataset = np.array(new_dataset)\n",
    "        return np.expand_dims(new_dataset, axis=3)\n",
    "\n",
    "    def __transfer_2d(self, segments, features):\n",
    "        new_dataset = []\n",
    "        for segment in segments:\n",
    "            row = []\n",
    "            for feature_i in range(len(features)):\n",
    "                row.append(segment[feature_i])\n",
    "            new_dataset.append(row)\n",
    "\n",
    "        new_dataset = np.array(new_dataset)\n",
    "        return new_dataset\n",
    "\n",
    "    def __transfer_3d_1ch(self, segments, features):\n",
    "        new_dataset = []\n",
    "        for segment in segments:\n",
    "            row = []\n",
    "            for feature_i in range(len(features)):\n",
    "                row.append(segment[feature_i])\n",
    "            new_dataset.append(row)\n",
    "\n",
    "        new_dataset = np.array(new_dataset)\n",
    "        return np.expand_dims(new_dataset, axis=3)\n",
    "\n",
    "    def __transfer_3d(self, segments, features):\n",
    "        new_dataset = []\n",
    "        for segment in segments:\n",
    "            row = []\n",
    "            for i in range(len(segment[0])):\n",
    "                cell = []\n",
    "                for feature_i in range(len(features)):\n",
    "                    cell.append(segment[feature_i][i])\n",
    "                row.append(cell)\n",
    "            new_dataset.append([row])\n",
    "\n",
    "        new_dataset = np.array(new_dataset)\n",
    "        return new_dataset\n",
    "\n",
    "    def __transfer_4d(self, segments, features):\n",
    "        new_dataset = []\n",
    "        for feature_i in range(len(features)):\n",
    "            row = []\n",
    "            for segment in segments:\n",
    "                cell = []\n",
    "                for element in segment[feature_i]:\n",
    "                    cell.append([element])\n",
    "                row.append([cell])\n",
    "            new_dataset.append(row)\n",
    "\n",
    "        new_dataset = np.array(new_dataset)\n",
    "        return new_dataset\n",
    "\n",
    "    def __windows(self, data):\n",
    "        start = 0\n",
    "        while start < data.count():\n",
    "            yield int(start), int(start + self.segments_size)\n",
    "            start += (self.segments_size - self.segments_overlap)\n",
    "\n",
    "    def __segment_signal(self, dataset, features):\n",
    "        segments = []\n",
    "        labels = []\n",
    "        for class_i in np.unique(dataset[\"id\"]):\n",
    "            subset = dataset[dataset[\"id\"] == class_i]\n",
    "            for (start, end) in self.__windows(subset[\"id\"]):\n",
    "                feature_slices = []\n",
    "                for feature in features:\n",
    "                    feature_slices.append(subset[feature][start:end].tolist())\n",
    "                if len(feature_slices[0]) == self.segments_size:\n",
    "                    segments.append(feature_slices)\n",
    "                    labels.append(class_i)\n",
    "        return np.array(segments), np.array(labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rsqjDnvY30UY"
   },
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RisgNiya33cE"
   },
   "outputs": [],
   "source": [
    "class Dataset:\n",
    "\n",
    "    def __init__(self, db_path, sample_rate, features,\n",
    "                 window_time, window_overlap_percentage,\n",
    "                 decision_time, decision_overlap_percentage,\n",
    "                 add_noise, noise_rate,\n",
    "                 label_noise_rate,\n",
    "                 train_blocks: list, valid_blocks: list, test_blocks: list,\n",
    "                 data_length_time=-1):\n",
    "        \"\"\"\n",
    "        :param db_path:\n",
    "        :param sample_rate:\n",
    "        :param features:\n",
    "        :param window_time: in seconds\n",
    "        :param window_overlap_percentage: example: 0.75 for 75%\n",
    "        :param add_noise: True or False\n",
    "        :param noise_rate:\n",
    "        :param train_blocks:\n",
    "        :param valid_blocks:\n",
    "        :param test_blocks:\n",
    "        :param data_length_time: the amount of data from each class in seconds. -1 means whole existing data.\n",
    "        \"\"\"\n",
    "        self.db_path = db_path\n",
    "        self.features = features\n",
    "        self.sample_rate = sample_rate\n",
    "        self.window_size = window_time * sample_rate\n",
    "        self.window_overlap_size = int(self.window_size * window_overlap_percentage)\n",
    "        self.decision_size = decision_time * sample_rate\n",
    "        self.decision_overlap_size = int(self.decision_size * decision_overlap_percentage)\n",
    "        self.decision_overlap_percentage = decision_overlap_percentage\n",
    "        self.add_noise = add_noise\n",
    "        self.noise_rate = noise_rate\n",
    "        self.label_noise_rate = label_noise_rate\n",
    "        self.train_blocks = train_blocks\n",
    "        self.valid_blocks = valid_blocks\n",
    "        self.test_blocks = test_blocks\n",
    "        self.data_length_size = data_length_time * sample_rate if data_length_time != -1 else -1\n",
    "\n",
    "        # Initialization\n",
    "        self.train_dataset = pd.DataFrame()\n",
    "        self.valid_dataset = pd.DataFrame()\n",
    "        self.test_dataset = pd.DataFrame()\n",
    "        self.n_train_dataset = pd.DataFrame()\n",
    "        self.n_valid_dataset = pd.DataFrame()\n",
    "        self.n_test_dataset = pd.DataFrame()\n",
    "        self.X_train = np.array([])\n",
    "        self.y_train = np.array([])\n",
    "        self.X_valid = np.array([])\n",
    "        self.y_valid = np.array([])\n",
    "        self.X_test = np.array([])\n",
    "        self.y_test = np.array([])\n",
    "\n",
    "    def load_data(self, n_classes, method, subset=None, read_cache=True, random_selection=True):\n",
    "        segments_path = \"./\" + self.db_path.split(\"/\")[-2] + \"/\" + \\\n",
    "                        \"segments/\" + \\\n",
    "                        \"method: \" + str(method) + os.sep + \\\n",
    "                        \"noise: \" + str(self.noise_rate) + os.sep + \\\n",
    "                        \"n_classes:\" + str(n_classes) + os.sep + \\\n",
    "                        \"wl: \" + str(self.window_size) + os.sep + \\\n",
    "                        \"wo: \" + str(self.window_overlap_size) + os.sep + \\\n",
    "                        \"dl: \" + str(self.decision_size) + os.sep + \\\n",
    "                        \"do: \" + str(self.decision_overlap_size) + os.sep + \\\n",
    "                        \"train: \" + str(self.train_blocks) + os.sep + \\\n",
    "                        \"valid: \" + str(self.valid_blocks) + os.sep + \\\n",
    "                        \"test: \" + str(self.test_blocks) + os.sep\n",
    "        print(segments_path)\n",
    "        if read_cache \\\n",
    "                and os.path.exists(segments_path + 'X_train.npy') \\\n",
    "                and os.path.exists(segments_path + 'y_train.npy') \\\n",
    "                and os.path.exists(segments_path + 'X_valid.npy') \\\n",
    "                and os.path.exists(segments_path + 'y_valid.npy') \\\n",
    "                and os.path.exists(segments_path + 'X_test.npy') \\\n",
    "                and os.path.exists(segments_path + 'y_test.npy'):\n",
    "            print(\"Dataset is already\")\n",
    "            self.X_train = np.load(segments_path + 'X_train.npy')\n",
    "            self.y_train = np.load(segments_path + 'y_train.npy')\n",
    "            self.X_valid = np.load(segments_path + 'X_valid.npy')\n",
    "            self.y_valid = np.load(segments_path + 'y_valid.npy')\n",
    "            self.X_test = np.load(segments_path + 'X_test.npy')\n",
    "            self.y_test = np.load(segments_path + 'y_test.npy')\n",
    "        else:\n",
    "            self.__preprocess(n_classes, method, subset, random_selection)\n",
    "            # Save Dataset\n",
    "            if not os.path.exists(segments_path):\n",
    "                os.makedirs(segments_path)\n",
    "            np.save(segments_path + 'X_train.npy', self.X_train)\n",
    "            np.save(segments_path + 'y_train.npy', self.y_train)\n",
    "            np.save(segments_path + 'X_valid.npy', self.X_valid)\n",
    "            np.save(segments_path + 'y_valid.npy', self.y_valid)\n",
    "            np.save(segments_path + 'X_test.npy', self.X_test)\n",
    "            np.save(segments_path + 'y_test.npy', self.y_test)\n",
    "\n",
    "        def to_dic(data):\n",
    "            dic = {}\n",
    "            for i, x in enumerate(data):\n",
    "                dic[str(i)] = x\n",
    "            return dic\n",
    "\n",
    "        if len(self.X_train.shape) == 5:\n",
    "            self.X_train = to_dic(self.X_train)\n",
    "            self.X_valid = to_dic(self.X_valid)\n",
    "            self.X_test = to_dic(self.X_test)\n",
    "\n",
    "    def __preprocess(self, n_classes, method, subset=None, random_selection=True):\n",
    "        if random_selection:\n",
    "          csv_paths = np.random.choice(glob.glob(self.db_path + \"*.csv\"), n_classes, replace=False)\n",
    "        else:\n",
    "          csv_paths = glob.glob(self.db_path + \"*.csv\")[:n_classes]\n",
    "\n",
    "        self.class_names = {}\n",
    "        for i, csv_path in enumerate(csv_paths):\n",
    "            label = os.path.basename(csv_path).split('.')[0]\n",
    "            self.class_names[label] = i\n",
    "            train, valid, test = self.__read_data(csv_path, self.features, label, subset)\n",
    "            train['id'] = i\n",
    "            valid['id'] = i\n",
    "            test['id'] = i\n",
    "\n",
    "            def flip_labels(labels, num_classes, flip_percentage):\n",
    "                num_samples = labels.shape[0]\n",
    "                num_flips = int(num_samples * flip_percentage)\n",
    "\n",
    "                # Randomly select indices to flip\n",
    "                flip_indices = np.random.choice(num_samples, num_flips, replace=False)\n",
    "                # print(flip_indices)\n",
    "\n",
    "                # Generate new random labels for flipped indices\n",
    "                new_labels = np.random.randint(0, num_classes, num_flips)\n",
    "\n",
    "                # Update the labels with flipped values\n",
    "                labels[flip_indices] = new_labels\n",
    "\n",
    "                return labels\n",
    "\n",
    "            if self.label_noise_rate > 0:\n",
    "                train['id'] = flip_labels(train['id'].to_numpy(), len(csv_paths), self.label_noise_rate)\n",
    "\n",
    "\n",
    "            self.train_dataset = pd.concat([self.train_dataset, train])\n",
    "            self.valid_dataset = pd.concat([self.valid_dataset, valid])\n",
    "            self.test_dataset = pd.concat([self.test_dataset, test])\n",
    "\n",
    "        self.__standardization()\n",
    "        self.__segmentation(method=method)\n",
    "\n",
    "    def __read_data(self, path, features, label, subset=None):\n",
    "        data = pd.read_csv(path, low_memory=False)\n",
    "        if subset != None:\n",
    "          data = data.loc[ data['activity'].isin(subset) ]\n",
    "        data = data[features]\n",
    "        data = data.fillna(data.mean())\n",
    "\n",
    "        # plt.plot(data)\n",
    "        # data = data.rolling(window=5).mean()\n",
    "        # plt.plot(data)\n",
    "        # plt.show()\n",
    "\n",
    "        length = self.data_length_size if self.data_length_size != -1 else data.shape[0]\n",
    "        print('class: %5s, data size: %s, selected data size: %s' % (\n",
    "            label, str(timedelta(seconds=int(data.shape[0] / self.sample_rate))),\n",
    "            str(timedelta(seconds=int(length / self.sample_rate)))))\n",
    "        return self.__split_to_train_valid_test(data)\n",
    "\n",
    "    def __split_to_train_valid_test(self, data):\n",
    "        n_blocks = max(self.train_blocks + self.valid_blocks + self.test_blocks) + 1\n",
    "        block_length = int(len(data[:self.data_length_size]) / n_blocks)\n",
    "\n",
    "        train_data = pd.DataFrame()\n",
    "        for i in range(len(self.train_blocks)):\n",
    "            start = self.train_blocks[i] * block_length\n",
    "            end = self.train_blocks[i] * block_length + block_length - 1\n",
    "            if train_data.empty:\n",
    "                train_data = data[start:end]\n",
    "            else:\n",
    "                train_data = pd.concat([data[start:end], train_data])\n",
    "\n",
    "        valid_data = pd.DataFrame()\n",
    "        for i in range(len(self.valid_blocks)):\n",
    "            start = self.valid_blocks[i] * block_length\n",
    "            end = self.valid_blocks[i] * block_length + block_length - 1\n",
    "            if valid_data.empty:\n",
    "                valid_data = data[start:end]\n",
    "            else:\n",
    "                valid_data = pd.concat([data[start:end], valid_data])\n",
    "\n",
    "        test_data = pd.DataFrame()\n",
    "        for i in range(len(self.test_blocks)):\n",
    "            start = self.test_blocks[i] * block_length\n",
    "            end = self.test_blocks[i] * block_length + block_length - 1\n",
    "            if test_data.empty:\n",
    "                test_data = data[start:end]\n",
    "            else:\n",
    "                test_data = pd.concat([data[start:end], test_data])\n",
    "\n",
    "        if self.add_noise:\n",
    "            test_data = self.__add_noise_to_data(test_data)\n",
    "\n",
    "        return train_data, valid_data, test_data\n",
    "\n",
    "    def __add_noise_to_data(self, x):\n",
    "        x_power = x ** 2\n",
    "        sig_avg_watts = np.mean(x_power)\n",
    "        sig_avg_db = 10 * np.log10(sig_avg_watts)\n",
    "        noise_avg_db = sig_avg_db - self.target_snr_db\n",
    "        noise_avg_watts = 10 ** (noise_avg_db / 10)\n",
    "        mean_noise = 0\n",
    "        noise_volts = np.random.normal(mean_noise, np.sqrt(noise_avg_watts), size=x.shape)\n",
    "        return x + noise_volts\n",
    "\n",
    "    def __standardization(self):\n",
    "        scaler = preprocessing.StandardScaler()\n",
    "        scaler = scaler.fit(self.train_dataset.iloc[:, :-1])\n",
    "        n_train_dataset = scaler.transform(self.train_dataset.iloc[:, :-1])\n",
    "        n_valid_dataset = scaler.transform(self.valid_dataset.iloc[:, :-1])\n",
    "        n_test_dataset = scaler.transform(self.test_dataset.iloc[:, :-1])\n",
    "\n",
    "        self.n_train_dataset = pd.DataFrame(n_train_dataset, columns=self.features)\n",
    "        self.n_valid_dataset = pd.DataFrame(n_valid_dataset, columns=self.features)\n",
    "        self.n_test_dataset = pd.DataFrame(n_test_dataset, columns=self.features)\n",
    "        self.n_train_dataset['id'] = self.train_dataset.iloc[:, -1].tolist()\n",
    "        self.n_valid_dataset['id'] = self.valid_dataset.iloc[:, -1].tolist()\n",
    "        self.n_test_dataset['id'] = self.test_dataset.iloc[:, -1].tolist()\n",
    "\n",
    "    def __segmentation(self, method):\n",
    "        transformer = Transformer(segments_size=self.window_size,\n",
    "                                  segments_overlap=self.window_overlap_size,\n",
    "                                  decision_size=self.decision_size,\n",
    "                                  decision_overlap=self.decision_overlap_percentage)\n",
    "        self.X_train, self.y_train = transformer.transfer(self.n_train_dataset, self.features, method=method)\n",
    "        self.X_valid, self.y_valid = transformer.transfer(self.n_valid_dataset, self.features, method=method)\n",
    "        self.X_test, self.y_test = transformer.transfer(self.n_test_dataset, self.features, method=method)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XJE0iXH6a0nk"
   },
   "source": [
    "# Dataset (DB2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fzpHdqt3a1-O"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "import random\n",
    "from random import sample\n",
    "from datetime import timedelta\n",
    "import pandas as pd\n",
    "\n",
    "class Dataset2(Dataset):\n",
    "\n",
    "    def __init__(self, db_path, sample_rate, features,\n",
    "                 window_time, window_overlap_percentage,\n",
    "                 decision_time, decision_overlap_percentage,\n",
    "                 add_noise, noise_rate,\n",
    "                 label_noise_rate,\n",
    "                 train_blocks: list, valid_blocks: list, test_blocks: list,\n",
    "                 data_length_time=-1):\n",
    "        self.w = window_time\n",
    "        self.r = self.w - int(self.w * window_overlap_percentage)\n",
    "        self.db_path = db_path\n",
    "        self.X_train = np.array([])\n",
    "        self.y_train = np.array([])\n",
    "        self.X_valid = np.array([])\n",
    "        self.y_valid = np.array([])\n",
    "        self.X_test = np.array([])\n",
    "        self.y_test = np.array([])\n",
    "\n",
    "        self.window_size = window_time * sample_rate\n",
    "        self.window_overlap_size = int(self.window_size * window_overlap_percentage)\n",
    "        self.decision_size = decision_time * sample_rate\n",
    "        self.decision_overlap_size = int(self.decision_size * decision_overlap_percentage)\n",
    "        self.decision_overlap_percentage = decision_overlap_percentage\n",
    "        self.add_noise = add_noise\n",
    "        self.noise_rate = noise_rate\n",
    "        self.label_noise_rate = label_noise_rate\n",
    "        self.train_blocks = train_blocks\n",
    "        self.valid_blocks = valid_blocks\n",
    "        self.test_blocks = test_blocks\n",
    "        self.data_length_size = data_length_time * sample_rate if data_length_time != -1 else -1\n",
    "        self.features = features\n",
    "\n",
    "    def load_data(self, n_classes, method):\n",
    "        segments_path = \"./\" + self.db_path.split(\"/\")[-2] + \"/\" + \\\n",
    "                        \"segments/\" + \\\n",
    "                        \"method: \" + str(method) + os.sep + \\\n",
    "                        \"noise: \" + str(self.noise_rate) + os.sep + \\\n",
    "                        \"n_classes:\" + str(n_classes) + os.sep + \\\n",
    "                        \"wl: \" + str(self.window_size) + os.sep + \\\n",
    "                        \"wo: \" + str(self.window_overlap_size) + os.sep + \\\n",
    "                        \"dl: \" + str(self.decision_size) + os.sep + \\\n",
    "                        \"do: \" + str(self.decision_overlap_size) + os.sep + \\\n",
    "                        \"train: \" + str(self.train_blocks) + os.sep + \\\n",
    "                        \"valid: \" + str(self.valid_blocks) + os.sep + \\\n",
    "                        \"test: \" + str(self.test_blocks) + os.sep\n",
    "        if os.path.exists(segments_path + 'X_train.npy') \\\n",
    "                and os.path.exists(segments_path + 'y_train.npy') \\\n",
    "                and os.path.exists(segments_path + 'X_valid.npy') \\\n",
    "                and os.path.exists(segments_path + 'y_valid.npy') \\\n",
    "                and os.path.exists(segments_path + 'X_test.npy') \\\n",
    "                and os.path.exists(segments_path + 'y_test.npy'):\n",
    "            print(\"Dataset is already\")\n",
    "            self.X_train = np.load(segments_path + 'X_train.npy')\n",
    "            self.y_train = np.load(segments_path + 'y_train.npy')\n",
    "            self.X_valid = np.load(segments_path + 'X_valid.npy')\n",
    "            self.y_valid = np.load(segments_path + 'y_valid.npy')\n",
    "            self.X_test = np.load(segments_path + 'X_test.npy')\n",
    "            self.y_test = np.load(segments_path + 'y_test.npy')\n",
    "        else:\n",
    "            y_signals = self.load_y(self.db_path+\"train/y_train.txt\")\n",
    "\n",
    "            instances = len(y_signals)\n",
    "            total = np.array([i for i in range(instances)])\n",
    "            valids = sample([i for i in range(instances)], int(instances * 0.3))\n",
    "            trains = np.delete(total, valids).tolist()\n",
    "\n",
    "            self.X_train, self.y_train = self.load_X(self.db_path + \"train/Inertial Signals/\", y_signals, samples=trains, w=self.w, r=self.r)\n",
    "            self.X_valid, self.y_valid = self.load_X(self.db_path + \"train/Inertial Signals/\", y_signals, samples=valids, w=self.w, r=self.r)\n",
    "\n",
    "            y_signals = self.load_y(self.db_path + \"test/y_test.txt\")\n",
    "            self.X_test, self.y_test = self.load_X(self.db_path + \"test/Inertial Signals/\", y_signals, w=self.w, r=self.r)\n",
    "\n",
    "            # Save Dataset\n",
    "            if not os.path.exists(segments_path):\n",
    "                os.makedirs(segments_path)\n",
    "            np.save(segments_path + 'X_train.npy', self.X_train)\n",
    "            np.save(segments_path + 'y_train.npy', self.y_train)\n",
    "            np.save(segments_path + 'X_valid.npy', self.X_valid)\n",
    "            np.save(segments_path + 'y_valid.npy', self.y_valid)\n",
    "            np.save(segments_path + 'X_test.npy', self.X_test)\n",
    "            np.save(segments_path + 'y_test.npy', self.y_test)\n",
    "\n",
    "\n",
    "    def windows(self, data, w, r):\n",
    "        start = 0\n",
    "        while start < len(data):\n",
    "            yield int(start), int(start + w)\n",
    "            start += (w - r)\n",
    "\n",
    "    def segment_signal(self, X, y, w, r):\n",
    "        X_new = []\n",
    "        y_new = []\n",
    "        for (start, end) in self.windows(X, w, r):\n",
    "            segment = X[start:end]\n",
    "            if len(segment) == w:\n",
    "                X_new.append(segment)\n",
    "                y_new.append(y-1)\n",
    "        return np.array(X_new), np.array(y_new)\n",
    "\n",
    "    def load_y(self, y_path):\n",
    "        file = open(y_path, 'r')\n",
    "        # Read dataset from disk, dealing with text file's syntax\n",
    "        y_ = np.array(\n",
    "            [elem for elem in [\n",
    "                row.replace('  ', ' ').strip().split(' ') for row in file\n",
    "            ]],\n",
    "            dtype=np.int32\n",
    "        )\n",
    "        file.close()\n",
    "        return np.array(y_, dtype=np.int32)\n",
    "\n",
    "    def load_X(self, path, y, samples=None, w=32, r=8):\n",
    "        X_signals = []\n",
    "        y_signals = []\n",
    "        files = os.listdir(path)\n",
    "        files.sort(key=str.lower)\n",
    "        files = glob.glob(path + \"*.txt\")\n",
    "\n",
    "        for fileName in files:\n",
    "            print(fileName)\n",
    "            file = open(fileName, 'r')\n",
    "            X_container = []\n",
    "            y_container = []\n",
    "\n",
    "            for i, row in enumerate(file):\n",
    "                if samples != None and i not in samples:\n",
    "                    continue\n",
    "                row = row.strip().replace('  ', ' ')\n",
    "                X_i, y_i = self.segment_signal(row.strip().split(' '), y=y[i][0], w=w, r=r)\n",
    "                X_container.append(np.array(X_i, dtype=np.float32))\n",
    "                y_container.append(np.array(y_i, dtype=np.float32))\n",
    "\n",
    "            file.close()\n",
    "\n",
    "            X_container = np.array(X_container)\n",
    "            X_container = X_container.reshape(-1, w)\n",
    "            X_signals.append(X_container)\n",
    "\n",
    "            y_container = np.array(y_container)\n",
    "            y_container = y_container.reshape(-1, 1)\n",
    "            y_signals.append(y_container)\n",
    "\n",
    "        X_signals = np.transpose(np.array(X_signals), (1, 2, 0))\n",
    "        X_signals = X_signals.reshape(-1 , 1, w, len(self.features))\n",
    "        return np.array(X_signals), y_signals[0].reshape(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6s0M8G76fIFQ"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1NRgkdXOYoky"
   },
   "source": [
    "# Dataset (HAR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Wi0A1QPTYrsU"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "import random\n",
    "from random import sample\n",
    "from datetime import timedelta\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "class Dataset3(Dataset):\n",
    "\n",
    "    def __init__(self, db_path, sample_rate, features,\n",
    "                 window_time, window_overlap_percentage,\n",
    "                 decision_time, decision_overlap_percentage,\n",
    "                 add_noise, noise_rate,\n",
    "                 label_noise_rate,\n",
    "                 train_blocks: list, valid_blocks: list, test_blocks: list,\n",
    "                 data_length_time=-1):\n",
    "        self.w = window_time\n",
    "        self.r = self.w - int(self.w * window_overlap_percentage)\n",
    "        self.db_path = db_path\n",
    "        self.X_train = np.array([])\n",
    "        self.y_train = np.array([])\n",
    "        self.X_valid = np.array([])\n",
    "        self.y_valid = np.array([])\n",
    "        self.X_test = np.array([])\n",
    "        self.y_test = np.array([])\n",
    "\n",
    "        self.window_size = window_time * sample_rate\n",
    "        self.window_overlap_size = int(self.window_size * window_overlap_percentage)\n",
    "        self.decision_size = decision_time * sample_rate\n",
    "        self.decision_overlap_size = int(self.decision_size * decision_overlap_percentage)\n",
    "        self.decision_overlap_percentage = decision_overlap_percentage\n",
    "        self.add_noise = add_noise\n",
    "        self.noise_rate = noise_rate\n",
    "        self.label_noise_rate = label_noise_rate\n",
    "        self.train_blocks = train_blocks\n",
    "        self.valid_blocks = valid_blocks\n",
    "        self.test_blocks = test_blocks\n",
    "        self.data_length_size = data_length_time * sample_rate if data_length_time != -1 else -1\n",
    "        self.features = features\n",
    "\n",
    "    def load_data(self, n_classes, method):\n",
    "        segments_path = \"./\" + self.db_path.split(\"/\")[-2] + \"/\" + \\\n",
    "                        \"segments/\" + \\\n",
    "                        \"method: \" + str(method) + os.sep + \\\n",
    "                        \"noise: \" + str(self.noise_rate) + os.sep + \\\n",
    "                        \"n_classes:\" + str(n_classes) + os.sep + \\\n",
    "                        \"wl: \" + str(self.window_size) + os.sep + \\\n",
    "                        \"wo: \" + str(self.window_overlap_size) + os.sep + \\\n",
    "                        \"dl: \" + str(self.decision_size) + os.sep + \\\n",
    "                        \"do: \" + str(self.decision_overlap_size) + os.sep + \\\n",
    "                        \"train: \" + str(self.train_blocks) + os.sep + \\\n",
    "                        \"valid: \" + str(self.valid_blocks) + os.sep + \\\n",
    "                        \"test: \" + str(self.test_blocks) + os.sep\n",
    "        if os.path.exists(segments_path + 'X_train.npy') \\\n",
    "                and os.path.exists(segments_path + 'y_train.npy') \\\n",
    "                and os.path.exists(segments_path + 'X_valid.npy') \\\n",
    "                and os.path.exists(segments_path + 'y_valid.npy') \\\n",
    "                and os.path.exists(segments_path + 'X_test.npy') \\\n",
    "                and os.path.exists(segments_path + 'y_test.npy'):\n",
    "            print(\"Dataset is already\")\n",
    "            self.X_train = np.load(segments_path + 'X_train.npy')\n",
    "            self.y_train = np.load(segments_path + 'y_train.npy')\n",
    "            self.X_valid = np.load(segments_path + 'X_valid.npy')\n",
    "            self.y_valid = np.load(segments_path + 'y_valid.npy')\n",
    "            self.X_test = np.load(segments_path + 'X_test.npy')\n",
    "            self.y_test = np.load(segments_path + 'y_test.npy')\n",
    "        else:\n",
    "            y_signals = self.load_y(self.db_path+\"train/y_train.txt\")\n",
    "            self.X_train, self.y_train = self.load_X(self.db_path + \"train/Inertial Signals/\", y_signals, w=self.w, r=self.r)\n",
    "\n",
    "            y_signals = self.load_y(self.db_path + \"test/y_test.txt\")\n",
    "            self.X_test, self.y_test = self.load_X(self.db_path + \"test/Inertial Signals/\", y_signals, w=self.w, r=self.r)\n",
    "\n",
    "            X = np.concatenate((self.X_train, self.X_test), axis=0)\n",
    "            y = np.concatenate((self.y_train, self.y_test), axis=0)\n",
    "\n",
    "            X_temp, self.X_valid, y_temp, self.y_valid = train_test_split(X, y, test_size=0.2, stratify=y, random_state=0)\n",
    "            self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(X_temp, y_temp, test_size=0.625, stratify=y_temp, random_state=0)\n",
    "\n",
    "            # self.__standardization()\n",
    "\n",
    "            # Save Dataset\n",
    "            if not os.path.exists(segments_path):\n",
    "                os.makedirs(segments_path)\n",
    "            np.save(segments_path + 'X_train.npy', self.X_train)\n",
    "            np.save(segments_path + 'y_train.npy', self.y_train)\n",
    "            np.save(segments_path + 'X_valid.npy', self.X_valid)\n",
    "            np.save(segments_path + 'y_valid.npy', self.y_valid)\n",
    "            np.save(segments_path + 'X_test.npy', self.X_test)\n",
    "            np.save(segments_path + 'y_test.npy', self.y_test)\n",
    "\n",
    "    def __standardization(self):\n",
    "        scaler = preprocessing.StandardScaler()\n",
    "        scaler = scaler.fit(self.X_train)\n",
    "        self.X_train = scaler.transform(self.X_train)\n",
    "        self.X_valid = scaler.transform(self.X_valid)\n",
    "        self.X_test = scaler.transform(self.X_test)\n",
    "\n",
    "    def windows(self, data, w, r):\n",
    "        start = 0\n",
    "        while start < len(data):\n",
    "            yield int(start), int(start + w)\n",
    "            start += (w - r)\n",
    "\n",
    "    def segment_signal(self, X, y, w, r):\n",
    "        X_new = []\n",
    "        y_new = []\n",
    "        for (start, end) in self.windows(X, w, r):\n",
    "            segment = X[start:end]\n",
    "            if len(segment) == w:\n",
    "                X_new.append(segment)\n",
    "                y_new.append(y)\n",
    "        return np.array(X_new), np.array(y_new)\n",
    "\n",
    "    def load_y(self, y_path):\n",
    "        file = open(y_path, 'r')\n",
    "        # Read dataset from disk, dealing with text file's syntax\n",
    "        y_ = np.array(\n",
    "            [elem for elem in [\n",
    "                row.replace('  ', ' ').strip().split(' ') for row in file\n",
    "            ]],\n",
    "            dtype=np.int32\n",
    "        )\n",
    "        file.close()\n",
    "        return np.array(y_, dtype=np.int32)\n",
    "\n",
    "    def load_X(self, path, y, samples=None, w=32, r=8):\n",
    "        X_signals = []\n",
    "        y_signals = []\n",
    "        files = os.listdir(path)\n",
    "        files.sort(key=str.lower)\n",
    "        files = glob.glob(path + \"*.txt\")\n",
    "\n",
    "        for fileName in files:\n",
    "            print(fileName)\n",
    "            file = open(fileName, 'r')\n",
    "            X_container = []\n",
    "            y_container = []\n",
    "\n",
    "            for i, row in enumerate(file):\n",
    "                if samples != None and i not in samples:\n",
    "                    continue\n",
    "                row = row.strip().replace('  ', ' ')\n",
    "                X_i, y_i = self.segment_signal(row.strip().split(' '), y=y[i][0], w=w, r=r)\n",
    "                X_container.append(np.array(X_i, dtype=np.float32))\n",
    "                y_container.append(np.array(y_i, dtype=np.float32))\n",
    "\n",
    "            file.close()\n",
    "\n",
    "            X_container = np.array(X_container)\n",
    "            X_container = X_container.reshape(-1, w)\n",
    "            X_signals.append(X_container)\n",
    "\n",
    "            y_container = np.array(y_container)\n",
    "            y_container = y_container.reshape(-1, 1)\n",
    "            y_signals.append(y_container)\n",
    "\n",
    "        X_signals = np.transpose(np.array(X_signals), (1, 2, 0))\n",
    "        X_signals = X_signals.reshape(-1 , 1, w, len(self.features))\n",
    "        return np.array(X_signals), y_signals[0].reshape(-1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fTAGp_3s364X"
   },
   "source": [
    "# Scoring Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cNfOUKPl4Crf"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def pesl(y, q):\n",
    "    y = tf.cast(y, tf.float32)\n",
    "    c = y.get_shape()[1]\n",
    "\n",
    "    ST = tf.math.subtract(q, tf.reduce_sum(tf.where(y == 1, q, y), axis=1)[:, None])\n",
    "    ST = tf.where(ST < 0, tf.constant(0, dtype=tf.float32), ST)\n",
    "    payoff = tf.reduce_sum(tf.math.ceil(ST), axis=1)\n",
    "    M = (c - 1) / (c ** 2)\n",
    "    payoff = tf.where(payoff > 0, tf.constant(M, dtype=tf.float32), payoff)\n",
    "    return tf.math.reduce_mean(tf.math.reduce_mean(tf.math.square(tf.math.subtract(y, q)), axis=1) + payoff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bnkKOUHKk6GY"
   },
   "outputs": [],
   "source": [
    "def esl(y,q):\n",
    "  return tf.math.reduce_mean(tf.math.reduce_mean(tf.math.square(tf.math.subtract(y, q)), axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "l8OsafN9S5aP"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def pll(y, q):\n",
    "    y = tf.cast(y, tf.float32)\n",
    "    c = y.get_shape()[1]\n",
    "\n",
    "    ST = tf.math.subtract(q, tf.reduce_sum(tf.where(y == 1, q, y), axis=1)[:, None])\n",
    "    ST = tf.where(ST < 0, tf.constant(0, dtype=tf.float32), ST)\n",
    "    payoff = tf.reduce_sum(tf.math.ceil(ST), axis=1)\n",
    "    M = math.log(1/c)\n",
    "    payoff = tf.where(payoff > 0, tf.constant(M, dtype=tf.float32), payoff)\n",
    "    log_loss = tf.keras.losses.categorical_crossentropy(y,q)\n",
    "    p_log_loss = tf.cast(log_loss, tf.float32) - payoff\n",
    "    return tf.math.reduce_mean(p_log_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KmmPDBs5k4ca"
   },
   "outputs": [],
   "source": [
    "def ll(y,q):\n",
    "  return tf.math.reduce_mean(tf.keras.losses.categorical_crossentropy(y,q))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9KP0pEQM4KNT"
   },
   "source": [
    "# Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2prG7BJT4Kt5"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "def f1_metric(y_true, y_pred):\n",
    "    # Define the true positives, false positives and false negatives\n",
    "    tp = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    fp = K.sum(K.round(K.clip(y_pred - y_true, 0, 1)))\n",
    "    fn = K.sum(K.round(K.clip(y_true - y_pred, 0, 1)))\n",
    "\n",
    "    # Calculate the precision and recall\n",
    "    precision = tp / (tp + fp + K.epsilon())\n",
    "    recall = tp / (tp + fn + K.epsilon())\n",
    "\n",
    "    # Calculate the F1 score\n",
    "    f1_score = 2 * ((precision * recall) / (precision + recall + K.epsilon()))\n",
    "\n",
    "    return f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_jM3l01B4R-Y"
   },
   "source": [
    "# MODELS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "byq18z6w4K5r"
   },
   "outputs": [],
   "source": [
    "class Classifier():\n",
    "    def __init__(self, loss_function):\n",
    "        self.loss_function = loss_function\n",
    "\n",
    "    def get_loss_function(self):\n",
    "        return self.loss_function\n",
    "\n",
    "    def get_loss_function_name(self):\n",
    "        return self.loss_function if type(self.loss_function) == str else self.loss_function.__name__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "95NTs4jw4aN8"
   },
   "outputs": [],
   "source": [
    "# Convolutinal Neural Network\n",
    "class CNN_L(Classifier):\n",
    "    def __init__(self, classes, n_features,\n",
    "                 segments_size, segments_overlap,\n",
    "                 decision_size, decision_overlap,\n",
    "                 loss_function, loss_metric, last_activation,\n",
    "                 lr=0.0001, beta_1=0.5, training=False):\n",
    "        super().__init__(loss_metric)\n",
    "        self.classes = classes\n",
    "        self.n_features = n_features\n",
    "        self.segments_size = segments_size\n",
    "        self.input_shape = self.get_input_shape()\n",
    "        self.segments_overlap = segments_overlap\n",
    "        self.decision_size = decision_size\n",
    "        self.decision_overlap = decision_overlap\n",
    "        self.initializer = tf.keras.initializers.GlorotNormal(seed=np.random.seed(1337))\n",
    "        self.last_activation = last_activation\n",
    "        self.training = training\n",
    "\n",
    "        # Build and compile the model\n",
    "        self.model = self.build_model_l()\n",
    "\n",
    "        optimizer = tf.keras.optimizers.Nadam(lr, beta_1)\n",
    "        self.model.compile(loss=loss_function,\n",
    "                           optimizer=optimizer,\n",
    "                           metrics=['accuracy',\n",
    "                                    #tf.keras.metrics.Precision(),\n",
    "                                    #tf.keras.metrics.Recall(),\n",
    "                                    # f1_metric,\n",
    "                                    # loss_metric\n",
    "                                    ])\n",
    "\n",
    "    def get_input_shape(self):\n",
    "        data_shape = Transformer.data_shape(method=self.get_data_arrangement(), n_features=self.n_features,\n",
    "                                            segments_size=self.segments_size)\n",
    "        return data_shape[-3], data_shape[-2], data_shape[-1]\n",
    "\n",
    "    @staticmethod\n",
    "    def get_data_arrangement():\n",
    "        return \"3d\"\n",
    "\n",
    "    def count_params(self):\n",
    "        return self.model.count_params()\n",
    "\n",
    "    def get_dropout(self, input_tensor, p=0.2, mc=False):\n",
    "        if mc:\n",
    "            return tf.keras.layers.Dropout(p)(input_tensor, training=True)\n",
    "        else:\n",
    "            return tf.keras.layers.Dropout(p)(input_tensor)\n",
    "\n",
    "    def build_model_l(self):\n",
    "        input_ = tf.keras.layers.Input(shape=self.input_shape)\n",
    "\n",
    "        x = tf.keras.layers.Conv2D(64, kernel_size=(1, 3), strides=1, padding=\"same\", activation='relu', kernel_initializer=self.initializer, bias_initializer=self.initializer)(input_)\n",
    "        x = tf.keras.layers.LeakyReLU(alpha=0.2)(x)\n",
    "        x = self.get_dropout(x, 0.2, mc=self.training)\n",
    "        x = tf.keras.layers.Conv2D(32, kernel_size=(1, 3), strides=1, padding=\"same\", activation='relu', kernel_initializer=self.initializer, bias_initializer=self.initializer)(x)\n",
    "        x = tf.keras.layers.BatchNormalization(momentum=0.9)(x)\n",
    "        x = tf.keras.layers.LeakyReLU(alpha=0.2)(x)\n",
    "        x = self.get_dropout(x, 0.2, mc=self.training)\n",
    "        x = tf.keras.layers.Conv2D(16, kernel_size=(1, 3), strides=1, padding=\"same\", activation='relu', kernel_initializer=self.initializer, bias_initializer=self.initializer)(x)\n",
    "        x = tf.keras.layers.BatchNormalization(momentum=0.9)(x)\n",
    "        x = tf.keras.layers.LeakyReLU(alpha=0.2)(x)\n",
    "        x = self.get_dropout(x, 0.2, mc=self.training)\n",
    "\n",
    "        dense = tf.keras.layers.Flatten()(x)\n",
    "        dense = tf.keras.layers.Dense(1024, activation='relu', kernel_initializer=self.initializer, bias_initializer=self.initializer)(dense)\n",
    "        dense = tf.keras.layers.LeakyReLU(alpha=0.2)(dense)\n",
    "        dense = self.get_dropout(dense, 0.2, mc=self.training)\n",
    "        dense = tf.keras.layers.Dense(self.classes, activation=self.last_activation)(dense)\n",
    "\n",
    "        model = tf.keras.models.Model(inputs=input_, outputs=[dense])\n",
    "        return model\n",
    "\n",
    "    def train(self, epochs, X_train, y_train, X_valid, y_valid, callback, monitor_mode, batch_size=128):\n",
    "        # Change the labels from categorical to one-hot encoding\n",
    "        y_train_onehot = np.asarray(pd.get_dummies(y_train), dtype=np.int8)\n",
    "        y_valid_onehot = np.asarray(pd.get_dummies(y_valid), dtype=np.int8)\n",
    "\n",
    "        history = self.model.fit(X_train, y_train_onehot,\n",
    "                                 validation_data=(X_valid, y_valid_onehot),\n",
    "                                 batch_size=batch_size,\n",
    "                                 epochs=epochs,\n",
    "                                 verbose=1,\n",
    "                                 shuffle=True,\n",
    "                                 callbacks=callback)\n",
    "\n",
    "        return history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LJsGcm6jBgAL"
   },
   "source": [
    "# Callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gKIV9uFSCnKA"
   },
   "outputs": [],
   "source": [
    "class ModelAnalyser:\n",
    "\n",
    "    def __init__(self, segment_size, segment_overlap, decision_size, decision_overlap, X, y):\n",
    "        \"\"\"\n",
    "        :param segment_size: for calculating number of segments in a decision window.\n",
    "        :param segment_overlap: for calculating number of segments in a decision window.\n",
    "        :param decision_size:\n",
    "        :param decision_overlap:\n",
    "        :param X:\n",
    "        :param y:\n",
    "        \"\"\"\n",
    "        self.segments_a_decision_window = get_segments_a_decision_window(segment_size, segment_overlap, decision_size)\n",
    "        self.decision_overlap = decision_overlap\n",
    "        self.X = X\n",
    "        self.y_real = y\n",
    "        self.y_real_onehot = np.asarray(pd.get_dummies(y), dtype=np.int8)\n",
    "\n",
    "    def measurement(self, y_prediction, value_coef, monitor):\n",
    "        \"\"\"\n",
    "        :param y_real: expected labels\n",
    "        :param y_prediction: the outcomes of core\n",
    "        :param monitor: '#METHOD_#MEASURE', #METHOD={'mv','ms'}, #MEASURE={'loss','accuracy','precision','recall','f1'}\n",
    "        \"\"\"\n",
    "\n",
    "        monitor_method = monitor.split('_')[0]\n",
    "        monitor_measure = monitor.split('_')[1]\n",
    "\n",
    "        if monitor_method == 'ms':\n",
    "            y_pred_labels, y_dw_real = AveragingProbabilities(y_truth=self.y_real,\n",
    "                                                              y_prediction=y_prediction,\n",
    "                                                              s=self.segments_a_decision_window,\n",
    "                                                              r=self.decision_overlap,\n",
    "                                                              weights=value_coef)\n",
    "\n",
    "            y_pred_one_hot = np.zeros_like(y_pred_labels)\n",
    "            y_pred_one_hot[np.arange(len(y_pred_one_hot)), y_pred_labels.argmax(1)] = 1\n",
    "            if monitor_measure == 'mse':\n",
    "                loss_fn = tf.keras.losses.MeanSquaredError()\n",
    "                return loss_fn(y_dw_real, y_pred_labels).numpy()\n",
    "            elif monitor_measure == 'pesl':\n",
    "                return pesl(y_dw_real, y_pred_labels).numpy()\n",
    "            elif monitor_measure == 'pll':\n",
    "                return pll(y_dw_real, y_pred_labels).numpy()\n",
    "            elif monitor_measure == 'accuracy':\n",
    "                return accuracy_score(y_dw_real, y_pred_one_hot)\n",
    "            elif monitor_measure == 'precision':\n",
    "                return precision_score(y_dw_real, y_pred_one_hot, average='macro')\n",
    "            elif monitor_measure == 'recall':\n",
    "                return recall_score(y_dw_real, y_pred_one_hot, average='macro')\n",
    "            elif monitor_measure == 'f1':\n",
    "                return f1_score(y_dw_real, y_pred_one_hot, average='macro')\n",
    "\n",
    "\n",
    "        if monitor_method == 'mv':\n",
    "            y_pred_labels, y_dw_real = MajorityVote(y_truth=self.y_real,\n",
    "                                                    y_prediction=y_prediction,\n",
    "                                                    s=self.segments_a_decision_window,\n",
    "                                                    r=self.decision_overlap,\n",
    "                                                    weights=value_coef)\n",
    "\n",
    "            y_pred_one_hot = np.zeros_like(y_pred_labels)\n",
    "            y_pred_one_hot[np.arange(len(y_pred_one_hot)), y_pred_labels.argmax(1)] = 1\n",
    "            if monitor_measure == 'mse':\n",
    "                loss_fn = tf.keras.losses.MeanSquaredError()\n",
    "                return loss_fn(y_dw_real, y_pred_labels).numpy()\n",
    "            elif monitor_measure == 'pesl':\n",
    "                return pesl(y_dw_real, y_pred_labels).numpy()\n",
    "            elif monitor_measure == 'pll':\n",
    "                return pll(y_dw_real, y_pred_labels).numpy()\n",
    "            elif monitor_measure == 'accuracy':\n",
    "                return accuracy_score(y_dw_real, y_pred_one_hot)\n",
    "            elif monitor_measure == 'precision':\n",
    "                return precision_score(y_dw_real, y_pred_one_hot, average='macro')\n",
    "            elif monitor_measure == 'recall':\n",
    "                return recall_score(y_dw_real, y_pred_one_hot, average='macro')\n",
    "            elif monitor_measure == 'f1':\n",
    "                return f1_score(y_dw_real, y_pred_one_hot, average='macro')\n",
    "\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "urkt3wz-Pe_q"
   },
   "source": [
    "# UN Logging Callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7-KYM_a1-LhW"
   },
   "outputs": [],
   "source": [
    "class UNLoggingCallback(tf.keras.callbacks.Callback):\n",
    "\n",
    "    def __init__(self, metric: ModelAnalyser, mode, monitor, checkpoint_dir):\n",
    "        super(tf.keras.callbacks.Callback, self).__init__()\n",
    "        self.metric = metric\n",
    "        self.monitor = monitor\n",
    "        self.monitor_method = monitor.split('_')[0]\n",
    "        self.monitor_measure = monitor.split('_')[1]\n",
    "        self.checkpoint_dir = checkpoint_dir\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        y_prediction = self.model.predict(self.metric.X)\n",
    "\n",
    "        value_coef = None\n",
    "        current = self.metric.measurement(y_prediction, value_coef, monitor=self.monitor)\n",
    "        mse = self.metric.measurement(y_prediction, value_coef, monitor=\"ms_mse\")\n",
    "        accuracy = self.metric.measurement(y_prediction, value_coef, monitor=\"ms_accuracy\")\n",
    "        f1 = self.metric.measurement(y_prediction, value_coef, monitor=\"ms_f1\")\n",
    "        logs[self.monitor] = current\n",
    "        logs[self.monitor_method  + \"_uqm_mse\"] = mse\n",
    "        logs[self.monitor_method  + \"_uqm_acc\"] = accuracy\n",
    "        logs[self.monitor_method  + \"_uqm_f1\"] = f1\n",
    "\n",
    "        print('UN epoch %d: [%s: %f] [%s: %f] [%s: %f] [%s: %f]' % (epoch+1, self.monitor, current, \"mse\", mse, \"accuracy\", accuracy, \"f1\", f1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JIPRTjzc4eLw"
   },
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ltka_UrJ4hcy"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score, recall_score, f1_score, precision_score\n",
    "\n",
    "def analysis_model(loss_fn, loss_name, y_pred, y_real_raw, segment_size, segment_overlap, decision_size, decision_overlap, value_coef=None):\n",
    "    result = {'Core': {}, 'MV': {}, 'MS': {}}\n",
    "    result['Core']['pesl'] = pesl(np.asarray(pd.get_dummies(y_real_raw), dtype=np.int8),\n",
    "                                  np.asarray(y_pred, dtype=np.float64)).numpy()\n",
    "    # result['Core']['mse'] = tf.keras.metrics.mean_squared_error(np.asarray(pd.get_dummies(y_real_raw), dtype=np.int8),\n",
    "                                  # np.asarray(y_pred, dtype=np.float64)).numpy().mean()\n",
    "    result['Core']['esl'] = esl(np.asarray(pd.get_dummies(y_real_raw), dtype=np.int8),\n",
    "                                  np.asarray(y_pred, dtype=np.float64)).numpy()\n",
    "    result['Core']['ll'] = ll(np.asarray(pd.get_dummies(y_real_raw), dtype=np.int8),\n",
    "                                  np.asarray(y_pred, dtype=np.float64)).numpy()\n",
    "    y_pred_arg = np.argmax(y_pred, axis=1)\n",
    "    result['Core']['accuracy'] = accuracy_score(y_real_raw, y_pred_arg)\n",
    "    result['Core']['precision'] = precision_score(y_real_raw, y_pred_arg, average='macro')\n",
    "    result['Core']['recall'] = recall_score(y_real_raw, y_pred_arg, average='macro')\n",
    "    result['Core']['f1'] = f1_score(y_real_raw, y_pred_arg, average='macro')\n",
    "\n",
    "    segments_a_decision_window = get_segments_a_decision_window(segment_size, segment_overlap, decision_size)\n",
    "\n",
    "    # Maximum Score\n",
    "    y_pred_labels, y_real = AveragingProbabilities(y_truth=y_real_raw, y_prediction=y_pred,\n",
    "                                                   s=segments_a_decision_window,\n",
    "                                                   r=decision_overlap,\n",
    "                                                   weights=value_coef)\n",
    "\n",
    "    result['MS']['pesl'] = pesl(y_real, y_pred_labels).numpy()\n",
    "    result['MS']['esl'] = esl(y_real, y_pred_labels).numpy()\n",
    "    result['MS']['ll'] = ll(y_real, y_pred_labels).numpy()\n",
    "    temp = y_pred_labels.copy()\n",
    "    y_pred_labels = np.zeros_like(temp)\n",
    "    y_pred_labels[np.arange(len(temp)), temp.argmax(1)] = 1\n",
    "    result['MS']['accuracy'] = accuracy_score(y_real, y_pred_labels)\n",
    "    result['MS']['precision'] = precision_score(y_real, y_pred_labels, average='macro')\n",
    "    result['MS']['recall'] = recall_score(y_real, y_pred_labels, average='macro')\n",
    "    result['MS']['f1'] = f1_score(y_real, y_pred_labels, average='macro')\n",
    "\n",
    "    # Majority Voting\n",
    "    y_pred_labels, y_real = MajorityVote(y_truth=y_real_raw, y_prediction=y_pred,\n",
    "                                         s=segments_a_decision_window,\n",
    "                                         r=decision_overlap,\n",
    "                                         weights=value_coef)\n",
    "\n",
    "    result['MV']['pesl'] = pesl(y_real, y_pred_labels).numpy()\n",
    "    result['MV']['esl'] = esl(y_real, y_pred_labels).numpy()\n",
    "    result['MV']['ll'] = ll(y_real, y_pred_labels).numpy()\n",
    "\n",
    "    temp = y_pred_labels.copy()\n",
    "    y_pred_labels = np.zeros_like(temp)\n",
    "    y_pred_labels[np.arange(len(temp)), temp.argmax(1)] = 1\n",
    "    result['MV']['accuracy'] = accuracy_score(y_real, y_pred_labels)\n",
    "    result['MV']['precision'] = precision_score(y_real, y_pred_labels, average='macro')\n",
    "    result['MV']['recall'] = recall_score(y_real, y_pred_labels, average='macro')\n",
    "    result['MV']['f1'] = f1_score(y_real, y_pred_labels, average='macro')\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DZDd-uxHdzY-"
   },
   "outputs": [],
   "source": [
    "def get_segments_a_decision_window(segment_size, segment_overlap, decision_size):\n",
    "    segment_overlap_size = segment_size * segment_overlap\n",
    "    return int((decision_size - segment_size) / (segment_size - segment_overlap_size) + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JJWrMn0Nd5gm"
   },
   "outputs": [],
   "source": [
    "def AveragingProbabilities(y_truth, y_prediction, s, r, weights=None):\n",
    "    t = False\n",
    "    if weights is None:\n",
    "      t = True\n",
    "      weights = np.ones(len(y_truth))\n",
    "    df = []\n",
    "    y_dw_truth = []\n",
    "    c = y_prediction.shape[1]\n",
    "    r = int(np.floor(s * r))\n",
    "    for _id in np.unique(y_truth):\n",
    "        subset = y_prediction[np.where(y_truth == _id)]\n",
    "        weights_sub = weights[np.where(y_truth == _id)]\n",
    "        n = subset.shape[0]\n",
    "        o = int(np.floor((n - r) / (s - r)))\n",
    "        for i in range(o):\n",
    "            row = np.zeros(c)\n",
    "            denominator = 0\n",
    "            cefs = weights_sub[(i * s) : (i * s) + s]\n",
    "            for j in range(s):\n",
    "                row += np.multiply(cefs[j], subset[(i * s) + j])\n",
    "                denominator += cefs[j]\n",
    "            df.append(row / denominator)\n",
    "            y_dw_truth.append(_id)\n",
    "    df = np.array(df)\n",
    "    y_dw_truth = np.asarray(pd.get_dummies(y_dw_truth), dtype=np.int8)\n",
    "    return df, y_dw_truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0OxO2RAVd6-D"
   },
   "outputs": [],
   "source": [
    "def MajorityVote(y_truth, y_prediction, s, r, weights=None):\n",
    "    if weights is None:\n",
    "      weights = np.ones(len(y_truth))\n",
    "\n",
    "    df = []\n",
    "    y_dw_truth = []\n",
    "    c = y_prediction.shape[1]\n",
    "    r = int(np.floor(s * r))\n",
    "    # Make prior prediction to one-hot\n",
    "    y_categorical_pred = np.zeros_like(y_prediction)\n",
    "    y_categorical_pred[np.arange(len(y_prediction)), y_prediction.argmax(1)] = 1\n",
    "\n",
    "    for _id in np.unique(y_truth):\n",
    "        subset = y_categorical_pred[np.where(y_truth == _id)]\n",
    "        weights_sub = weights[np.where(y_truth == _id)]\n",
    "        n = subset.shape[0]\n",
    "        o = int(np.floor((n - r) / (s - r)))\n",
    "        for i in range(o):\n",
    "            row = np.zeros(c)\n",
    "            denominator = 0\n",
    "            cefs = weights_sub[(i * s) : (i * s) + s]\n",
    "            for j in range(s):\n",
    "                row += np.multiply(cefs[j], subset[(i * s) + j])\n",
    "                denominator += cefs[j]\n",
    "            df.append(row / denominator)\n",
    "            y_dw_truth.append(_id)\n",
    "    df = np.array(df)\n",
    "    y_dw_truth = np.asarray(pd.get_dummies(y_dw_truth), dtype=np.int8)\n",
    "    return df, y_dw_truth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vDN5yNvo4jZ0"
   },
   "source": [
    "# Training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nVjqWBgQ4mBR"
   },
   "outputs": [],
   "source": [
    "def train_model(dataset: Dataset, classifier: Classifier, epochs, batch_size,\n",
    "                log_dir, monitor_metric, monitor_mode, restore_best=True):\n",
    "    callbacks = []\n",
    "    tbCallBack = tf.keras.callbacks.TensorBoard(log_dir=log_dir)\n",
    "    callbacks.append(tbCallBack)\n",
    "\n",
    "    checkpoint_path = log_dir + \"/weights-improvement-{epoch:02d}-{val_accuracy:.2f}.keras\"\n",
    "    checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "\n",
    "    if restore_best:\n",
    "        metric = ModelAnalyser(segment_size=classifier.segments_size,\n",
    "                               segment_overlap=classifier.segments_overlap,\n",
    "                               decision_size=classifier.decision_size,\n",
    "                               decision_overlap=classifier.decision_overlap,\n",
    "                               X=dataset.X_valid,\n",
    "                               y=dataset.y_valid)\n",
    "        restoring_best_valid = UNLoggingCallback(metric=metric,\n",
    "                                                 monitor=monitor_metric,\n",
    "                                                 mode=monitor_mode,\n",
    "                                                 checkpoint_dir=checkpoint_dir)\n",
    "        callbacks.append(restoring_best_valid)\n",
    "\n",
    "        checkpoint = ModelCheckpoint(checkpoint_path, monitor=monitor_metric, verbose=1, save_best_only=True,\n",
    "                                     mode=monitor_mode)\n",
    "        callbacks.append(checkpoint)\n",
    "\n",
    "    history = classifier.train(epochs=epochs,\n",
    "                               X_train=dataset.X_train,\n",
    "                               y_train=dataset.y_train,\n",
    "                               X_valid=dataset.X_valid,\n",
    "                               y_valid=dataset.y_valid,\n",
    "                               batch_size=batch_size,\n",
    "                               callback=callbacks,\n",
    "                               monitor_mode=monitor_mode)\n",
    "\n",
    "\n",
    "    y_test_pred = classifier.model.predict(dataset.X_test)\n",
    "    result_test = final_evaluation(classifier, y_test_pred, dataset.y_test)\n",
    "    print_results(result_test)\n",
    "\n",
    "    # Add Validation Monitor Metric to Results\n",
    "    result_test = add_validation_monitor_metric_to_results(history, monitor_mode, monitor_metric, result_test)\n",
    "\n",
    "    result = {}\n",
    "\n",
    "    result['Inner Classifier'] = result_test\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3PUQLt_277Js"
   },
   "outputs": [],
   "source": [
    "def final_evaluation(classifier, y_test_pred, y_real_raw):\n",
    "\n",
    "    result_test = analysis_model(loss_fn=classifier.get_loss_function(),\n",
    "                                 loss_name=classifier.get_loss_function_name(),\n",
    "                                 y_pred=y_test_pred,\n",
    "                                 value_coef=None,\n",
    "                                 y_real_raw=y_real_raw,\n",
    "                                 segment_size=classifier.segments_size,\n",
    "                                 segment_overlap=classifier.segments_overlap,\n",
    "                                 decision_size=classifier.decision_size,\n",
    "                                 decision_overlap=classifier.decision_overlap)\n",
    "\n",
    "    return result_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yIAK2SUuANIg"
   },
   "outputs": [],
   "source": [
    "def print_results(result_test):\n",
    "    print('OR Results: Classifier:%s Test(Core):%5.4f Test(MS)/acc:%5.4f Test(MS)/f1:%5.4f' % (\n",
    "    type(classifier).__name__, result_test['Core']['accuracy'], result_test['MS']['accuracy'], result_test['MS']['f1']))\n",
    "\n",
    "    print('OR Results: Classifier:%s Test(Core):%5.4f Test(MV)/acc:%5.4f Test(MV)/f1:%5.4f' % (\n",
    "    type(classifier).__name__, result_test['Core']['accuracy'], result_test['MV']['accuracy'], result_test['MV']['f1']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sw5v9E9CBBWI"
   },
   "outputs": [],
   "source": [
    "def add_validation_monitor_metric_to_results(history, monitor_mode, monitor_metric, result_test):\n",
    "    result_test['validation_metric'] = {}\n",
    "\n",
    "    if monitor_mode == 'min':\n",
    "        result_test['validation_metric'][monitor_metric] = np.min(history.history[monitor_metric])\n",
    "    if monitor_mode == 'max':\n",
    "        result_test['validation_metric'][monitor_metric] = np.min(history.history[monitor_metric])\n",
    "\n",
    "    return result_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z9CZOYRH4y8U"
   },
   "source": [
    "# Spliting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lX06Xc3o4zQw"
   },
   "outputs": [],
   "source": [
    "def h_block_analyzer(db_path, sample_rate, features, n_classes, noise_rate, label_noise_rate, segments_time,\n",
    "                     segments_overlap, decision_time, decision_overlap, classifier, epochs, batch_size,\n",
    "                     data_length_time, monitor_metric, monitor_mode, restore_best, repetitions, n_h_block,\n",
    "                     n_train_h_block, n_valid_h_block, n_test_h_block, h_moving_step=1, is_sota=False):\n",
    "    \"\"\"\n",
    "    :param db_path: the address of dataset directory\n",
    "    :param sample_rate: the sampling rate of signals\n",
    "    :param features: the signals of original data\n",
    "    :param n_classes: the number of classes\n",
    "    :param noise_rate: the rate of noises injected to test data\n",
    "    :param segments_time: the length of each segment in seconds.\n",
    "    :param segments_overlap: the overlap of each segment\n",
    "    :param classifier: the inner classifier\n",
    "    :param calibrator_title: the calibration model\n",
    "    :param epochs: the number of training epochs\n",
    "    :param batch_size: the number of segments in each batch\n",
    "    :param data_length_time: the length of data of each class. -1 = whole.\n",
    "    :param n_h_block: the number of all hv blocks\n",
    "    :param n_train_h_block: the number of hv blocks to train network\n",
    "    :param n_valid_h_block: the number of hv blocks to validate network\n",
    "    :param n_test_h_block: the number of hv blocks to test network\n",
    "    :param h_moving_step: the number of movement of test and validation blocks in each iteration\n",
    "    :return:\n",
    "    \"\"\"\n",
    "\n",
    "    add_noise = noise_rate < 100\n",
    "\n",
    "    # Create hv blocks\n",
    "    data_blocks = [i for i in range(n_h_block)]\n",
    "    if not is_sota:\n",
    "      np.random.shuffle(data_blocks)\n",
    "    n_vt = (n_valid_h_block + n_test_h_block)\n",
    "    n_iteration = int((n_h_block - n_vt) / h_moving_step)\n",
    "    date_str = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "\n",
    "    if is_sota:\n",
    "      n_iteration = 0\n",
    "\n",
    "    final_statistics = {}\n",
    "    for i in range(n_iteration + 1):\n",
    "        i = 1\n",
    "        i_statistics = {}\n",
    "        for j in range(repetitions):\n",
    "            print('Iteration:  %d/%d +++++++++++++++++++++++++++++++++++++++++++++++++' % (i + 1, n_iteration + 1))\n",
    "            print('Repetition: %d/%d -------------------------------------------------' % (j + 1, repetitions))\n",
    "\n",
    "            training_container = data_blocks[0:i] + data_blocks[i + n_vt:n_h_block]\n",
    "            train_blocks = training_container[:n_train_h_block]\n",
    "            valid_blocks = data_blocks[i: i + n_valid_h_block]\n",
    "            test_blocks = data_blocks[i + n_valid_h_block: i + n_vt]\n",
    "            dataset = Dataset(db_path,\n",
    "                              sample_rate,\n",
    "                              features=features,\n",
    "                              window_time=segments_time,\n",
    "                              window_overlap_percentage=segments_overlap,\n",
    "                              decision_time=decision_time,\n",
    "                              decision_overlap_percentage=decision_overlap,\n",
    "                              add_noise=add_noise,\n",
    "                              noise_rate=noise_rate,\n",
    "                              label_noise_rate=label_noise_rate,\n",
    "                              train_blocks=train_blocks,\n",
    "                              valid_blocks=valid_blocks,\n",
    "                              test_blocks=test_blocks,\n",
    "                              data_length_time=data_length_time)\n",
    "            dataset.load_data(n_classes=n_classes, method=classifier.get_data_arrangement())\n",
    "            print(dataset.X_train.shape)\n",
    "            print(dataset.y_train.shape)\n",
    "\n",
    "            logdir = os.path.join(\"logs/checkpointss/\", date_str + \"[\" + str(i) + \"]/[\" + str(j) + \"]\")\n",
    "            if not os.path.exists(logdir):\n",
    "                os.makedirs(logdir)\n",
    "\n",
    "            result = train_model(dataset=dataset,\n",
    "                                                   classifier=classifier,\n",
    "                                                   epochs=epochs,\n",
    "                                                   batch_size=batch_size,\n",
    "                                                   restore_best=restore_best,\n",
    "                                                   monitor_metric=monitor_metric,\n",
    "                                                   monitor_mode=monitor_mode,\n",
    "                                                   log_dir=logdir)\n",
    "\n",
    "            for prediction_method in result.keys():\n",
    "                if not prediction_method in i_statistics:\n",
    "                  i_statistics[prediction_method] = {}\n",
    "                for key in result[prediction_method].keys():\n",
    "                    if not key in i_statistics[prediction_method]:\n",
    "                        i_statistics[prediction_method][key] = {}\n",
    "                    for inner_key in result[prediction_method][key].keys():\n",
    "                        if not inner_key in i_statistics[prediction_method][key]:\n",
    "                            i_statistics[prediction_method][key][inner_key] = []\n",
    "                        i_statistics[prediction_method][key][inner_key].append(result[prediction_method][key][inner_key])\n",
    "            shutil.rmtree(logdir, ignore_errors=True)\n",
    "\n",
    "        for prediction_method in i_statistics.keys():\n",
    "            if not prediction_method in final_statistics:\n",
    "                final_statistics[prediction_method] = {}\n",
    "            for key in i_statistics[prediction_method].keys():\n",
    "                if not key in final_statistics[prediction_method]:\n",
    "                    final_statistics[prediction_method][key] = {}\n",
    "                for inner_key in i_statistics[prediction_method][key].keys():\n",
    "                    if not inner_key in final_statistics[prediction_method][key]:\n",
    "                      final_statistics[prediction_method][key][inner_key] = []\n",
    "\n",
    "                    if monitor_mode == \"max\":\n",
    "                      selected_index = np.nanargmax(i_statistics[prediction_method]['validation_metric'][monitor_metric])\n",
    "                    else:\n",
    "                      selected_index = np.nanargmin(i_statistics[prediction_method]['validation_metric'][monitor_metric])\n",
    "                    final_statistics[prediction_method][key][inner_key].append(i_statistics[prediction_method][key][inner_key][selected_index])\n",
    "\n",
    "    # print(\"final_statistics\", final_statistics)\n",
    "    return final_statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0TtSkS-W454c"
   },
   "source": [
    "# Save Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t1mJTX0Y46OV"
   },
   "outputs": [],
   "source": [
    "def save_result(log_dir, data: dict):\n",
    "    if not os.path.exists(log_dir):\n",
    "        os.makedirs(log_dir)\n",
    "\n",
    "    # Save to file\n",
    "    with open(log_dir + 'statistics.txt', 'a') as f:\n",
    "        f.write('\\n==========***==========\\n')\n",
    "        f.write(str(data))\n",
    "        f.write('\\n')\n",
    "\n",
    "    csv_file = log_dir + 'statistics.csv'\n",
    "    file_exists = os.path.isfile(csv_file)\n",
    "    try:\n",
    "        with open(csv_file, 'a') as csvfile:\n",
    "            writer = csv.writer(csvfile)\n",
    "            if not file_exists:\n",
    "                writer.writerow(data.keys())\n",
    "            writer.writerow(data.values())\n",
    "            csvfile.close()\n",
    "    except IOError:\n",
    "        print(\"I/O error\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Whr7yWQL48po"
   },
   "outputs": [],
   "source": [
    "def cumulative_average(numbers):\n",
    "    avg = []\n",
    "    for i in range(len(numbers)):\n",
    "        check = 0\n",
    "        l = i + 1\n",
    "        for j in range(i+1):\n",
    "            check += numbers[j]\n",
    "        avg.append(check/l)\n",
    "    return avg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oo0q8aqH4-4w"
   },
   "source": [
    "# Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZzgHB2zC5Bk_"
   },
   "outputs": [],
   "source": [
    "problems = {'DriverIdentification':{},\n",
    "            'ConfLongDemo_JSI':{},\n",
    "            'Healthy_Older_People':{},\n",
    "            'RSSI':{},\n",
    "            'User_Identification_From_Walking':{},\n",
    "            'WISDM':{},\n",
    "            'Dataset#2':{},\n",
    "            'UCI-HAR-Dataset':{},\n",
    "           }\n",
    "\n",
    "problems['ConfLongDemo_JSI']['dataset'] = './datasets/ConfLongDemo_JSI/'\n",
    "problems['ConfLongDemo_JSI']['n_classes'] = 5\n",
    "problems['ConfLongDemo_JSI']['features'] = [\"x\", \"y\", \"z\"]\n",
    "problems['ConfLongDemo_JSI']['sample_rate'] = 30\n",
    "problems['ConfLongDemo_JSI']['data_length_time'] = -1\n",
    "problems['ConfLongDemo_JSI']['n_h_block'] = 10\n",
    "problems['ConfLongDemo_JSI']['n_train_h_block'] = 5\n",
    "problems['ConfLongDemo_JSI']['n_valid_h_block'] = 2\n",
    "problems['ConfLongDemo_JSI']['n_test_h_block'] = 3\n",
    "problems['ConfLongDemo_JSI']['h_moving_step'] = 1\n",
    "problems['ConfLongDemo_JSI']['segments_overlaps'] = 0.75\n",
    "problems['ConfLongDemo_JSI']['MLP/segments_time'] = 3\n",
    "problems['ConfLongDemo_JSI']['CNN_L/segments_time'] = 3\n",
    "problems['ConfLongDemo_JSI']['decision_overlaps'] = 0\n",
    "problems['ConfLongDemo_JSI']['MLP/decision_times'] = 2 * 60\n",
    "problems['ConfLongDemo_JSI']['CNN_L/decision_times'] = 2 * 60\n",
    "\n",
    "problems['Healthy_Older_People']['dataset'] = './datasets/Healthy_Older_People/'\n",
    "problems['Healthy_Older_People']['n_classes'] = 12\n",
    "problems['Healthy_Older_People']['features'] = [\"X\", \"Y\", \"Z\"]\n",
    "problems['Healthy_Older_People']['sample_rate'] = 1\n",
    "problems['Healthy_Older_People']['data_length_time'] = -1\n",
    "problems['Healthy_Older_People']['n_h_block'] = 10\n",
    "problems['Healthy_Older_People']['n_train_h_block'] = 5\n",
    "problems['Healthy_Older_People']['n_valid_h_block'] = 2\n",
    "problems['Healthy_Older_People']['n_test_h_block'] = 3\n",
    "problems['Healthy_Older_People']['h_moving_step'] = 1\n",
    "problems['Healthy_Older_People']['segments_overlaps'] = 0.75\n",
    "problems['Healthy_Older_People']['MLP/segments_time'] = 8\n",
    "problems['Healthy_Older_People']['CNN_L/segments_time'] = 8\n",
    "problems['Healthy_Older_People']['decision_overlaps'] = 0\n",
    "problems['Healthy_Older_People']['MLP/decision_times'] = 3 * 60\n",
    "problems['Healthy_Older_People']['CNN_L/decision_times'] = 3 * 60\n",
    "\n",
    "problems['User_Identification_From_Walking']['dataset'] = './datasets/User_Identification_From_Walking/'\n",
    "problems['User_Identification_From_Walking']['n_classes'] = 13\n",
    "problems['User_Identification_From_Walking']['features'] = [' x acceleration', ' y acceleration', ' z acceleration']\n",
    "problems['User_Identification_From_Walking']['sample_rate'] = 32\n",
    "problems['User_Identification_From_Walking']['data_length_time'] = -1\n",
    "problems['User_Identification_From_Walking']['n_h_block'] = 10\n",
    "problems['User_Identification_From_Walking']['n_train_h_block'] = 5\n",
    "problems['User_Identification_From_Walking']['n_valid_h_block'] = 2\n",
    "problems['User_Identification_From_Walking']['n_test_h_block'] = 3\n",
    "problems['User_Identification_From_Walking']['h_moving_step'] = 1\n",
    "problems['User_Identification_From_Walking']['segments_overlaps'] = 0.75\n",
    "problems['User_Identification_From_Walking']['MLP/segments_time'] = 3\n",
    "problems['User_Identification_From_Walking']['CNN_L/segments_time'] = 3\n",
    "problems['User_Identification_From_Walking']['decision_overlaps'] = 0\n",
    "problems['User_Identification_From_Walking']['MLP/decision_times'] = 22\n",
    "problems['User_Identification_From_Walking']['CNN_L/decision_times'] = 22\n",
    "\n",
    "problems['RSSI']['dataset'] = './datasets/RSSI/'\n",
    "problems['RSSI']['n_classes'] = 12\n",
    "problems['RSSI']['features'] = ['rssiOne', 'rssiTwo']\n",
    "problems['RSSI']['sample_rate'] = 1\n",
    "problems['RSSI']['data_length_time'] = -1\n",
    "problems['RSSI']['n_h_block'] = 10\n",
    "problems['RSSI']['n_train_h_block'] = 5\n",
    "problems['RSSI']['n_valid_h_block'] = 2\n",
    "problems['RSSI']['n_test_h_block'] = 3\n",
    "problems['RSSI']['h_moving_step'] = 1\n",
    "problems['RSSI']['segments_overlaps'] = 0.75\n",
    "problems['RSSI']['MLP/segments_time'] = 4\n",
    "problems['RSSI']['CNN_L/segments_time'] = 4\n",
    "problems['RSSI']['decision_overlaps'] = 0\n",
    "problems['RSSI']['MLP/decision_times'] = 1 * 60\n",
    "problems['RSSI']['CNN_L/decision_times'] = 1 * 60\n",
    "\n",
    "problems['WISDM']['dataset'] = './datasets/WISDM/'\n",
    "problems['WISDM']['n_classes'] = 10\n",
    "problems['WISDM']['features'] = ['X-accel', 'Y-accel', 'Z-accel']\n",
    "problems['WISDM']['sample_rate'] = 20\n",
    "problems['WISDM']['data_length_time'] = -1\n",
    "problems['WISDM']['n_h_block'] = 10\n",
    "problems['WISDM']['n_train_h_block'] = 5\n",
    "problems['WISDM']['n_valid_h_block'] = 2\n",
    "problems['WISDM']['n_test_h_block'] = 3\n",
    "problems['WISDM']['h_moving_step'] = 1\n",
    "problems['WISDM']['segments_overlaps'] = 0.75\n",
    "problems['WISDM']['MLP/segments_time'] = 4\n",
    "problems['WISDM']['CNN_L/segments_time'] = 4\n",
    "problems['WISDM']['decision_overlaps'] = 0\n",
    "problems['WISDM']['MLP/decision_times'] = 5 * 60\n",
    "problems['WISDM']['CNN_L/decision_times'] = 5 * 60\n",
    "\n",
    "problems['DriverIdentification']['dataset'] = './datasets/DriverIdentification/'\n",
    "problems['DriverIdentification']['n_classes'] = 10\n",
    "problems['DriverIdentification']['features'] = ['x-accelerometer', 'y-accelerometer', 'z-accelerometer', 'x-gyroscope', 'y-gyroscope', 'z-gyroscope']\n",
    "problems['DriverIdentification']['sample_rate'] = 2\n",
    "problems['DriverIdentification']['data_length_time'] = -1\n",
    "problems['DriverIdentification']['n_h_block'] = 10\n",
    "problems['DriverIdentification']['n_train_h_block'] = 5\n",
    "problems['DriverIdentification']['n_valid_h_block'] = 2\n",
    "problems['DriverIdentification']['n_test_h_block'] = 3\n",
    "problems['DriverIdentification']['h_moving_step'] = 1\n",
    "problems['DriverIdentification']['segments_overlaps'] = 0.75\n",
    "problems['DriverIdentification']['MLP/segments_time'] = 4\n",
    "problems['DriverIdentification']['CNN_L/segments_time'] = 4\n",
    "problems['DriverIdentification']['decision_overlaps'] = 0\n",
    "problems['DriverIdentification']['MLP/decision_times'] = 5 * 60\n",
    "problems['DriverIdentification']['CNN_L/decision_times'] = 5 * 60\n",
    "\n",
    "problems['Dataset#2']['dataset'] = './datasets/Dataset#2/'\n",
    "problems['Dataset#2']['n_classes'] = 20\n",
    "problems['Dataset#2']['features'] = ['x1','y1','z1','x2','y2','z2']\n",
    "problems['Dataset#2']['sample_rate'] = 1\n",
    "problems['Dataset#2']['data_length_time'] = -1\n",
    "problems['Dataset#2']['n_h_block'] = 2\n",
    "problems['Dataset#2']['n_train_h_block'] = 1\n",
    "problems['Dataset#2']['n_valid_h_block'] = 1\n",
    "problems['Dataset#2']['n_test_h_block'] = 1\n",
    "problems['Dataset#2']['h_moving_step'] = 1\n",
    "problems['Dataset#2']['segments_overlaps'] = 0.75\n",
    "problems['Dataset#2']['CNN_L/segments_time'] = 32\n",
    "problems['Dataset#2']['decision_overlaps'] = 0\n",
    "problems['Dataset#2']['CNN_L/decision_times'] = 128\n",
    "\n",
    "problems['UCI-HAR-Dataset']['dataset'] = './datasets/UCI-HAR-Dataset/'\n",
    "problems['UCI-HAR-Dataset']['n_classes'] = 30\n",
    "problems['UCI-HAR-Dataset']['features'] = ['x1','y1','z1','x2','y2','z2','x3','y3','z3']\n",
    "problems['UCI-HAR-Dataset']['sample_rate'] = 1\n",
    "problems['UCI-HAR-Dataset']['data_length_time'] = -1\n",
    "problems['UCI-HAR-Dataset']['n_h_block'] = 2\n",
    "problems['UCI-HAR-Dataset']['n_train_h_block'] = 1\n",
    "problems['UCI-HAR-Dataset']['n_valid_h_block'] = 1\n",
    "problems['UCI-HAR-Dataset']['n_test_h_block'] = 1\n",
    "problems['UCI-HAR-Dataset']['h_moving_step'] = 1\n",
    "problems['UCI-HAR-Dataset']['segments_overlaps'] = 0.75\n",
    "problems['UCI-HAR-Dataset']['CNN_L/segments_time'] = 32\n",
    "problems['UCI-HAR-Dataset']['decision_overlaps'] = 0\n",
    "problems['UCI-HAR-Dataset']['CNN_L/decision_times'] = 128"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LFsacgLZ5EG4"
   },
   "source": [
    "# Main Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O8nfqyGV5GzQ"
   },
   "outputs": [],
   "source": [
    "train_config = [\n",
    "    {\n",
    "        \"loss_function\": tf.keras.losses.CategoricalCrossentropy(from_logits=False),\n",
    "        \"loss_metric\": pesl,\n",
    "        \"monitor_metric\": \"ms_pesl\",\n",
    "        \"monitor_mode\": \"min\"\n",
    "    },\n",
    "    {\n",
    "        \"loss_function\": tf.keras.losses.CategoricalCrossentropy(from_logits=False),\n",
    "        \"loss_metric\": pll,\n",
    "        \"monitor_metric\": \"ms_pll\",\n",
    "        \"monitor_mode\": \"min\"\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9EcygQRU5SK_"
   },
   "source": [
    "# Models Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WRP2mSzx5U2I",
    "outputId": "ee1a3fb3-4b94-4700-8487-27fd4c32eb2c"
   },
   "outputs": [],
   "source": [
    "noise_rate = 100\n",
    "label_noise_rate = 0.0\n",
    "epochs = 50\n",
    "batch_size = 32\n",
    "models = ['CNN_L']\n",
    "repetitions = 10\n",
    "restore_best = True\n",
    "lr = 0.0001\n",
    "beta_1 = 0.5\n",
    "\n",
    "# 'ConfLongDemo_JSI', 'Healthy_Older_People', 'User_Identification_From_Walking', 'DriverIdentification'\n",
    "datasets = ['DriverIdentification']\n",
    "for model in models:\n",
    "        for problem in datasets:\n",
    "            log_dir = f\"./logs/sota/{problem}/\"\n",
    "            dataset = problems[problem]['dataset']\n",
    "            n_classes = problems[problem]['n_classes']\n",
    "            features = problems[problem]['features']\n",
    "            sample_rate = problems[problem]['sample_rate']\n",
    "            data_length_time = problems[problem]['data_length_time']\n",
    "            n_h_block = problems[problem]['n_h_block']\n",
    "            n_train_h_block = problems[problem]['n_train_h_block']\n",
    "            n_valid_h_block = problems[problem]['n_valid_h_block']\n",
    "            n_test_h_block = problems[problem]['n_test_h_block']\n",
    "            h_moving_step = problems[problem]['h_moving_step']\n",
    "            # train config index\n",
    "            for tci in [0]:\n",
    "                last_activation = \"softmax\"\n",
    "                segments_time = problems[problem][model + '/segments_time']\n",
    "                segments_overlap = problems[problem]['segments_overlaps']\n",
    "                decision_time = problems[problem][model + '/decision_times']\n",
    "                decision_overlap = problems[problem]['decision_overlaps']\n",
    "                classifier = eval(model)(classes=n_classes,\n",
    "                                         n_features=len(features),\n",
    "                                         segments_size=int(segments_time * sample_rate),\n",
    "                                         segments_overlap=segments_overlap,\n",
    "                                         decision_size=int(decision_time * sample_rate),\n",
    "                                         decision_overlap=decision_overlap,\n",
    "                                         loss_metric=train_config[tci][\"loss_metric\"],\n",
    "                                         loss_function=train_config[tci][\"loss_function\"],\n",
    "                                         lr=lr,\n",
    "                                         beta_1=beta_1,\n",
    "                                         last_activation=last_activation,\n",
    "                                         training=False)\n",
    "\n",
    "                # cross-validation\n",
    "                start = datetime.now()\n",
    "                all_statistics = h_block_analyzer(db_path=dataset,\n",
    "                                                                     sample_rate=sample_rate,\n",
    "                                                                     features=features,\n",
    "                                                                     n_classes=n_classes,\n",
    "                                                                     noise_rate=noise_rate,\n",
    "                                                                     label_noise_rate=label_noise_rate,\n",
    "                                                                     segments_time=segments_time,\n",
    "                                                                     segments_overlap=segments_overlap,\n",
    "                                                                     decision_time=decision_time,\n",
    "                                                                     decision_overlap=decision_overlap,\n",
    "                                                                     classifier=classifier,\n",
    "                                                                     epochs=epochs,\n",
    "                                                                     batch_size=batch_size,\n",
    "                                                                     data_length_time=data_length_time,\n",
    "                                                                     n_h_block=n_h_block,\n",
    "                                                                     n_train_h_block=n_train_h_block,\n",
    "                                                                     n_valid_h_block=n_valid_h_block,\n",
    "                                                                     n_test_h_block=n_test_h_block,\n",
    "                                                                     h_moving_step=h_moving_step,\n",
    "                                                                     monitor_metric=train_config[tci][\"monitor_metric\"],\n",
    "                                                                     monitor_mode=train_config[tci][\"monitor_mode\"],\n",
    "                                                                     restore_best=restore_best,\n",
    "                                                                     repetitions=repetitions)\n",
    "                end = datetime.now()\n",
    "                running_time = end - start\n",
    "                for prediction_method in all_statistics.keys():\n",
    "                    # Summarizing the results of cross-validation\n",
    "                    data = {}\n",
    "                    data['dataset'] = dataset\n",
    "                    data['prediction_method'] = prediction_method\n",
    "                    data['class'] = str(n_classes)\n",
    "                    loss_metric = train_config[tci][\"loss_metric\"]\n",
    "                    data['loss_metric'] = loss_metric if type(loss_metric) == str else loss_metric.__name__\n",
    "                    loss_function = train_config[tci][\"loss_function\"]\n",
    "                    data['loss_function'] = loss_function if type(loss_function) == str else str(loss_function)\n",
    "                    data['lr'] = lr\n",
    "                    data['beta_1'] = beta_1\n",
    "                    data['monitor_metric'] = train_config[tci][\"monitor_metric\"]\n",
    "                    data['monitor_mode'] = train_config[tci][\"monitor_mode\"]\n",
    "                    data['restore_best'] = str(restore_best)\n",
    "                    data['features'] = str(features)\n",
    "                    data['sample_rate'] = str(sample_rate)\n",
    "                    data['noise_rate'] = str(noise_rate)\n",
    "                    data['label_noise_rate'] = str(label_noise_rate)\n",
    "                    data['epochs'] = str(epochs)\n",
    "                    data['batch_size'] = str(batch_size)\n",
    "                    data['data_length_time'] = str(data_length_time)\n",
    "                    data['repetitions'] = str(repetitions)\n",
    "                    data['n_h_block'] = str(n_h_block)\n",
    "                    data['n_train_h_block'] = str(n_train_h_block)\n",
    "                    data['n_valid_h_block'] = str(n_valid_h_block)\n",
    "                    data['n_test_h_block'] = str(n_test_h_block)\n",
    "                    data['h_moving_step'] = str(h_moving_step)\n",
    "                    data['segments_time'] = str(segments_time)\n",
    "                    data['segments_overlap'] = str(segments_overlap)\n",
    "                    data['inner_classifier'] = str(model)\n",
    "                    data['datetime'] = datetime.now().strftime(\"%Y:%m:%d %H:%M:%S\")\n",
    "                    data['running_time'] = str(running_time.seconds) + \" seconds\"\n",
    "                    data['n_params'] = classifier.count_params()\n",
    "                    data['segments_time'] = timedelta(seconds=int(segments_time))\n",
    "                    data['segments_overlap'] = segments_overlap\n",
    "                    data['decision_time'] = timedelta(seconds=int(decision_time))\n",
    "                    data['decision_overlap'] = decision_overlap\n",
    "                    statistics_summary = {}\n",
    "                    for key in all_statistics[prediction_method].keys():\n",
    "                        for inner_key in all_statistics[prediction_method][key].keys():\n",
    "                            statistics_summary[key + '_' + inner_key + '_mean'] = np.average(\n",
    "                                all_statistics[prediction_method][key][inner_key])\n",
    "                            statistics_summary[key + '_' + inner_key + '_std'] = np.std(\n",
    "                                all_statistics[prediction_method][key][inner_key])\n",
    "                            statistics_summary[key + '_' + inner_key + '_max'] = np.max(\n",
    "                                all_statistics[prediction_method][key][inner_key])\n",
    "                            statistics_summary[key + '_' + inner_key + '_min'] = np.min(\n",
    "                                all_statistics[prediction_method][key][inner_key])\n",
    "                    data.update(statistics_summary)\n",
    "                    # Save information\n",
    "                    save_result(log_dir=log_dir, data=data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4nolQhVbbq8P"
   },
   "source": [
    "#SOTA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Z0PEMv-sbsWc",
    "outputId": "b6261761-a948-4cb8-f9d0-b903c1e7ee93"
   },
   "outputs": [],
   "source": [
    "noise_rate = 100\n",
    "label_noise_rate = 0.0\n",
    "\n",
    "epochs = 50\n",
    "batch_size = 32\n",
    "models = ['CNN_L']\n",
    "repetitions = 10\n",
    "restore_best = True\n",
    "lr = 0.0001\n",
    "beta_1 = 0.5\n",
    "\n",
    "datasets = ['DriverIdentification']\n",
    "for model in models:\n",
    "    for problem in datasets:\n",
    "          log_dir = f\"./logs/sota/{problem}/\"\n",
    "          dataset = problems[problem]['dataset']\n",
    "          n_classes = problems[problem]['n_classes']\n",
    "          features = problems[problem]['features']\n",
    "          sample_rate = problems[problem]['sample_rate']\n",
    "          data_length_time = problems[problem]['data_length_time']\n",
    "          n_h_block = problems[problem]['n_h_block']\n",
    "          n_train_h_block = problems[problem]['n_train_h_block']\n",
    "          n_valid_h_block = problems[problem]['n_valid_h_block']\n",
    "          n_test_h_block = problems[problem]['n_test_h_block']\n",
    "          h_moving_step = problems[problem]['h_moving_step']\n",
    "          # train config index\n",
    "          for tci in [0]:\n",
    "              last_activation = \"softmax\"\n",
    "              segments_time = problems[problem][model + '/segments_time']\n",
    "              segments_overlap = problems[problem]['segments_overlaps']\n",
    "              decision_time = problems[problem][model + '/decision_times']\n",
    "              decision_overlap = problems[problem]['decision_overlaps']\n",
    "              classifier = eval(model)(classes=n_classes,\n",
    "                                         n_features=len(features),\n",
    "                                         segments_size=int(segments_time * sample_rate),\n",
    "                                         segments_overlap=segments_overlap,\n",
    "                                         decision_size=int(decision_time * sample_rate),\n",
    "                                         decision_overlap=decision_overlap,\n",
    "                                         loss_metric=train_config[tci][\"loss_metric\"],\n",
    "                                         loss_function=train_config[tci][\"loss_function\"],\n",
    "                                         lr=lr,\n",
    "                                         beta_1=beta_1,\n",
    "                                         last_activation=last_activation,\n",
    "                                         training=False)\n",
    "              start = datetime.now()\n",
    "\n",
    "              add_noise = noise_rate < 100\n",
    "              data_blocks = [i for i in range(n_h_block)]\n",
    "              n_vt = (n_valid_h_block + n_test_h_block)\n",
    "              n_iteration = int((n_h_block - n_vt) / h_moving_step)\n",
    "              i = 0\n",
    "              training_container = data_blocks[0:i] + data_blocks[i + n_vt:n_h_block]\n",
    "              train_blocks = training_container[:n_train_h_block]\n",
    "              valid_blocks = data_blocks[i: i + n_valid_h_block]\n",
    "              test_blocks = data_blocks[i + n_valid_h_block: i + n_vt]\n",
    "              if problem == 'Dataset#2':\n",
    "                db = Dataset3(dataset,\n",
    "                                  sample_rate,\n",
    "                                  features=features,\n",
    "                                  window_time=segments_time,\n",
    "                                  window_overlap_percentage=segments_overlap,\n",
    "                                  decision_time=decision_time,\n",
    "                                  decision_overlap_percentage=decision_overlap,\n",
    "                                  add_noise=add_noise,\n",
    "                                  noise_rate=noise_rate,\n",
    "                                  label_noise_rate=label_noise_rate,\n",
    "                                  train_blocks=train_blocks,\n",
    "                                  valid_blocks=valid_blocks,\n",
    "                                  test_blocks=test_blocks,\n",
    "                                  data_length_time=data_length_time)\n",
    "              elif problem == 'UCI-HAR-Dataset':\n",
    "                db = Dataset3(dataset,\n",
    "                                  sample_rate,\n",
    "                                  features=features,\n",
    "                                  window_time=segments_time,\n",
    "                                  window_overlap_percentage=segments_overlap,\n",
    "                                  decision_time=decision_time,\n",
    "                                  decision_overlap_percentage=decision_overlap,\n",
    "                                  add_noise=add_noise,\n",
    "                                  noise_rate=noise_rate,\n",
    "                                  label_noise_rate=label_noise_rate,\n",
    "                                  train_blocks=train_blocks,\n",
    "                                  valid_blocks=valid_blocks,\n",
    "                                  test_blocks=test_blocks,\n",
    "                                  data_length_time=data_length_time)\n",
    "              elif problem == 'DriverIdentification':\n",
    "                db = Dataset(dataset,\n",
    "                                  sample_rate,\n",
    "                                  features=features,\n",
    "                                  window_time=segments_time,\n",
    "                                  window_overlap_percentage=segments_overlap,\n",
    "                                  decision_time=decision_time,\n",
    "                                  decision_overlap_percentage=decision_overlap,\n",
    "                                  add_noise=add_noise,\n",
    "                                  noise_rate=noise_rate,\n",
    "                                  label_noise_rate=label_noise_rate,\n",
    "                                  train_blocks=train_blocks,\n",
    "                                  valid_blocks=valid_blocks,\n",
    "                                  test_blocks=test_blocks,\n",
    "                                  data_length_time=data_length_time)\n",
    "              db.load_data(n_classes=n_classes, method=classifier.get_data_arrangement())\n",
    "\n",
    "              all_statistics = h_block_analyzer(db_path=dataset,\n",
    "                                                                     sample_rate=sample_rate,\n",
    "                                                                     features=features,\n",
    "                                                                     n_classes=n_classes,\n",
    "                                                                     noise_rate=noise_rate,\n",
    "                                                                     label_noise_rate=label_noise_rate,\n",
    "                                                                     segments_time=segments_time,\n",
    "                                                                     segments_overlap=segments_overlap,\n",
    "                                                                     decision_time=decision_time,\n",
    "                                                                     decision_overlap=decision_overlap,\n",
    "                                                                     classifier=classifier,\n",
    "                                                                     epochs=epochs,\n",
    "                                                                     batch_size=batch_size,\n",
    "                                                                     data_length_time=data_length_time,\n",
    "                                                                     n_h_block=n_h_block,\n",
    "                                                                     n_train_h_block=n_train_h_block,\n",
    "                                                                     n_valid_h_block=n_valid_h_block,\n",
    "                                                                     n_test_h_block=n_test_h_block,\n",
    "                                                                     h_moving_step=h_moving_step,\n",
    "                                                                     monitor_metric=train_config[tci][\"monitor_metric\"],\n",
    "                                                                     monitor_mode=train_config[tci][\"monitor_mode\"],\n",
    "                                                                     restore_best=restore_best,\n",
    "                                                                     repetitions=repetitions,\n",
    "                                                                   is_sota=True)\n",
    "              end = datetime.now()\n",
    "              running_time = end - start\n",
    "              for prediction_method in all_statistics.keys():\n",
    "                    # Summarizing the results of cross-validation\n",
    "                    data = {}\n",
    "                    data['dataset'] = dataset\n",
    "                    data['prediction_method'] = prediction_method\n",
    "                    data['class'] = str(n_classes)\n",
    "                    loss_metric = train_config[tci][\"loss_metric\"]\n",
    "                    data['loss_metric'] = loss_metric if type(loss_metric) == str else loss_metric.__name__\n",
    "                    loss_function = train_config[tci][\"loss_function\"]\n",
    "                    data['loss_function'] = loss_function if type(loss_function) == str else str(loss_function)\n",
    "                    data['lr'] = lr\n",
    "                    data['beta_1'] = beta_1\n",
    "                    data['monitor_metric'] = train_config[tci][\"monitor_metric\"]\n",
    "                    data['monitor_mode'] = train_config[tci][\"monitor_mode\"]\n",
    "                    data['restore_best'] = str(restore_best)\n",
    "                    data['features'] = str(features)\n",
    "                    data['sample_rate'] = str(sample_rate)\n",
    "                    data['noise_rate'] = str(noise_rate)\n",
    "                    data['label_noise_rate'] = str(label_noise_rate)\n",
    "                    data['epochs'] = str(epochs)\n",
    "                    data['batch_size'] = str(batch_size)\n",
    "                    data['data_length_time'] = str(data_length_time)\n",
    "                    data['repetitions'] = str(repetitions)\n",
    "                    data['n_h_block'] = str(n_h_block)\n",
    "                    data['n_train_h_block'] = str(n_train_h_block)\n",
    "                    data['n_valid_h_block'] = str(n_valid_h_block)\n",
    "                    data['n_test_h_block'] = str(n_test_h_block)\n",
    "                    data['h_moving_step'] = str(h_moving_step)\n",
    "                    data['segments_time'] = str(segments_time)\n",
    "                    data['segments_overlap'] = str(segments_overlap)\n",
    "                    data['inner_classifier'] = str(model)\n",
    "                    data['datetime'] = datetime.now().strftime(\"%Y:%m:%d %H:%M:%S\")\n",
    "                    data['running_time'] = str(running_time.seconds) + \" seconds\"\n",
    "                    data['n_params'] = classifier.count_params()\n",
    "                    data['segments_time'] = timedelta(seconds=int(segments_time))\n",
    "                    data['segments_overlap'] = segments_overlap\n",
    "                    data['decision_time'] = timedelta(seconds=int(decision_time))\n",
    "                    data['decision_overlap'] = decision_overlap\n",
    "                    statistics_summary = {}\n",
    "                    for key in all_statistics[prediction_method].keys():\n",
    "                        for inner_key in all_statistics[prediction_method][key].keys():\n",
    "                            statistics_summary[key + '_' + inner_key + '_mean'] = np.average(\n",
    "                                all_statistics[prediction_method][key][inner_key])\n",
    "                            statistics_summary[key + '_' + inner_key + '_std'] = np.std(\n",
    "                                all_statistics[prediction_method][key][inner_key])\n",
    "                            statistics_summary[key + '_' + inner_key + '_max'] = np.max(\n",
    "                                all_statistics[prediction_method][key][inner_key])\n",
    "                            statistics_summary[key + '_' + inner_key + '_min'] = np.min(\n",
    "                                all_statistics[prediction_method][key][inner_key])\n",
    "                    data.update(statistics_summary)\n",
    "                    # Save information\n",
    "                    save_result(log_dir=log_dir, data=data)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "qnEf299z3qfE",
    "J3yxSrGp3u-E",
    "rsqjDnvY30UY",
    "XJE0iXH6a0nk",
    "1NRgkdXOYoky",
    "fTAGp_3s364X",
    "9KP0pEQM4KNT",
    "_jM3l01B4R-Y",
    "LJsGcm6jBgAL",
    "urkt3wz-Pe_q",
    "JIPRTjzc4eLw",
    "vDN5yNvo4jZ0",
    "Z9CZOYRH4y8U",
    "0TtSkS-W454c",
    "oo0q8aqH4-4w",
    "LFsacgLZ5EG4",
    "BHeTm0ql5Oye",
    "9EcygQRU5SK_"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "tf_m1",
   "language": "python",
   "name": "tf_m1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
