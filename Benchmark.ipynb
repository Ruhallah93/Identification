{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZIm0OmusVgXS"
   },
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "652KpBBeANzx"
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from datetime import timedelta\n",
    "from tensorflow.keras.optimizers.legacy import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.9.2\r\n"
     ]
    }
   ],
   "source": [
    "!python --version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-Ng6rf-7xygn"
   },
   "source": [
    "# Data Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "42vYsEBkx3ED"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "class Transformer:\n",
    "\n",
    "    def __init__(self, decision_size, decision_overlap, segments_size=90, segments_overlap=45, sampling=2):\n",
    "        self.segments_size = segments_size\n",
    "        self.segments_overlap = segments_overlap\n",
    "        self.sampling = sampling\n",
    "        self.decision_size = decision_size\n",
    "        self.decision_overlap = decision_overlap\n",
    "\n",
    "    def transfer(self, dataset, features, method):\n",
    "        print(\"segmenting data with \" + str(len(dataset)) + \" points\")\n",
    "        segments, labels = self.__segment_signal(dataset, features)\n",
    "        print(\"making \" + str(len(segments)) + \" segments\")\n",
    "        if method == \"table\":\n",
    "            segments_dataset = self.__transfer_table(segments, features)\n",
    "        elif method == \"1d\":\n",
    "            segments_dataset = self.__transfer_1d(segments, features)\n",
    "        elif method == \"2d\":\n",
    "            segments_dataset = self.__transfer_2d(segments, features)\n",
    "        elif method == \"3d_1ch\":\n",
    "          segments_dataset = self.__transfer_2d_1ch(segments, features)\n",
    "        elif method == \"3d\":\n",
    "            segments_dataset = self.__transfer_3d(segments, features)\n",
    "        elif method == \"4d\":\n",
    "            segments_dataset = self.__transfer_4d(segments, features)\n",
    "        elif method == \"rnn_2d\":\n",
    "            segments_dataset, labels =  self.__transfer_rnn_2d(segments, labels, features)\n",
    "        elif method == \"rnn_3d_1ch\":\n",
    "            segments_dataset, labels =  self.__transfer_rnn_3d_1ch(segments, labels, features)\n",
    "        return segments_dataset, labels\n",
    "\n",
    "    @staticmethod\n",
    "    def data_shape(method, n_features, segments_size, segments_overlap=None, decision_size=None):\n",
    "        if method == \"table\":\n",
    "            return (None, n_features * segments_size)\n",
    "        elif method == \"1d\":\n",
    "            return (None, 1, n_features * segments_size, 1)\n",
    "        elif method == \"2d\":\n",
    "            return (None, n_features, segments_size)\n",
    "        elif method == \"3d_1ch\":\n",
    "            return (None, n_features, segments_size, 1)\n",
    "        elif method == \"3d\":\n",
    "            return (None, 1, segments_size, n_features)\n",
    "        elif method == \"4d\":\n",
    "            return (n_features, None, 1, segments_size, 1)\n",
    "        elif method == \"rnn_2d\":\n",
    "            s_b = Transformer.get_segments_a_decision_window(segments_size,\n",
    "                                                             int(segments_size * segments_overlap),\n",
    "                                                             decision_size)\n",
    "            return (None, s_b, segments_size*n_features)\n",
    "        elif method == \"rnn_3d_1ch\":\n",
    "            s_b = Transformer.get_segments_a_decision_window(segments_size,\n",
    "                                                             int(segments_size * segments_overlap),\n",
    "                                                             decision_size)\n",
    "            return (None, s_b, 1, segments_size*n_features)\n",
    "        return ()\n",
    "\n",
    "    @staticmethod\n",
    "    def get_segments_a_decision_window(segment_size, segment_overlap_size, decision_size):\n",
    "        return int((decision_size - segment_size) / (segment_size - segment_overlap_size) + 1)\n",
    "\n",
    "    def __transfer_rnn_2d(self, segments, labels, features):\n",
    "        y_output = []\n",
    "        x_output = []\n",
    "        c = len(np.unique(labels))\n",
    "        s = Transformer.get_segments_a_decision_window(self.segments_size,\n",
    "                                                       self.segments_overlap,\n",
    "                                                       self.decision_size)\n",
    "        r = int(np.floor(s * self.decision_overlap))\n",
    "        for _id in np.unique(labels):\n",
    "          subset = segments[np.where(labels == _id)]\n",
    "          n = subset.shape[0]\n",
    "          o = int(np.floor((n - r) / (s - r)))\n",
    "          for i in range(o):\n",
    "            row = []\n",
    "            for j in range(s):\n",
    "              A = subset[i * (s-r) + j]\n",
    "              A = A.reshape(A.shape[0]*A.shape[1])\n",
    "              row.append(A)\n",
    "            y_output.append(_id)\n",
    "            x_output.append(row)\n",
    "        x_output = np.array(x_output)\n",
    "        y_output = np.array(y_output)\n",
    "        return x_output, y_output\n",
    "\n",
    "    def __transfer_rnn_3d_1ch(self, segments, labels, features):\n",
    "        # (samples, time, channels=1, rows)\n",
    "        y_output = []\n",
    "        x_output = []\n",
    "        c = len(np.unique(labels))\n",
    "        s = Transformer.get_segments_a_decision_window(self.segments_size,\n",
    "                                                       self.segments_overlap,\n",
    "                                                       self.decision_size)\n",
    "        r = int(np.floor(s * self.decision_overlap))\n",
    "        for _id in np.unique(labels):\n",
    "          subset = segments[np.where(labels == _id)]\n",
    "          n = subset.shape[0]\n",
    "          o = int(np.floor((n - r) / (s - r)))\n",
    "          for i in range(o):\n",
    "            row = []\n",
    "            for j in range(s):\n",
    "              A = subset[i * (s-r) + j]\n",
    "              A = A.reshape(A.shape[0]*A.shape[1])\n",
    "              row.append([A])\n",
    "            y_output.append(_id)\n",
    "            x_output.append(row)\n",
    "        x_output = np.array(x_output)\n",
    "        y_output = np.array(y_output)\n",
    "        return x_output, y_output\n",
    "\n",
    "    def __transfer_table(self, segments, features):\n",
    "        new_dataset = []\n",
    "        for segment in segments:\n",
    "            row = []\n",
    "            for feature_i in range(len(features)):\n",
    "                for i in range(len(segment[feature_i])):\n",
    "                    row.append(segment[feature_i][i])\n",
    "            new_dataset.append(row)\n",
    "\n",
    "        new_dataset = np.array(new_dataset)\n",
    "        return new_dataset\n",
    "\n",
    "    def __transfer_1d(self, segments, features):\n",
    "        new_dataset = []\n",
    "        for segment in segments:\n",
    "            row = []\n",
    "            for feature_i in range(len(features)):\n",
    "                for i in range(len(segment[feature_i])):\n",
    "                    row.append(segment[feature_i][i])\n",
    "            new_dataset.append([row])\n",
    "\n",
    "        new_dataset = np.array(new_dataset)\n",
    "        return np.expand_dims(new_dataset, axis=3)\n",
    "\n",
    "    def __transfer_2d(self, segments, features):\n",
    "        new_dataset = []\n",
    "        for segment in segments:\n",
    "            row = []\n",
    "            for feature_i in range(len(features)):\n",
    "                row.append(segment[feature_i])\n",
    "            new_dataset.append(row)\n",
    "\n",
    "        new_dataset = np.array(new_dataset)\n",
    "        return new_dataset\n",
    "\n",
    "    def __transfer_3d_1ch(self, segments, features):\n",
    "        new_dataset = []\n",
    "        for segment in segments:\n",
    "            row = []\n",
    "            for feature_i in range(len(features)):\n",
    "                row.append(segment[feature_i])\n",
    "            new_dataset.append(row)\n",
    "\n",
    "        new_dataset = np.array(new_dataset)\n",
    "        return np.expand_dims(new_dataset, axis=3)\n",
    "\n",
    "    def __transfer_3d(self, segments, features):\n",
    "        new_dataset = []\n",
    "        for segment in segments:\n",
    "            row = []\n",
    "            for i in range(len(segment[0])):\n",
    "                cell = []\n",
    "                for feature_i in range(len(features)):\n",
    "                    cell.append(segment[feature_i][i])\n",
    "                row.append(cell)\n",
    "            new_dataset.append([row])\n",
    "\n",
    "        new_dataset = np.array(new_dataset)\n",
    "        return new_dataset\n",
    "\n",
    "    def __transfer_4d(self, segments, features):\n",
    "        new_dataset = []\n",
    "        for feature_i in range(len(features)):\n",
    "            row = []\n",
    "            for segment in segments:\n",
    "                cell = []\n",
    "                for element in segment[feature_i]:\n",
    "                    cell.append([element])\n",
    "                row.append([cell])\n",
    "            new_dataset.append(row)\n",
    "\n",
    "        new_dataset = np.array(new_dataset)\n",
    "        return new_dataset\n",
    "\n",
    "    def __windows(self, data):\n",
    "        start = 0\n",
    "        while start < data.count():\n",
    "            yield int(start), int(start + self.segments_size)\n",
    "            start += (self.segments_size - self.segments_overlap)\n",
    "\n",
    "    def __segment_signal(self, dataset, features):\n",
    "        segments = []\n",
    "        labels = []\n",
    "        for class_i in np.unique(dataset[\"id\"]):\n",
    "            subset = dataset[dataset[\"id\"] == class_i]\n",
    "            for (start, end) in self.__windows(subset[\"id\"]):\n",
    "                feature_slices = []\n",
    "                for feature in features:\n",
    "                    feature_slices.append(subset[feature][start:end].tolist())\n",
    "                if len(feature_slices[0]) == self.segments_size:\n",
    "                    segments.append(feature_slices)\n",
    "                    labels.append(class_i)\n",
    "        return np.array(segments), np.array(labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VbUrAsY1enTC"
   },
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "ZdYZjcS33pzd"
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "from datetime import timedelta\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "\n",
    "\n",
    "class Dataset:\n",
    "\n",
    "    def __init__(self, db_path, sample_rate, features, \n",
    "                 window_time, window_overlap_percentage, \n",
    "                 decision_time, decision_overlap_percentage, \n",
    "                 add_noise, noise_rate,\n",
    "                 train_blocks: list, valid_blocks: list, test_blocks: list, \n",
    "                 data_length_time=-1):\n",
    "        \"\"\"\n",
    "        :param db_path:\n",
    "        :param sample_rate:\n",
    "        :param features:\n",
    "        :param window_time: in seconds\n",
    "        :param window_overlap_percentage: example: 0.75 for 75%\n",
    "        :param add_noise: True or False\n",
    "        :param noise_rate:\n",
    "        :param train_blocks:\n",
    "        :param valid_blocks:\n",
    "        :param test_blocks:\n",
    "        :param data_length_time: the amount of data from each class in seconds. -1 means whole existing data.\n",
    "        \"\"\"\n",
    "        self.db_path = db_path\n",
    "        self.features = features\n",
    "        self.sample_rate = sample_rate\n",
    "        self.window_size = window_time * sample_rate\n",
    "        self.window_overlap_size = int(self.window_size * window_overlap_percentage)\n",
    "        self.decision_size = decision_time * sample_rate\n",
    "        self.decision_overlap_size = int(self.decision_size * decision_overlap_percentage)\n",
    "        self.decision_overlap_percentage = decision_overlap_percentage\n",
    "        self.add_noise = add_noise\n",
    "        self.noise_rate = noise_rate\n",
    "        self.train_blocks = train_blocks\n",
    "        self.valid_blocks = valid_blocks\n",
    "        self.test_blocks = test_blocks\n",
    "        self.data_length_size = data_length_time * sample_rate if data_length_time != -1 else -1\n",
    "\n",
    "        # Initialization\n",
    "        self.train_dataset = pd.DataFrame()\n",
    "        self.valid_dataset = pd.DataFrame()\n",
    "        self.test_dataset = pd.DataFrame()\n",
    "        self.n_train_dataset = pd.DataFrame()\n",
    "        self.n_valid_dataset = pd.DataFrame()\n",
    "        self.n_test_dataset = pd.DataFrame()\n",
    "        self.X_train = np.array([])\n",
    "        self.y_train = np.array([])\n",
    "        self.X_valid = np.array([])\n",
    "        self.y_valid = np.array([])\n",
    "        self.X_test = np.array([])\n",
    "        self.y_test = np.array([])\n",
    "\n",
    "    def load_data(self, n_classes, method):\n",
    "        segments_path = self.db_path + \\\n",
    "                        \"segments/\" + \\\n",
    "                        \"method \" + str(method) + os.sep + \\\n",
    "                        \"wl \" + str(self.window_size) + os.sep + \\\n",
    "                        \"wo \" + str(self.window_overlap_size) + os.sep + \\\n",
    "                        \"dl \" + str(self.decision_size) + os.sep + \\\n",
    "                        \"do \" + str(self.decision_overlap_size) + os.sep + \\\n",
    "                        \"train \" + str(self.train_blocks) + os.sep + \\\n",
    "                        \"valid \" + str(self.valid_blocks) + os.sep + \\\n",
    "                        \"test \" + str(self.test_blocks) + os.sep\n",
    "        print(segments_path)\n",
    "        if os.path.exists(segments_path + 'X_train.npy') \\\n",
    "                and os.path.exists(segments_path + 'y_train.npy') \\\n",
    "                and os.path.exists(segments_path + 'X_valid.npy') \\\n",
    "                and os.path.exists(segments_path + 'y_valid.npy') \\\n",
    "                and os.path.exists(segments_path + 'X_test.npy') \\\n",
    "                and os.path.exists(segments_path + 'y_test.npy'):\n",
    "            print(\"Dataset is already existed!\")\n",
    "            self.X_train = np.load(segments_path + 'X_train.npy')\n",
    "            self.y_train = np.load(segments_path + 'y_train.npy')\n",
    "            self.X_valid = np.load(segments_path + 'X_valid.npy')\n",
    "            self.y_valid = np.load(segments_path + 'y_valid.npy')\n",
    "            self.X_test = np.load(segments_path + 'X_test.npy')\n",
    "            self.y_test = np.load(segments_path + 'y_test.npy')\n",
    "        else:\n",
    "            self.__preprocess(n_classes, method)\n",
    "            # Save Dataset\n",
    "            if not os.path.exists(segments_path):\n",
    "                os.makedirs(segments_path)\n",
    "            np.save(segments_path + 'X_train.npy', self.X_train)\n",
    "            np.save(segments_path + 'y_train.npy', self.y_train)\n",
    "            np.save(segments_path + 'X_valid.npy', self.X_valid)\n",
    "            np.save(segments_path + 'y_valid.npy', self.y_valid)\n",
    "            np.save(segments_path + 'X_test.npy', self.X_test)\n",
    "            np.save(segments_path + 'y_test.npy', self.y_test)\n",
    "\n",
    "        def to_dic(data):\n",
    "            dic = {}\n",
    "            for i, x in enumerate(data):\n",
    "                dic[str(i)] = x\n",
    "            return dic\n",
    "\n",
    "        if len(self.X_train.shape) == 5:\n",
    "            self.X_train = to_dic(self.X_train)\n",
    "            self.X_valid = to_dic(self.X_valid)\n",
    "            self.X_test = to_dic(self.X_test)\n",
    "\n",
    "    def __preprocess(self, n_classes, method):\n",
    "        csv_paths = np.random.choice(glob.glob(self.db_path + \"*.csv\"), n_classes, replace=False)\n",
    "\n",
    "        self.class_names = {}\n",
    "        for i, csv_path in enumerate(csv_paths):\n",
    "            label = os.path.basename(csv_path).split('.')[0]\n",
    "            self.class_names[label] = i\n",
    "            train, valid, test = self.__read_data(csv_path, self.features, label)\n",
    "            train['id'] = i\n",
    "            valid['id'] = i\n",
    "            test['id'] = i\n",
    "            self.train_dataset = pd.concat([self.train_dataset, train])\n",
    "            self.valid_dataset = pd.concat([self.valid_dataset, valid])\n",
    "            self.test_dataset = pd.concat([self.test_dataset, test])\n",
    "\n",
    "        self.__standardization()\n",
    "        self.__segmentation(method=method)\n",
    "\n",
    "    def __read_data(self, path, features, label):\n",
    "        data = pd.read_csv(path, low_memory=False)\n",
    "        data = data[features]\n",
    "        data = data.fillna(data.mean())\n",
    "        length = self.data_length_size if self.data_length_size != -1 else data.shape[0]\n",
    "        print('class: %5s, data size: %s, selected data size: %s' % (\n",
    "            label, str(timedelta(seconds=int(data.shape[0] / self.sample_rate))),\n",
    "            str(timedelta(seconds=int(length / self.sample_rate)))))\n",
    "        return self.__split_to_train_valid_test(data)\n",
    "\n",
    "    def __split_to_train_valid_test(self, data):\n",
    "        n_blocks = max(self.train_blocks + self.valid_blocks + self.test_blocks) + 1\n",
    "        block_length = int(len(data[:self.data_length_size]) / n_blocks)\n",
    "\n",
    "        train_data = pd.DataFrame()\n",
    "        for i in range(len(self.train_blocks)):\n",
    "            start = self.train_blocks[i] * block_length\n",
    "            end = self.train_blocks[i] * block_length + block_length - 1\n",
    "            if train_data.empty:\n",
    "                train_data = data[start:end]\n",
    "            else:\n",
    "                train_data = pd.concat([data[start:end], train_data])\n",
    "\n",
    "        valid_data = pd.DataFrame()\n",
    "        for i in range(len(self.valid_blocks)):\n",
    "            start = self.valid_blocks[i] * block_length\n",
    "            end = self.valid_blocks[i] * block_length + block_length - 1\n",
    "            if valid_data.empty:\n",
    "                valid_data = data[start:end]\n",
    "            else:\n",
    "                valid_data = pd.concat([data[start:end], valid_data])\n",
    "\n",
    "        test_data = pd.DataFrame()\n",
    "        for i in range(len(self.test_blocks)):\n",
    "            start = self.test_blocks[i] * block_length\n",
    "            end = self.test_blocks[i] * block_length + block_length - 1\n",
    "            if test_data.empty:\n",
    "                test_data = data[start:end]\n",
    "            else:\n",
    "                test_data = pd.concat([data[start:end], test_data])\n",
    "\n",
    "        if self.add_noise:\n",
    "            test_data = self.__add_noise_to_data(test_data)\n",
    "\n",
    "        return train_data, valid_data, test_data\n",
    "\n",
    "    def __add_noise_to_data(self, x):\n",
    "        x_power = x ** 2\n",
    "        sig_avg_watts = np.mean(x_power)\n",
    "        sig_avg_db = 10 * np.log10(sig_avg_watts)\n",
    "        noise_avg_db = sig_avg_db - self.target_snr_db\n",
    "        noise_avg_watts = 10 ** (noise_avg_db / 10)\n",
    "        mean_noise = 0\n",
    "        noise_volts = np.random.normal(mean_noise, np.sqrt(noise_avg_watts), size=x.shape)\n",
    "        return x + noise_volts\n",
    "\n",
    "    def __standardization(self):\n",
    "        scaler = preprocessing.StandardScaler()\n",
    "        scaler = scaler.fit(self.train_dataset.iloc[:, :-1])\n",
    "        n_train_dataset = scaler.transform(self.train_dataset.iloc[:, :-1])\n",
    "        n_valid_dataset = scaler.transform(self.valid_dataset.iloc[:, :-1])\n",
    "        n_test_dataset = scaler.transform(self.test_dataset.iloc[:, :-1])\n",
    "\n",
    "        self.n_train_dataset = pd.DataFrame(n_train_dataset, columns=self.features)\n",
    "        self.n_valid_dataset = pd.DataFrame(n_valid_dataset, columns=self.features)\n",
    "        self.n_test_dataset = pd.DataFrame(n_test_dataset, columns=self.features)\n",
    "        self.n_train_dataset['id'] = self.train_dataset.iloc[:, -1].tolist()\n",
    "        self.n_valid_dataset['id'] = self.valid_dataset.iloc[:, -1].tolist()\n",
    "        self.n_test_dataset['id'] = self.test_dataset.iloc[:, -1].tolist()\n",
    "\n",
    "    def __segmentation(self, method):\n",
    "        transformer = Transformer(segments_size=self.window_size, \n",
    "                                  segments_overlap=self.window_overlap_size,\n",
    "                                  decision_size=self.decision_size,\n",
    "                                  decision_overlap=self.decision_overlap_percentage)\n",
    "        self.X_train, self.y_train = transformer.transfer(self.n_train_dataset, self.features, method=method)\n",
    "        self.X_valid, self.y_valid = transformer.transfer(self.n_valid_dataset, self.features, method=method)\n",
    "        self.X_test, self.y_test = transformer.transfer(self.n_test_dataset, self.features, method=method)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RECURRENT MODELS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "w0GbZblOypVb"
   },
   "outputs": [],
   "source": [
    "class LSTM():\n",
    "    def __init__(self, classes, n_features,\n",
    "                 segments_size, segments_overlap, decision_size, decision_overlap):\n",
    "        self.classes = classes\n",
    "        self.n_features = n_features\n",
    "        self.segments_size = segments_size\n",
    "        self.segments_overlap = segments_overlap\n",
    "        self.decision_size = decision_size\n",
    "        self.decision_overlap = decision_overlap\n",
    "        self.input_shape = self.get_input_shape()\n",
    "        self.initializer = tf.keras.initializers.GlorotNormal()\n",
    "\n",
    "        # Build and compile the model\n",
    "        self.model = self.build_model_l()\n",
    "\n",
    "        optimizer = Adam(0.001, 0.5)\n",
    "        loss_fn = tf.keras.losses.CategoricalCrossentropy()\n",
    "        self.model.compile(loss=loss_fn,\n",
    "                           optimizer=optimizer,\n",
    "                           metrics=['accuracy'])\n",
    "\n",
    "    def get_input_shape(self):\n",
    "        data_shape = Transformer.data_shape(method=self.get_data_arrangement(),\n",
    "                                            n_features=self.n_features,\n",
    "                                            segments_size=self.segments_size,\n",
    "                                            segments_overlap=self.segments_overlap,\n",
    "                                            decision_size=self.decision_size)\n",
    "        return data_shape[-2], data_shape[-1]\n",
    "\n",
    "    @staticmethod\n",
    "    def get_data_arrangement():\n",
    "        return \"rnn_2d\"\n",
    "\n",
    "    def count_params(self):\n",
    "        return self.model.count_params()\n",
    "\n",
    "    def build_model_l(self):\n",
    "        print(self.input_shape)\n",
    "        input_ = tf.keras.layers.Input(shape=self.input_shape)\n",
    "\n",
    "        x = tf.keras.layers.LSTM(48)(input_)\n",
    "        dense = tf.keras.layers.Dense(self.classes, activation='softmax')(x)\n",
    "\n",
    "        model = tf.keras.models.Model(inputs=input_, outputs=[dense])\n",
    "        return model\n",
    "\n",
    "    def train(self, epochs, X_train, y_train, X_valid, y_valid, X_test, batch_size=128):\n",
    "        # Change the labels from categorical to one-hot encoding\n",
    "        y_train_onehot = np.asarray(pd.get_dummies(y_train), dtype=np.int8)\n",
    "        y_valid_onehot = np.asarray(pd.get_dummies(y_valid), dtype=np.int8)\n",
    "        self.model.fit(X_train, y_train_onehot,\n",
    "                           validation_data=(X_valid, y_valid_onehot),\n",
    "                           batch_size=batch_size,\n",
    "                           epochs=epochs,\n",
    "                           verbose=1,\n",
    "                           shuffle=True)\n",
    "\n",
    "        return self.model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "pbNdQNTt_h__"
   },
   "outputs": [],
   "source": [
    "class ConvLSTM():\n",
    "    def __init__(self, classes, n_features,\n",
    "                 segments_size, segments_overlap, decision_size, decision_overlap):\n",
    "        self.classes = classes\n",
    "        self.n_features = n_features\n",
    "        self.segments_size = segments_size\n",
    "        self.segments_overlap = segments_overlap\n",
    "        self.decision_size = decision_size\n",
    "        self.decision_overlap = decision_overlap\n",
    "        self.input_shape = self.get_input_shape()\n",
    "        self.initializer = tf.keras.initializers.GlorotNormal()\n",
    "\n",
    "        # Build and compile the model\n",
    "        self.model = self.build_model_l()\n",
    "\n",
    "        optimizer = tf.keras.optimizers.legacy.Adam(0.001, 0.5)\n",
    "        loss_fn = tf.keras.losses.CategoricalCrossentropy()\n",
    "        self.model.compile(loss=loss_fn,\n",
    "                           optimizer=optimizer,\n",
    "                           metrics=['accuracy'])\n",
    "\n",
    "    def get_input_shape(self):\n",
    "        data_shape = Transformer.data_shape(method=self.get_data_arrangement(),\n",
    "                                            n_features=self.n_features,\n",
    "                                            segments_size=self.segments_size,\n",
    "                                            segments_overlap=self.segments_overlap,\n",
    "                                            decision_size=self.decision_size)\n",
    "        return data_shape[-3], data_shape[-2], data_shape[-1]\n",
    "\n",
    "    @staticmethod\n",
    "    def get_data_arrangement():\n",
    "        return \"rnn_3d_1ch\"\n",
    "\n",
    "    def count_params(self):\n",
    "        return self.model.count_params()\n",
    "\n",
    "    def build_model_l(self):\n",
    "        print(self.input_shape)\n",
    "        input_ = tf.keras.layers.Input(shape=self.input_shape)\n",
    "\n",
    "        x = tf.keras.layers.ConvLSTM1D(2, kernel_size=3, padding='valid',\n",
    "                                       data_format=\"channels_first\",\n",
    "                                       return_sequences=True)(input_)\n",
    "        x = tf.keras.layers.ConvLSTM1D(4, kernel_size=3, padding='valid'\n",
    "                                       , data_format=\"channels_first\"\n",
    "                                       , activation=\"relu\")(x)\n",
    "        x = tf.keras.layers.Flatten()(x)\n",
    "        dense = tf.keras.layers.Dense(self.classes, activation='softmax')(x)\n",
    "\n",
    "        model = tf.keras.models.Model(inputs=input_, outputs=[dense])\n",
    "        return model\n",
    "\n",
    "    def train(self, epochs, X_train, y_train, X_valid, y_valid, X_test, batch_size=128):\n",
    "        # Change the labels from categorical to one-hot encoding\n",
    "        y_train_onehot = np.asarray(pd.get_dummies(y_train), dtype=np.int8)\n",
    "        y_valid_onehot = np.asarray(pd.get_dummies(y_valid), dtype=np.int8)\n",
    "        self.model.fit(X_train, y_train_onehot,\n",
    "                           validation_data=(X_valid, y_valid_onehot),\n",
    "                           batch_size=batch_size,\n",
    "                           epochs=epochs,\n",
    "                           verbose=1,\n",
    "                           shuffle=True)\n",
    "\n",
    "        return self.model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "7z0dXCw5_QzF"
   },
   "outputs": [],
   "source": [
    "class GRU():\n",
    "    def __init__(self, classes, n_features,\n",
    "                 segments_size, segments_overlap, decision_size, decision_overlap):\n",
    "        self.classes = classes\n",
    "        self.n_features = n_features\n",
    "        self.segments_size = segments_size\n",
    "        self.segments_overlap = segments_overlap\n",
    "        self.decision_size = decision_size\n",
    "        self.decision_overlap = decision_overlap\n",
    "        self.input_shape = self.get_input_shape()\n",
    "        self.initializer = tf.keras.initializers.GlorotNormal()\n",
    "\n",
    "        # Build and compile the model\n",
    "        self.model = self.build_model_l()\n",
    "\n",
    "        optimizer = Adam(0.001, 0.5)\n",
    "        loss_fn = tf.keras.losses.CategoricalCrossentropy()\n",
    "        self.model.compile(loss=loss_fn,\n",
    "                           optimizer=optimizer,\n",
    "                           metrics=['accuracy'])\n",
    "\n",
    "    def get_input_shape(self):\n",
    "        data_shape = Transformer.data_shape(method=self.get_data_arrangement(),\n",
    "                                            n_features=self.n_features,\n",
    "                                            segments_size=self.segments_size,\n",
    "                                            segments_overlap=self.segments_overlap,\n",
    "                                            decision_size=self.decision_size)\n",
    "        return data_shape[-2], data_shape[-1]\n",
    "\n",
    "    @staticmethod\n",
    "    def get_data_arrangement():\n",
    "        return \"rnn_2d\"\n",
    "\n",
    "    def count_params(self):\n",
    "        return self.model.count_params()\n",
    "\n",
    "    def build_model_l(self):\n",
    "        print(\"input_shape:\",self.input_shape)\n",
    "        input_ = tf.keras.layers.Input(shape=self.input_shape)\n",
    "\n",
    "        x = tf.keras.layers.GRU(48)(input_)\n",
    "        dense = tf.keras.layers.Dense(self.classes, activation='softmax')(x)\n",
    "\n",
    "        model = tf.keras.models.Model(inputs=input_, outputs=[dense])\n",
    "        return model\n",
    "\n",
    "    def train(self, epochs, X_train, y_train, X_valid, y_valid, X_test, batch_size=128):\n",
    "        # Change the labels from categorical to one-hot encoding\n",
    "        y_train_onehot = np.asarray(pd.get_dummies(y_train), dtype=np.int8)\n",
    "        y_valid_onehot = np.asarray(pd.get_dummies(y_valid), dtype=np.int8)\n",
    "        self.model.fit(X_train, y_train_onehot,\n",
    "                           validation_data=(X_valid, y_valid_onehot),\n",
    "                           batch_size=batch_size,\n",
    "                           epochs=epochs,\n",
    "                           verbose=1,\n",
    "                           shuffle=True)\n",
    "\n",
    "        return self.model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiLSTM():\n",
    "    def __init__(self, classes, n_features,\n",
    "                 segments_size, segments_overlap, decision_size, decision_overlap):\n",
    "        self.classes = classes\n",
    "        self.n_features = n_features\n",
    "        self.segments_size = segments_size\n",
    "        self.segments_overlap = segments_overlap\n",
    "        self.decision_size = decision_size\n",
    "        self.decision_overlap = decision_overlap\n",
    "        self.input_shape = self.get_input_shape()\n",
    "        self.initializer = tf.keras.initializers.GlorotNormal()\n",
    "\n",
    "        # Build and compile the model\n",
    "        self.model = self.build_model_l()\n",
    "\n",
    "        optimizer = Adam(0.001, 0.5)\n",
    "        loss_fn = tf.keras.losses.CategoricalCrossentropy()\n",
    "        self.model.compile(loss=loss_fn,\n",
    "                           optimizer=optimizer,\n",
    "                           metrics=['accuracy'])\n",
    "\n",
    "    def get_input_shape(self):\n",
    "        data_shape = Transformer.data_shape(method=self.get_data_arrangement(),\n",
    "                                            n_features=self.n_features,\n",
    "                                            segments_size=self.segments_size,\n",
    "                                            segments_overlap=self.segments_overlap,\n",
    "                                            decision_size=self.decision_size)\n",
    "        return data_shape[-2], data_shape[-1]\n",
    "\n",
    "    @staticmethod\n",
    "    def get_data_arrangement():\n",
    "        return \"rnn_2d\"\n",
    "\n",
    "    def count_params(self):\n",
    "        return self.model.count_params()\n",
    "\n",
    "    def build_model_l(self):\n",
    "        print(self.input_shape)\n",
    "        input_ = tf.keras.layers.Input(shape=self.input_shape)\n",
    "\n",
    "        x = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(48, return_sequences=True))(input_)\n",
    "        x = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(48))(x)\n",
    "        dense = tf.keras.layers.Dense(self.classes, activation='softmax')(x)\n",
    "\n",
    "        model = tf.keras.models.Model(inputs=input_, outputs=[dense])\n",
    "        return model\n",
    "\n",
    "    def train(self, epochs, X_train, y_train, X_valid, y_valid, X_test, batch_size=128):\n",
    "        # Change the labels from categorical to one-hot encoding\n",
    "        y_train_onehot = np.asarray(pd.get_dummies(y_train), dtype=np.int8)\n",
    "        y_valid_onehot = np.asarray(pd.get_dummies(y_valid), dtype=np.int8)\n",
    "        self.model.fit(X_train, y_train_onehot,\n",
    "                           validation_data=(X_valid, y_valid_onehot),\n",
    "                           batch_size=batch_size,\n",
    "                           epochs=epochs,\n",
    "                           verbose=1,\n",
    "                           shuffle=True)\n",
    "\n",
    "        return self.model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_w5A0xO9Upk1"
   },
   "source": [
    "# Classic Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "SPWOSdNZUr2k"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import preprocessing\n",
    "\n",
    "\n",
    "# Ensembles of Convolutinal Neural Network\n",
    "class MULTI_CNN():\n",
    "    def __init__(self, classes, n_features,\n",
    "                 segments_size, segments_overlap, decision_size, decision_overlap):\n",
    "        self.classes = classes\n",
    "        self.n_features = n_features\n",
    "        self.segments_size = segments_size\n",
    "        self.input_shape = self.get_input_shape()\n",
    "        self.segments_overlap = segments_overlap\n",
    "        self.decision_size = decision_size\n",
    "        self.decision_overlap = decision_overlap\n",
    "        self.initializer = tf.keras.initializers.GlorotNormal()\n",
    "\n",
    "        # Build and compile the model\n",
    "        self.model = self.build_model_multi()\n",
    "\n",
    "        optimizer = Adam(0.001, 0.5)\n",
    "        loss_fn = tf.keras.losses.CategoricalCrossentropy()\n",
    "        self.model.compile(loss=loss_fn,\n",
    "                           optimizer=optimizer,\n",
    "                           metrics=['accuracy'])\n",
    "\n",
    "    def get_input_shape(self):\n",
    "        data_shape = Transformer.data_shape(method=self.get_data_arrangement(), n_features=self.n_features,\n",
    "                                            segments_size=self.segments_size)\n",
    "        return data_shape[-3], data_shape[-2], data_shape[-1]\n",
    "\n",
    "    @staticmethod\n",
    "    def get_data_arrangement():\n",
    "        return \"4d\"\n",
    "\n",
    "    def count_params(self):\n",
    "        return self.model.count_params()\n",
    "\n",
    "    def build_model_multi(self):\n",
    "        input_ = [tf.keras.layers.Input(shape=self.input_shape, name=str(i)) for i in range(self.n_features)]\n",
    "        models = [self.build_cn_part(input_[i]) for i in range(self.n_features)]\n",
    "\n",
    "        combined = tf.keras.layers.Concatenate(axis=1)(models)\n",
    "\n",
    "        dense = tf.keras.layers.Flatten()(combined)\n",
    "        dense = tf.keras.layers.Dense(1024)(dense)\n",
    "        dense = tf.keras.layers.LeakyReLU(alpha=0.2)(dense)\n",
    "        dense = tf.keras.layers.Dropout(0.2)(dense)\n",
    "        dense = tf.keras.layers.Dense(self.classes, activation='softmax')(dense)\n",
    "\n",
    "        model = tf.keras.models.Model(inputs=input_, outputs=[dense])\n",
    "        return model\n",
    "\n",
    "    def build_cn_part(self, x):\n",
    "        x = tf.keras.layers.Conv2D(64, kernel_size=(1, 3), strides=1, padding=\"same\", activation='relu')(x)\n",
    "        x = tf.keras.layers.LeakyReLU(alpha=0.2)(x)\n",
    "        x = tf.keras.layers.Dropout(0.2)(x)\n",
    "        x = tf.keras.layers.Conv2D(32, kernel_size=(1, 3), strides=1, padding=\"same\", activation='relu')(x)\n",
    "        x = tf.keras.layers.BatchNormalization(momentum=0.9)(x)\n",
    "        x = tf.keras.layers.LeakyReLU(alpha=0.2)(x)\n",
    "        x = tf.keras.layers.Dropout(0.2)(x)\n",
    "        x = tf.keras.layers.Conv2D(16, kernel_size=(1, 3), strides=1, padding=\"same\", activation='relu')(x)\n",
    "        x = tf.keras.layers.BatchNormalization(momentum=0.9)(x)\n",
    "        x = tf.keras.layers.LeakyReLU(alpha=0.2)(x)\n",
    "        x = tf.keras.layers.Dropout(0.2)(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def train(self, epochs, X_train, y_train, X_valid, y_valid, X_test, restore_best=True, batch_size=128):\n",
    "        # Change the labels from categorical to one-hot encoding\n",
    "        y_train_onehot = np.asarray(pd.get_dummies(y_train), dtype=np.int8)\n",
    "        y_valid_onehot = np.asarray(pd.get_dummies(y_valid), dtype=np.int8)\n",
    "        self.model.fit(X_train, y_train_onehot,\n",
    "                           validation_data=(X_valid, y_valid_onehot),\n",
    "                           batch_size=batch_size,\n",
    "                           epochs=epochs,\n",
    "                           verbose=1,\n",
    "                           shuffle=True)\n",
    "\n",
    "        return self.model.predict(X_test)\n",
    "\n",
    "\n",
    "# Convolutinal Neural Network\n",
    "class CNN_L():\n",
    "    def __init__(self, classes, n_features,\n",
    "                 segments_size, segments_overlap, decision_size, decision_overlap):\n",
    "        self.classes = classes\n",
    "        self.n_features = n_features\n",
    "        self.segments_size = segments_size\n",
    "        self.input_shape = self.get_input_shape()\n",
    "        self.segments_overlap = segments_overlap\n",
    "        self.decision_size = decision_size\n",
    "        self.decision_overlap = decision_overlap\n",
    "        self.initializer = tf.keras.initializers.GlorotNormal()\n",
    "\n",
    "        # Build and compile the model\n",
    "        self.model = self.build_model_l()\n",
    "\n",
    "        optimizer = Adam(0.001, 0.5)\n",
    "        loss_fn = tf.keras.losses.CategoricalCrossentropy()\n",
    "        self.model.compile(loss=loss_fn,\n",
    "                           optimizer=optimizer,\n",
    "                           metrics=['accuracy'])\n",
    "\n",
    "    def get_input_shape(self):\n",
    "        data_shape = Transformer.data_shape(method=self.get_data_arrangement(), n_features=self.n_features,\n",
    "                                            segments_size=self.segments_size)\n",
    "        return data_shape[-3], data_shape[-2], data_shape[-1]\n",
    "\n",
    "    @staticmethod\n",
    "    def get_data_arrangement():\n",
    "        return \"3d\"\n",
    "\n",
    "    def count_params(self):\n",
    "        return self.model.count_params()\n",
    "\n",
    "    def build_model_l(self):\n",
    "        input_ = tf.keras.layers.Input(shape=self.input_shape)\n",
    "\n",
    "        x = tf.keras.layers.Conv2D(64, kernel_size=(1, 3), strides=1, padding=\"same\", activation='relu')(input_)\n",
    "        x = tf.keras.layers.LeakyReLU(alpha=0.2)(x)\n",
    "        x = tf.keras.layers.Dropout(0.2)(x)\n",
    "        x = tf.keras.layers.Conv2D(32, kernel_size=(1, 3), strides=1, padding=\"same\", activation='relu')(x)\n",
    "        x = tf.keras.layers.BatchNormalization(momentum=0.9)(x)\n",
    "        x = tf.keras.layers.LeakyReLU(alpha=0.2)(x)\n",
    "        x = tf.keras.layers.Dropout(0.2)(x)\n",
    "        x = tf.keras.layers.Conv2D(16, kernel_size=(1, 3), strides=1, padding=\"same\", activation='relu')(x)\n",
    "        x = tf.keras.layers.BatchNormalization(momentum=0.9)(x)\n",
    "        x = tf.keras.layers.LeakyReLU(alpha=0.2)(x)\n",
    "        x = tf.keras.layers.Dropout(0.2)(x)\n",
    "\n",
    "        dense = tf.keras.layers.Flatten()(x)\n",
    "        dense = tf.keras.layers.Dense(1024)(dense)\n",
    "        dense = tf.keras.layers.LeakyReLU(alpha=0.2)(dense)\n",
    "        dense = tf.keras.layers.Dropout(0.2)(dense)\n",
    "        dense = tf.keras.layers.Dense(self.classes, activation='softmax')(dense)\n",
    "\n",
    "        model = tf.keras.models.Model(inputs=input_, outputs=[dense])\n",
    "        return model\n",
    "\n",
    "    def train(self, epochs, X_train, y_train, X_valid, y_valid, X_test, restore_best=True, batch_size=128):\n",
    "        # Change the labels from categorical to one-hot encoding\n",
    "        y_train_onehot = np.asarray(pd.get_dummies(y_train), dtype=np.int8)\n",
    "        y_valid_onehot = np.asarray(pd.get_dummies(y_valid), dtype=np.int8)\n",
    "        self.model.fit(X_train, y_train_onehot,\n",
    "                           validation_data=(X_valid, y_valid_onehot),\n",
    "                           batch_size=batch_size,\n",
    "                           epochs=epochs,\n",
    "                           verbose=1,\n",
    "                           shuffle=True)\n",
    "\n",
    "        return self.model.predict(X_test)\n",
    "\n",
    "\n",
    "# Multi Linear Perceptron\n",
    "class MLP():\n",
    "    def __init__(self, classes, n_features,\n",
    "                 segments_size, segments_overlap, decision_size, decision_overlap):\n",
    "        self.classes = classes\n",
    "        self.n_features = n_features\n",
    "        self.segments_size = segments_size\n",
    "        self.input_shape = self.get_input_shape()\n",
    "        self.segments_overlap = segments_overlap\n",
    "        self.decision_size = decision_size\n",
    "        self.decision_overlap = decision_overlap\n",
    "        self.initializer = tf.keras.initializers.GlorotNormal()\n",
    "\n",
    "        # Build and compile the model\n",
    "        self.model = self.build_model()\n",
    "\n",
    "        optimizer = Adam(0.001, 0.5)\n",
    "        loss_fn = tf.keras.losses.CategoricalCrossentropy()\n",
    "        self.model.compile(loss=loss_fn,\n",
    "                           optimizer=optimizer,\n",
    "                           metrics=['accuracy'])\n",
    "\n",
    "    def get_input_shape(self):\n",
    "        data_shape = Transformer.data_shape(method=self.get_data_arrangement(), n_features=self.n_features,\n",
    "                                            segments_size=self.segments_size)\n",
    "        return data_shape[-3], data_shape[-2], data_shape[-1]\n",
    "\n",
    "    @staticmethod\n",
    "    def get_data_arrangement():\n",
    "        return \"3d\"\n",
    "\n",
    "    def count_params(self):\n",
    "        return self.model.count_params()\n",
    "\n",
    "    def build_model(self):\n",
    "        input_ = tf.keras.layers.Input(shape=self.input_shape)\n",
    "\n",
    "        dense = tf.keras.layers.Flatten()(input_)\n",
    "        dense = tf.keras.layers.Dense(1024)(dense)\n",
    "        dense = tf.keras.layers.LeakyReLU(alpha=0.2)(dense)\n",
    "        dense = tf.keras.layers.Dropout(0.2)(dense)\n",
    "        dense = tf.keras.layers.Dense(self.classes, activation='softmax')(dense)\n",
    "\n",
    "        model = tf.keras.models.Model(inputs=input_, outputs=[dense])\n",
    "        return model\n",
    "\n",
    "    def train(self, epochs, X_train, y_train, X_valid, y_valid, X_test, restore_best=True, batch_size=128):\n",
    "        # Change the labels from categorical to one-hot encoding\n",
    "        y_train_onehot = np.asarray(pd.get_dummies(y_train), dtype=np.int8)\n",
    "        y_valid_onehot = np.asarray(pd.get_dummies(y_valid), dtype=np.int8)\n",
    "        self.model.fit(X_train, y_train_onehot,\n",
    "                           validation_data=(X_valid, y_valid_onehot),\n",
    "                           batch_size=batch_size,\n",
    "                           epochs=epochs,\n",
    "                           verbose=1,\n",
    "                           shuffle=True)\n",
    "\n",
    "        return self.model.predict(X_test)\n",
    "\n",
    "\n",
    "# K-Nearest Neighbors\n",
    "class KNN():\n",
    "    def __init__(self, classes, n_features,\n",
    "                 segments_size, segments_overlap, decision_size, decision_overlap):\n",
    "        self.classes = classes\n",
    "        self.n_features = n_features\n",
    "        self.segments_size = segments_size\n",
    "        self.input_shape = self.get_input_shape()\n",
    "        self.segments_overlap = segments_overlap\n",
    "        self.decision_size = decision_size\n",
    "        self.decision_overlap = decision_overlap\n",
    "\n",
    "        # Build and compile the model\n",
    "        self.model = KNeighborsClassifier()\n",
    "\n",
    "    def get_input_shape(self):\n",
    "        data_shape = Transformer.data_shape(method=self.get_data_arrangement(), n_features=self.n_features,\n",
    "                                            segments_size=self.segments_size)\n",
    "        return data_shape[-1]\n",
    "\n",
    "    @staticmethod\n",
    "    def get_data_arrangement():\n",
    "        return \"table\"\n",
    "\n",
    "    def count_params(self):\n",
    "        return -1\n",
    "\n",
    "    def train(self, epochs, X_train, y_train, X_valid, y_valid, X_test, restore_best=True, batch_size=128):\n",
    "        # Change the labels from categorical to one-hot encoding\n",
    "        y_train_onehot = np.asarray(pd.get_dummies(y_train), dtype=np.int8)\n",
    "        y_valid_onehot = np.asarray(pd.get_dummies(y_valid), dtype=np.int8)\n",
    "\n",
    "        self.model.fit(X_train, y_train_onehot)\n",
    "\n",
    "        return self.model.predict(X_test)\n",
    "\n",
    "\n",
    "# Logistic Regression\n",
    "class LR():\n",
    "    def __init__(self, classes, n_features,\n",
    "                 segments_size, segments_overlap, decision_size, decision_overlap):\n",
    "        self.classes = classes\n",
    "        self.n_features = n_features\n",
    "        self.segments_size = segments_size\n",
    "        self.input_shape = self.get_input_shape()\n",
    "        self.segments_overlap = segments_overlap\n",
    "        self.decision_size = decision_size\n",
    "        self.decision_overlap = decision_overlap\n",
    "\n",
    "        # Build and compile the model\n",
    "        self.model = LogisticRegression(solver='sag', multi_class='multinomial')\n",
    "\n",
    "    def get_input_shape(self):\n",
    "        data_shape = Transformer.data_shape(method=self.get_data_arrangement(), n_features=self.n_features,\n",
    "                                            segments_size=self.segments_size)\n",
    "        return data_shape[-1]\n",
    "\n",
    "    @staticmethod\n",
    "    def get_data_arrangement():\n",
    "        return \"table\"\n",
    "\n",
    "    def count_params(self):\n",
    "        return -1\n",
    "\n",
    "    def train(self, epochs, X_train, y_train, X_valid, y_valid, X_test, restore_best=True, batch_size=128):\n",
    "        # Change the labels from categorical to one-hot encoding\n",
    "        y_train_onehot = np.asarray(pd.get_dummies(y_train), dtype=np.int8)\n",
    "        y_valid_onehot = np.asarray(pd.get_dummies(y_valid), dtype=np.int8)\n",
    "        self.model.fit(X_train, y_train)\n",
    "\n",
    "        enc = preprocessing.OneHotEncoder()\n",
    "        enc.fit(y_train.reshape(-1, 1))\n",
    "        return enc.transform(self.model.predict(X_test).reshape(-1, 1)).toarray()\n",
    "\n",
    "\n",
    "# Random Forest\n",
    "class RF():\n",
    "    def __init__(self, classes, n_features,\n",
    "                 segments_size, segments_overlap, decision_size, decision_overlap):\n",
    "        self.classes = classes\n",
    "        self.n_features = n_features\n",
    "        self.segments_size = segments_size\n",
    "        self.input_shape = self.get_input_shape()\n",
    "        self.segments_overlap = segments_overlap\n",
    "        self.decision_size = decision_size\n",
    "        self.decision_overlap = decision_overlap\n",
    "\n",
    "        # Build and compile the model\n",
    "        self.model = RandomForestClassifier()\n",
    "\n",
    "    def get_input_shape(self):\n",
    "        data_shape = Transformer.data_shape(method=self.get_data_arrangement(), n_features=self.n_features,\n",
    "                                            segments_size=self.segments_size)\n",
    "        return data_shape[-1]\n",
    "\n",
    "    @staticmethod\n",
    "    def get_data_arrangement():\n",
    "        return \"table\"\n",
    "\n",
    "    def count_params(self):\n",
    "        return -1\n",
    "\n",
    "    def train(self, epochs, X_train, y_train, X_valid, y_valid, X_test, restore_best=True, batch_size=128):\n",
    "        # Change the labels from categorical to one-hot encoding\n",
    "        y_train_onehot = np.asarray(pd.get_dummies(y_train), dtype=np.int8)\n",
    "        y_valid_onehot = np.asarray(pd.get_dummies(y_valid), dtype=np.int8)\n",
    "\n",
    "        self.model.fit(X_train, y_train_onehot)\n",
    "\n",
    "        return self.model.predict(X_test)\n",
    "\n",
    "\n",
    "# Support Vector Machine\n",
    "class SVM():\n",
    "    def __init__(self, classes, n_features,\n",
    "                 segments_size, segments_overlap, decision_size, decision_overlap):\n",
    "        self.classes = classes\n",
    "        self.n_features = n_features\n",
    "        self.segments_size = segments_size\n",
    "        self.input_shape = self.get_input_shape()\n",
    "        self.segments_overlap = segments_overlap\n",
    "        self.decision_size = decision_size\n",
    "        self.decision_overlap = decision_overlap\n",
    "\n",
    "        # Build and compile the model\n",
    "        self.model = LinearSVC()\n",
    "\n",
    "    def get_input_shape(self):\n",
    "        data_shape = Transformer.data_shape(method=self.get_data_arrangement(), n_features=self.n_features,\n",
    "                                            segments_size=self.segments_size)\n",
    "        return data_shape[-1]\n",
    "\n",
    "    @staticmethod\n",
    "    def get_data_arrangement():\n",
    "        return \"table\"\n",
    "\n",
    "    def count_params(self):\n",
    "        return -1\n",
    "\n",
    "    def train(self, epochs, X_train, y_train, X_valid, y_valid, X_test, restore_best=True, batch_size=128):\n",
    "        # Change the labels from categorical to one-hot encoding\n",
    "        y_train_onehot = np.asarray(pd.get_dummies(y_train), dtype=np.int8)\n",
    "        y_valid_onehot = np.asarray(pd.get_dummies(y_valid), dtype=np.int8)\n",
    "        self.model.fit(X_train, y_train)\n",
    "\n",
    "        enc = preprocessing.OneHotEncoder()\n",
    "        enc.fit(y_train.reshape(-1, 1))\n",
    "        return enc.transform(self.model.predict(X_test).reshape(-1, 1)).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2bB8dITyHh5H"
   },
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "jwaxs5pGE05T"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow.keras.losses import MeanSquaredError\n",
    "from sklearn.metrics import accuracy_score, recall_score, f1_score, precision_score\n",
    "\n",
    "def analysis_model(y_pred, y_real_raw):\n",
    "    result = {}\n",
    "    loss_fn = MeanSquaredError()\n",
    "\n",
    "    result['mse_loss'] = loss_fn(np.asarray(pd.get_dummies(y_real_raw), dtype=np.int8),\n",
    "                                         np.asarray(y_pred, dtype=np.float64)).numpy()\n",
    "    y_pred_arg = np.argmax(y_pred, axis=1)\n",
    "    result['accuracy'] = accuracy_score(y_real_raw, y_pred_arg)\n",
    "    result['precision'] = precision_score(y_real_raw, y_pred_arg, average='macro')\n",
    "    result['recall'] = recall_score(y_real_raw, y_pred_arg, average='macro')\n",
    "    result['f1'] = f1_score(y_real_raw, y_pred_arg, average='macro')\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "caa9SgQ8HmsI"
   },
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "8_3iDZgqvCJ1"
   },
   "outputs": [],
   "source": [
    "def train_model(dataset: Dataset, classifier, epochs, batch_size):\n",
    "    y_test_prediction = classifier.train(epochs=epochs,\n",
    "                                         X_train=dataset.X_train,\n",
    "                                         y_train=dataset.y_train,\n",
    "                                         X_valid=dataset.X_valid,\n",
    "                                         y_valid=dataset.y_valid,\n",
    "                                         X_test=dataset.X_test,\n",
    "                                         batch_size=batch_size)\n",
    "\n",
    "    result_test = analysis_model(y_pred=y_test_prediction,\n",
    "                                 y_real_raw=dataset.y_test)\n",
    "\n",
    "    print('Test(%s):%5.2f' % (str(classifier), result_test['accuracy']))\n",
    "\n",
    "    return result_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lS6yHM-MHosi"
   },
   "source": [
    "# Spliting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "8SGLGGQfvHer"
   },
   "outputs": [],
   "source": [
    "def h_block_analyzer(db_path, sample_rate, features, n_classes, noise_rate, segments_time,\n",
    "                     segments_overlap, \n",
    "                     decision_time, decision_overlap,\n",
    "                     classifier, epochs, batch_size, data_length_time,\n",
    "                     n_h_block, n_train_h_block, n_valid_h_block, n_test_h_block, h_moving_step=1):\n",
    "    \"\"\"\n",
    "    :param db_path: the address of dataset directory\n",
    "    :param sample_rate: the sampling rate of signals\n",
    "    :param features: the signals of original data\n",
    "    :param n_classes: the number of classes\n",
    "    :param noise_rate: the rate of noises injected to test data\n",
    "    :param segments_time: the length of each segment in seconds.\n",
    "    :param segments_overlap: the overlap of each segment\n",
    "    :param classifier: the neural network\n",
    "    :param epochs: the number of training epochs\n",
    "    :param batch_size: the number of segments in each batch\n",
    "    :param n_h_block: the number of all hv blocks\n",
    "    :param n_train_h_block: the number of hv blocks to train network\n",
    "    :param n_valid_h_block: the number of hv blocks to validate network\n",
    "    :param n_test_h_block: the number of hv blocks to test network\n",
    "    :param h_moving_step: the number of movement of test and validation blocks in each iteration\n",
    "    :return:\n",
    "    \"\"\"\n",
    "\n",
    "    statistics = {}\n",
    "    add_noise = noise_rate < 100\n",
    "\n",
    "    # Create hv blocks\n",
    "    data_blocks = [i for i in range(n_h_block)]\n",
    "    n_vt = (n_valid_h_block + n_test_h_block)\n",
    "    n_iteration = int((n_h_block - n_vt) / h_moving_step)\n",
    "    for i in range(n_iteration + 1):\n",
    "        print('iteration: %d/%d' % (i + 1, n_iteration + 1))\n",
    "\n",
    "        training_container = data_blocks[0:i] + data_blocks[i + n_vt:n_h_block]\n",
    "        train_blocks = training_container[:n_train_h_block]\n",
    "        valid_blocks = data_blocks[i: i + n_valid_h_block]\n",
    "        test_blocks = data_blocks[i + n_valid_h_block: i + n_vt]\n",
    "\n",
    "        dataset = Dataset(db_path,\n",
    "                          sample_rate,\n",
    "                          features=features,\n",
    "                          window_time=segments_time,\n",
    "                          window_overlap_percentage=segments_overlap,\n",
    "                          decision_time=decision_time,\n",
    "                          decision_overlap_percentage=decision_overlap,\n",
    "                          add_noise=add_noise,\n",
    "                          noise_rate=noise_rate,\n",
    "                          train_blocks=train_blocks,\n",
    "                          valid_blocks=valid_blocks,\n",
    "                          test_blocks=test_blocks,\n",
    "                          data_length_time=data_length_time)\n",
    "\n",
    "        dataset.load_data(n_classes=n_classes, method=classifier.get_data_arrangement())\n",
    "        result = train_model(dataset=dataset, classifier=classifier, epochs=epochs,\n",
    "                             batch_size=batch_size)\n",
    "\n",
    "        for key in result.keys():\n",
    "            if not key in statistics:\n",
    "                statistics[key] = []\n",
    "            statistics[key].append(result[key])\n",
    "        \n",
    "        print(statistics)\n",
    "    return statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yqRhRm3V3SjM"
   },
   "source": [
    "# Save Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "afgTXC083W14"
   },
   "outputs": [],
   "source": [
    "def save_result(log_dir, data: dict):\n",
    "    if not os.path.exists(log_dir):\n",
    "        os.makedirs(log_dir)\n",
    "\n",
    "    # Save to file\n",
    "    with open(log_dir + 'statistics.txt', 'a') as f:\n",
    "        f.write('\\n==========***==========\\n')\n",
    "        f.write(str(data))\n",
    "        f.write('\\n')\n",
    "\n",
    "    csv_file = log_dir + 'statistics.csv'\n",
    "    file_exists = os.path.isfile(csv_file)\n",
    "    try:\n",
    "        with open(csv_file, 'a') as csvfile:\n",
    "            writer = csv.writer(csvfile)\n",
    "            if not file_exists:\n",
    "                writer.writerow(data.keys())\n",
    "            writer.writerow(data.values())\n",
    "            csvfile.close()\n",
    "    except IOError:\n",
    "        print(\"I/O error\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GB5LzSgfHr13"
   },
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "problems = {'DriverIdentification':{},\n",
    "            'ConfLongDemo_JSI':{},\n",
    "            'Healthy_Older_People':{},\n",
    "            'Motor_Failure_Time':{},\n",
    "            'Power_consumption':{},\n",
    "            'PRSA2017':{},\n",
    "            'RSSI':{},\n",
    "            'User_Identification_From_Walking':{},\n",
    "            'WISDM':{},\n",
    "           }\n",
    "problems['DriverIdentification']['dataset'] = './datasets/DriverIdentification/'\n",
    "problems['DriverIdentification']['n_classes'] = 10\n",
    "problems['DriverIdentification']['features'] = ['x-accelerometer', 'y-accelerometer', 'z-accelerometer', 'x-gyroscope', 'y-gyroscope', 'z-gyroscope']\n",
    "problems['DriverIdentification']['sample_rate'] = 2\n",
    "problems['DriverIdentification']['data_length_time'] = -1\n",
    "problems['DriverIdentification']['n_h_block'] = 15\n",
    "problems['DriverIdentification']['n_train_h_block'] = 9\n",
    "problems['DriverIdentification']['n_valid_h_block'] = 2\n",
    "problems['DriverIdentification']['n_test_h_block'] = 4\n",
    "problems['DriverIdentification']['h_moving_step'] = 1\n",
    "problems['DriverIdentification']['decision_times'] = [3,4,5,6,7,8,9,10,30,60,2*60,3*60,4*60,5*60,6*60,7*60,8*60,9*60,10*60]\n",
    "problems['DriverIdentification']['segments_times'] = [3,4,5,6,7,8,9,10,30,60,2*60,3*60,4*60,5*60,6*60,7*60]\n",
    "\n",
    "problems['ConfLongDemo_JSI']['dataset'] = './datasets/ConfLongDemo_JSI/'\n",
    "problems['ConfLongDemo_JSI']['n_classes'] = 5\n",
    "problems['ConfLongDemo_JSI']['features'] = [\"x\", \"y\", \"z\"]\n",
    "problems['ConfLongDemo_JSI']['sample_rate'] = 30\n",
    "problems['ConfLongDemo_JSI']['data_length_time'] = -1\n",
    "problems['ConfLongDemo_JSI']['n_h_block'] = 15\n",
    "problems['ConfLongDemo_JSI']['n_train_h_block'] = 9\n",
    "problems['ConfLongDemo_JSI']['n_valid_h_block'] = 2\n",
    "problems['ConfLongDemo_JSI']['n_test_h_block'] = 4\n",
    "problems['ConfLongDemo_JSI']['h_moving_step'] = 1\n",
    "problems['ConfLongDemo_JSI']['decision_times'] = [3,4,5,6,7,8,9,10,30,60,2*60]\n",
    "problems['ConfLongDemo_JSI']['segments_times'] = [3,4,5,6,7,8,9,10,30,60]\n",
    "\n",
    "problems['Healthy_Older_People']['dataset'] = './datasets/Healthy_Older_People/'\n",
    "problems['Healthy_Older_People']['n_classes'] = 12\n",
    "problems['Healthy_Older_People']['features'] = [\"X\", \"Y\", \"Z\"]\n",
    "problems['Healthy_Older_People']['sample_rate'] = 1\n",
    "problems['Healthy_Older_People']['data_length_time'] = -1\n",
    "problems['Healthy_Older_People']['n_h_block'] = 15\n",
    "problems['Healthy_Older_People']['n_train_h_block'] = 9\n",
    "problems['Healthy_Older_People']['n_valid_h_block'] = 2\n",
    "problems['Healthy_Older_People']['n_test_h_block'] = 4\n",
    "problems['Healthy_Older_People']['h_moving_step'] = 1\n",
    "problems['Healthy_Older_People']['decision_times'] = [3,4,5,6,7,8,9,10,30,60,2*60,3*60]\n",
    "problems['Healthy_Older_People']['segments_times'] = [3,4,5,6,7,8,9,10,30,60,2*60]\n",
    "\n",
    "problems['Motor_Failure_Time']['dataset'] = './datasets/Motor_Failure_Time/'\n",
    "problems['Motor_Failure_Time']['n_classes'] = 3\n",
    "problems['Motor_Failure_Time']['features'] = ['x', 'y', 'z']\n",
    "problems['Motor_Failure_Time']['sample_rate'] = 18\n",
    "problems['Motor_Failure_Time']['data_length_time'] = -1\n",
    "problems['Motor_Failure_Time']['n_h_block'] = 15\n",
    "problems['Motor_Failure_Time']['n_train_h_block'] = 9\n",
    "problems['Motor_Failure_Time']['n_valid_h_block'] = 2\n",
    "problems['Motor_Failure_Time']['n_test_h_block'] = 4\n",
    "problems['Motor_Failure_Time']['h_moving_step'] = 1\n",
    "problems['Motor_Failure_Time']['decision_times'] = [3,4,5,6,7,8,9,10,30,60,2*60,3*60,4*60,5*60,6*60,7*60,8*60,9*60,10*60]\n",
    "problems['Motor_Failure_Time']['segments_times'] = [3,4,5,6,7,8,9,10,30,60,2*60,3*60,4*60,5*60,6*60,7*60]\n",
    "\n",
    "problems['Power_consumption']['dataset'] = './datasets/Power_consumption/'\n",
    "problems['Power_consumption']['n_classes'] = 3\n",
    "problems['Power_consumption']['features'] = ['Temperature', 'Humidity', 'Wind Speed', \n",
    "                                             'general diffuse flows', 'diffuse flows', \n",
    "                                             'Consumption']\n",
    "problems['Power_consumption']['sample_rate'] = 1\n",
    "problems['Power_consumption']['data_length_time'] = -1\n",
    "problems['Power_consumption']['n_h_block'] = 15\n",
    "problems['Power_consumption']['n_train_h_block'] = 9\n",
    "problems['Power_consumption']['n_valid_h_block'] = 2\n",
    "problems['Power_consumption']['n_test_h_block'] = 4\n",
    "problems['Power_consumption']['h_moving_step'] = 1\n",
    "problems['Power_consumption']['decision_times'] = [3,4,5,6,7,8,9,10,30,60,2*60,3*60,4*60,5*60,6*60,7*60,8*60,9*60,10*60,20*60,30*60,40*60]\n",
    "problems['Power_consumption']['segments_times'] = [3,4,5,6,7,8,9,10,30,60,2*60,3*60,4*60,5*60,6*60,7*60,8*60,9*60,10*60,20*60]\n",
    "\n",
    "problems['PRSA2017']['dataset'] = './datasets/PRSA2017/'\n",
    "problems['PRSA2017']['n_classes'] = 12\n",
    "problems['PRSA2017']['features'] = ['PM2.5','PM10','SO2','NO2','CO','O3','TEMP','PRES','DEWP','RAIN','wd','WSPM']\n",
    "problems['PRSA2017']['sample_rate'] = 1\n",
    "problems['PRSA2017']['data_length_time'] = -1\n",
    "problems['PRSA2017']['n_h_block'] = 15\n",
    "problems['PRSA2017']['n_train_h_block'] = 9\n",
    "problems['PRSA2017']['n_valid_h_block'] = 2\n",
    "problems['PRSA2017']['n_test_h_block'] = 4\n",
    "problems['PRSA2017']['h_moving_step'] = 1\n",
    "problems['PRSA2017']['decision_times'] = [3,4,5,6,7,8,9,10,30,60,2*60,3*60,4*60,5*60,6*60,7*60,8*60,9*60,10*60,20*60,30*60,40*60]\n",
    "problems['PRSA2017']['segments_times'] = [3,4,5,6,7,8,9,10,30,60,2*60,3*60,4*60,5*60,6*60,7*60,8*60,9*60,10*60,20*60]\n",
    "\n",
    "problems['RSSI']['dataset'] = './datasets/RSSI/'\n",
    "problems['RSSI']['n_classes'] = 12\n",
    "problems['RSSI']['features'] = ['rssiOne', 'rssiTwo']\n",
    "problems['RSSI']['sample_rate'] = 1\n",
    "problems['RSSI']['data_length_time'] = -1\n",
    "problems['RSSI']['n_h_block'] = 6\n",
    "problems['RSSI']['n_train_h_block'] = 4\n",
    "problems['RSSI']['n_valid_h_block'] = 1\n",
    "problems['RSSI']['n_test_h_block'] = 1\n",
    "problems['RSSI']['h_moving_step'] = 1\n",
    "problems['RSSI']['decision_times'] = [3,4,5,6,7,8,9,10,30,60,2*60]\n",
    "problems['RSSI']['segments_times'] = [3,4,5,6,7,8,9,10,30,60]\n",
    "\n",
    "problems['User_Identification_From_Walking']['dataset'] = './datasets/User_Identification_From_Walking/'\n",
    "problems['User_Identification_From_Walking']['n_classes'] = 13\n",
    "problems['User_Identification_From_Walking']['features'] = [' x acceleration', ' y acceleration', ' z acceleration']\n",
    "problems['User_Identification_From_Walking']['sample_rate'] = 32\n",
    "problems['User_Identification_From_Walking']['data_length_time'] = -1\n",
    "problems['User_Identification_From_Walking']['n_h_block'] = 10\n",
    "problems['User_Identification_From_Walking']['n_train_h_block'] = 5\n",
    "problems['User_Identification_From_Walking']['n_valid_h_block'] = 2\n",
    "problems['User_Identification_From_Walking']['n_test_h_block'] = 3\n",
    "problems['User_Identification_From_Walking']['h_moving_step'] = 1\n",
    "problems['User_Identification_From_Walking']['decision_times'] = [3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24]\n",
    "problems['User_Identification_From_Walking']['segments_times'] = [3,4,5,6,7,8,9,10,11,12,13,14,15,16,17]\n",
    "\n",
    "problems['WISDM']['dataset'] = './datasets/WISDM/'\n",
    "problems['WISDM']['n_classes'] = 10\n",
    "problems['WISDM']['features'] = ['X-accel', 'Y-accel', 'Z-accel']\n",
    "problems['WISDM']['sample_rate'] = 20\n",
    "problems['WISDM']['data_length_time'] = -1\n",
    "problems['WISDM']['n_h_block'] = 15\n",
    "problems['WISDM']['n_train_h_block'] = 9\n",
    "problems['WISDM']['n_valid_h_block'] = 4\n",
    "problems['WISDM']['n_test_h_block'] = 2\n",
    "problems['WISDM']['h_moving_step'] = 1\n",
    "problems['WISDM']['decision_times'] = [3,4,5,6,7,8,9,10,30,60,2*60,3*60,4*60,5*60,6*60,7*60,8*60,9*60,10*60]\n",
    "problems['WISDM']['segments_times'] = [3,4,5,6,7,8,9,10,30,60,2*60,3*60,4*60,5*60,6*60,7*60]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise_rate = 100\n",
    "epochs = 50\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "problem = 'Healthy_Older_People'\n",
    "dataset = problems[problem]['dataset']\n",
    "n_classes = problems[problem]['n_classes']\n",
    "features = problems[problem]['features']\n",
    "sample_rate = problems[problem]['sample_rate']\n",
    "data_length_time = problems[problem]['data_length_time']\n",
    "n_h_block = problems[problem]['n_h_block']\n",
    "n_train_h_block = problems[problem]['n_train_h_block']\n",
    "n_valid_h_block = problems[problem]['n_valid_h_block']\n",
    "n_test_h_block = problems[problem]['n_test_h_block']\n",
    "h_moving_step = problems[problem]['h_moving_step']\n",
    "decision_times = problems[problem]['decision_times']\n",
    "segments_times = problems[problem]['segments_times']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classic Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lyNTFQNg8wQm",
    "outputId": "442a9edb-413b-4538-f53b-f1d6d8032499",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# models = ['KNN','MLP','LR','RF','SVM','CNN_L']\n",
    "# # 'CNN_L','MLP','KNN','LR','RF','SVM'\n",
    "# decision_time = 0\n",
    "# decision_overlap = 0\n",
    "# log_dir = f\"./comparisons/log/{problem}/classic/\"\n",
    "\n",
    "# for model in models:\n",
    "#     for segments_time in decision_times:\n",
    "#         for segments_overlap in [0.0]:\n",
    "#             classifier = eval(model)(classes=n_classes,\n",
    "#                                      n_features=len(features),\n",
    "#                                      segments_size=int(segments_time * sample_rate),\n",
    "#                                      segments_overlap=segments_overlap,\n",
    "#                                      decision_size=int(decision_time * sample_rate),\n",
    "#                                      decision_overlap=decision_overlap)\n",
    "\n",
    "#             # cross-validation\n",
    "#             start = datetime.now()\n",
    "#             statistics = h_block_analyzer(db_path=dataset,\n",
    "#                                           sample_rate=sample_rate,\n",
    "#                                           features=features,\n",
    "#                                           n_classes=n_classes,\n",
    "#                                           noise_rate=noise_rate,\n",
    "#                                           segments_time=segments_time,\n",
    "#                                           segments_overlap=segments_overlap,\n",
    "#                                           decision_time=decision_time,\n",
    "#                                           decision_overlap=decision_overlap,\n",
    "#                                           classifier=classifier,\n",
    "#                                           epochs=epochs,\n",
    "#                                           batch_size=batch_size,\n",
    "#                                           data_length_time=data_length_time,\n",
    "#                                           n_h_block=n_h_block,\n",
    "#                                           n_train_h_block=n_train_h_block,\n",
    "#                                           n_valid_h_block=n_valid_h_block,\n",
    "#                                           n_test_h_block=n_test_h_block,\n",
    "#                                           h_moving_step=h_moving_step)\n",
    "#             end = datetime.now()\n",
    "#             running_time = end - start\n",
    "\n",
    "#             # Summarizing the results of cross-validation\n",
    "#             data = {}\n",
    "#             data['dataset'] = dataset\n",
    "#             data['class'] = str(n_classes)\n",
    "#             data['features'] = str(features)\n",
    "#             data['sample_rate'] = str(sample_rate)\n",
    "#             data['noise_rate'] = str(noise_rate)\n",
    "#             data['epochs'] = str(epochs)\n",
    "#             data['batch_size'] = str(batch_size)\n",
    "#             data['data_length_time'] = str(data_length_time)\n",
    "#             data['n_h_block'] = str(n_h_block)\n",
    "#             data['n_train_h_block'] = str(n_train_h_block)\n",
    "#             data['n_valid_h_block'] = str(n_valid_h_block)\n",
    "#             data['n_test_h_block'] = str(n_test_h_block)\n",
    "#             data['h_moving_step'] = str(h_moving_step)\n",
    "#             data['segments_time'] = str(segments_time)\n",
    "#             data['segments_overlap'] = str(segments_overlap)\n",
    "#             data['inner_classifier'] = str(model)\n",
    "#             data['datetime'] = datetime.now().strftime(\"%Y:%m:%d %H:%M:%S\")\n",
    "#             data['running_time'] = str(running_time.seconds) + \" seconds\"\n",
    "#             data['n_params'] = classifier.count_params()\n",
    "#             data['segments_time'] = timedelta(seconds=int(segments_time))\n",
    "#             data['segments_overlap'] = segments_overlap\n",
    "#             data['decision_time'] = timedelta(seconds=int(decision_time))\n",
    "#             data['decision_overlap'] = decision_overlap\n",
    "#             statistics_summary = {}\n",
    "#             for key in statistics.keys():\n",
    "#                 statistics_summary[key + '_mean'] = np.average(statistics[key])\n",
    "#                 statistics_summary[key + '_std'] = np.std(statistics[key])\n",
    "#                 statistics_summary[key + '_max'] = np.max(statistics[key])\n",
    "#                 statistics_summary[key + '_min'] = np.min(statistics[key])\n",
    "#             data.update(statistics_summary)\n",
    "#             # Save information\n",
    "#             save_result(log_dir=log_dir, data=data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset: WISDM model: GRU dl: 00:00:04 and wl:00:00:03 is investigated!\n",
      "dataset: WISDM model: GRU dl: 00:00:05 and wl:00:00:03 is investigated!\n",
      "dataset: WISDM model: GRU dl: 00:00:06 and wl:00:00:03 is investigated!\n",
      "dataset: WISDM model: GRU dl: 00:00:06 and wl:00:00:04 is investigated!\n",
      "dataset: WISDM model: GRU dl: 00:00:07 and wl:00:00:03 is investigated!\n",
      "dataset: WISDM model: GRU dl: 00:00:07 and wl:00:00:04 is investigated!\n",
      "dataset: WISDM model: GRU dl: 00:00:07 and wl:00:00:05 is investigated!\n",
      "dataset: WISDM model: GRU dl: 00:00:08 and wl:00:00:03 is investigated!\n",
      "dataset: WISDM model: GRU dl: 00:00:08 and wl:00:00:04 is investigated!\n",
      "dataset: WISDM model: GRU dl: 00:00:08 and wl:00:00:05 is investigated!\n",
      "dataset: WISDM model: GRU dl: 00:00:08 and wl:00:00:06 is investigated!\n",
      "dataset: WISDM model: GRU dl: 00:00:09 and wl:00:00:03 is investigated!\n",
      "dataset: WISDM model: GRU dl: 00:00:09 and wl:00:00:04 is investigated!\n",
      "dataset: WISDM model: GRU dl: 00:00:09 and wl:00:00:05 is investigated!\n",
      "dataset: WISDM model: GRU dl: 00:00:09 and wl:00:00:06 is investigated!\n",
      "dataset: WISDM model: GRU dl: 00:00:10 and wl:00:00:03 is investigated!\n",
      "dataset: WISDM model: GRU dl: 00:00:10 and wl:00:00:04 is investigated!\n",
      "dataset: WISDM model: GRU dl: 00:00:10 and wl:00:00:05 is investigated!\n",
      "dataset: WISDM model: GRU dl: 00:00:10 and wl:00:00:06 is investigated!\n",
      "dataset: WISDM model: GRU dl: 00:00:10 and wl:00:00:07 is investigated!\n",
      "dataset: WISDM model: GRU dl: 00:00:30 and wl:00:00:03 is investigated!\n",
      "dataset: WISDM model: GRU dl: 00:00:30 and wl:00:00:04 is investigated!\n",
      "dataset: WISDM model: GRU dl: 00:00:30 and wl:00:00:05 is investigated!\n",
      "dataset: WISDM model: GRU dl: 00:00:30 and wl:00:00:06 is investigated!\n",
      "dataset: WISDM model: GRU dl: 00:00:30 and wl:00:00:07 is investigated!\n",
      "dataset: WISDM model: GRU dl: 00:00:30 and wl:00:00:08 is investigated!\n",
      "dataset: WISDM model: GRU dl: 00:00:30 and wl:00:00:09 is investigated!\n",
      "dataset: WISDM model: GRU dl: 00:00:30 and wl:00:00:10 is investigated!\n",
      "dataset: WISDM model: GRU dl: 00:01:00 and wl:00:00:03 is investigated!\n",
      "dataset: WISDM model: GRU dl: 00:01:00 and wl:00:00:04 is investigated!\n",
      "dataset: WISDM model: GRU dl: 00:01:00 and wl:00:00:05 is investigated!\n",
      "dataset: WISDM model: GRU dl: 00:01:00 and wl:00:00:06 is investigated!\n",
      "dataset: WISDM model: GRU dl: 00:01:00 and wl:00:00:07 is investigated!\n",
      "dataset: WISDM model: GRU dl: 00:01:00 and wl:00:00:08 is investigated!\n",
      "dataset: WISDM model: GRU dl: 00:01:00 and wl:00:00:09 is investigated!\n",
      "dataset: WISDM model: GRU dl: 00:01:00 and wl:00:00:10 is investigated!\n",
      "dataset: WISDM model: GRU dl: 00:01:00 and wl:00:00:30 is investigated!\n",
      "dataset: WISDM model: GRU dl: 00:02:00 and wl:00:00:03 is investigated!\n",
      "dataset: WISDM model: GRU dl: 00:02:00 and wl:00:00:04 is investigated!\n",
      "dataset: WISDM model: GRU dl: 00:02:00 and wl:00:00:05 is investigated!\n",
      "dataset: WISDM model: GRU dl: 00:02:00 and wl:00:00:06 is investigated!\n",
      "dataset: WISDM model: GRU dl: 00:02:00 and wl:00:00:07 is investigated!\n",
      "dataset: WISDM model: GRU dl: 00:02:00 and wl:00:00:08 is investigated!\n",
      "dataset: WISDM model: GRU dl: 00:02:00 and wl:00:00:09 is investigated!\n",
      "dataset: WISDM model: GRU dl: 00:02:00 and wl:00:00:10 is investigated!\n",
      "dataset: WISDM model: GRU dl: 00:02:00 and wl:00:00:30 is investigated!\n",
      "dataset: WISDM model: GRU dl: 00:02:00 and wl:00:01:00 is investigated!\n",
      "dataset: WISDM model: GRU dl: 00:03:00 and wl:00:00:03 is investigated!\n",
      "dataset: WISDM model: GRU dl: 00:03:00 and wl:00:00:04 is investigated!\n",
      "dataset: WISDM model: GRU dl: 00:03:00 and wl:00:00:05 is investigated!\n",
      "dataset: WISDM model: GRU dl: 00:03:00 and wl:00:00:06 is investigated!\n",
      "dataset: WISDM model: GRU dl: 00:03:00 and wl:00:00:07 is investigated!\n",
      "dataset: WISDM model: GRU dl: 00:03:00 and wl:00:00:08 is investigated!\n",
      "dataset: WISDM model: GRU dl: 00:03:00 and wl:00:00:09 is investigated!\n",
      "dataset: WISDM model: GRU dl: 00:03:00 and wl:00:00:10 is investigated!\n",
      "dataset: WISDM model: GRU dl: 00:03:00 and wl:00:00:30 is investigated!\n",
      "dataset: WISDM model: GRU dl: 00:03:00 and wl:00:01:00 is investigated!\n",
      "dataset: WISDM model: GRU dl: 00:03:00 and wl:00:02:00 is investigated!\n",
      "dataset: WISDM model: GRU dl: 00:04:00 and wl:00:00:03 is investigated!\n",
      "dataset: WISDM model: GRU dl: 00:04:00 and wl:00:00:04 is investigated!\n",
      "dataset: WISDM model: GRU dl: 00:04:00 and wl:00:00:05 is investigated!\n",
      "dataset: WISDM model: GRU dl: 00:04:00 and wl:00:00:06 is investigated!\n",
      "dataset: WISDM model: GRU dl: 00:04:00 and wl:00:00:07 is investigated!\n",
      "dataset: WISDM model: GRU dl: 00:04:00 and wl:00:00:08 is investigated!\n",
      "dataset: WISDM model: GRU dl: 00:04:00 and wl:00:00:09 is investigated!\n",
      "dataset: WISDM model: GRU dl: 00:04:00 and wl:00:00:10 is investigated!\n",
      "dataset: WISDM model: GRU dl: 00:04:00 and wl:00:00:30 is investigated!\n",
      "dataset: WISDM model: GRU dl: 00:04:00 and wl:00:01:00 is investigated!\n",
      "dataset: WISDM model: GRU dl: 00:04:00 and wl:00:02:00 is investigated!\n",
      "dataset: WISDM model: GRU dl: 00:04:00 and wl:00:03:00 is investigated!\n",
      "dataset: WISDM model: GRU dl: 00:05:00 and wl:00:00:03 is investigated!\n",
      "dataset: WISDM model: GRU dl: 00:05:00 and wl:00:00:04 is investigated!\n",
      "dataset: WISDM model: GRU dl: 00:05:00 and wl:00:00:05 is investigated!\n",
      "dataset: WISDM model: GRU dl: 00:05:00 and wl:00:00:06 is investigated!\n",
      "dataset: WISDM model: GRU dl: 00:05:00 and wl:00:00:07 is investigated!\n",
      "dataset: WISDM model: GRU dl: 00:05:00 and wl:00:00:08 is investigated!\n",
      "dataset: WISDM model: GRU dl: 00:05:00 and wl:00:00:09 is investigated!\n",
      "dataset: WISDM model: GRU dl: 00:05:00 and wl:00:00:10 is investigated!\n",
      "dataset: WISDM model: GRU dl: 00:05:00 and wl:00:00:30 is investigated!\n",
      "dataset: WISDM model: GRU dl: 00:05:00 and wl:00:01:00 is investigated!\n",
      "dataset: WISDM model: GRU dl: 00:05:00 and wl:00:02:00 is investigated!\n",
      "dataset: WISDM model: GRU dl: 00:05:00 and wl:00:03:00 is investigated!\n",
      "dataset: WISDM model: GRU dl: 00:06:00 and wl:00:00:03 is investigated!\n",
      "dataset: WISDM model: GRU dl: 00:06:00 and wl:00:00:04 is investigated!\n",
      "dataset: WISDM model: GRU dl: 00:06:00 and wl:00:00:05 is investigated!\n",
      "dataset: WISDM model: GRU dl: 00:06:00 and wl:00:00:06 is investigated!\n",
      "dataset: WISDM model: GRU dl: 00:06:00 and wl:00:00:07 is investigated!\n",
      "dataset: WISDM model: GRU dl: 00:06:00 and wl:00:00:08 is investigated!\n",
      "dataset: WISDM model: GRU dl: 00:06:00 and wl:00:00:09 is investigated!\n",
      "dataset: WISDM model: GRU dl: 00:06:00 and wl:00:00:10 is investigated!\n",
      "dataset: WISDM model: GRU dl: 00:06:00 and wl:00:00:30 is investigated!\n",
      "dataset: WISDM model: GRU dl: 00:06:00 and wl:00:01:00 is investigated!\n",
      "dataset: WISDM model: GRU dl: 00:06:00 and wl:00:02:00 is investigated!\n",
      "dataset: WISDM model: GRU dl: 00:06:00 and wl:00:03:00 is investigated!\n",
      "dataset: WISDM model: GRU dl: 00:06:00 and wl:00:04:00 is investigated!\n",
      "dataset: WISDM model: GRU dl: 00:07:00 and wl:00:00:03 is investigated!\n",
      "dataset: WISDM model: GRU dl: 00:07:00 and wl:00:00:04 is investigated!\n",
      "dataset: WISDM model: GRU dl: 00:07:00 and wl:00:00:05 is investigated!\n",
      "dataset: WISDM model: GRU dl: 00:07:00 and wl:00:00:06 is investigated!\n",
      "dataset: WISDM model: GRU dl: 00:07:00 and wl:00:00:07 is investigated!\n",
      "dataset: WISDM model: GRU dl: 00:07:00 and wl:00:00:08 is investigated!\n",
      "dataset: WISDM model: GRU dl: 00:07:00 and wl:00:00:09 is investigated!\n",
      "dataset: WISDM model: GRU dl: 00:07:00 and wl:00:00:10 is investigated!\n",
      "dataset: WISDM model: GRU dl: 00:07:00 and wl:00:00:30 is investigated!\n",
      "dataset: WISDM model: GRU dl: 00:07:00 and wl:00:01:00 is investigated!\n",
      "dataset: WISDM model: GRU dl: 00:07:00 and wl:00:02:00 is investigated!\n",
      "dataset: WISDM model: GRU dl: 00:07:00 and wl:00:03:00 is investigated!\n",
      "dataset: WISDM model: GRU dl: 00:07:00 and wl:00:04:00 is investigated!\n",
      "dataset: WISDM model: GRU dl: 00:07:00 and wl:00:05:00 is investigated!\n",
      "dataset: WISDM model: GRU dl: 00:08:00 and wl:00:00:03 is investigated!\n",
      "dataset: WISDM model: GRU dl: 00:08:00 and wl:00:00:04 is investigated!\n",
      "dataset: WISDM model: GRU dl: 00:08:00 and wl:00:00:05 is investigated!\n",
      "dataset: WISDM model: GRU dl: 00:08:00 and wl:00:00:06 is investigated!\n",
      "dataset: WISDM model: GRU dl: 00:08:00 and wl:00:00:07 is investigated!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset: WISDM model: GRU dl: 00:08:00 and wl:00:00:08 is investigated!\n",
      "dataset: WISDM model: GRU dl: 00:08:00 and wl:00:00:09 is investigated!\n",
      "dataset: WISDM model: GRU dl: 00:08:00 and wl:00:00:10 is investigated!\n",
      "dataset: WISDM model: GRU dl: 00:08:00 and wl:00:00:30 is investigated!\n",
      "dataset: WISDM model: GRU dl: 00:08:00 and wl:00:01:00 is investigated!\n",
      "dataset: WISDM model: GRU dl: 00:08:00 and wl:00:02:00 is investigated!\n",
      "dataset: WISDM model: GRU dl: 00:08:00 and wl:00:03:00 is investigated!\n",
      "dataset: WISDM model: GRU dl: 00:08:00 and wl:00:04:00 is investigated!\n",
      "dataset: WISDM model: GRU dl: 00:08:00 and wl:00:05:00 is investigated!\n",
      "dataset: WISDM model: GRU dl: 00:08:00 and wl:00:06:00 is investigated!\n",
      "dataset: WISDM model: GRU dl: 00:09:00 and wl:00:00:03 is investigated!\n",
      "dataset: WISDM model: GRU dl: 00:09:00 and wl:00:00:04 is investigated!\n",
      "dataset: WISDM model: GRU dl: 00:09:00 and wl:00:00:05 is investigated!\n",
      "dataset: WISDM model: GRU dl: 00:09:00 and wl:00:00:06 is investigated!\n",
      "dataset: WISDM model: GRU dl: 00:09:00 and wl:00:00:07 is investigated!\n",
      "dataset: WISDM model: GRU dl: 00:09:00 and wl:00:00:08 is investigated!\n",
      "dataset: WISDM model: GRU dl: 00:09:00 and wl:00:00:09 is investigated!\n",
      "dataset: WISDM model: GRU dl: 00:09:00 and wl:00:00:10 is investigated!\n",
      "dataset: WISDM model: GRU dl: 00:09:00 and wl:00:00:30 is investigated!\n",
      "dataset: WISDM model: GRU dl: 00:09:00 and wl:00:01:00 is investigated!\n",
      "dataset: WISDM model: GRU dl: 00:09:00 and wl:00:02:00 is investigated!\n",
      "dataset: WISDM model: GRU dl: 00:09:00 and wl:00:03:00 is investigated!\n",
      "dataset: WISDM model: GRU dl: 00:09:00 and wl:00:04:00 is investigated!\n",
      "dataset: WISDM model: GRU dl: 00:09:00 and wl:00:05:00 is investigated!\n",
      "dataset: WISDM model: GRU dl: 00:09:00 and wl:00:06:00 is investigated!\n",
      "dataset: WISDM model: GRU dl: 0:10:00 and wl:0:00:03 is investigated!\n",
      "dataset: WISDM model: GRU dl: 0:10:00 and wl:0:00:04 is investigated!\n",
      "dataset: WISDM model: GRU dl: 0:10:00 and wl:0:00:05 is investigated!\n",
      "dataset: WISDM model: GRU dl: 0:10:00 and wl:0:00:06 is investigated!\n",
      "dataset: WISDM model: GRU dl: 0:10:00 and wl:0:00:07 is investigated!\n",
      "dataset: WISDM model: GRU dl: 0:10:00 and wl:0:00:08 is investigated!\n",
      "dataset: WISDM model: GRU dl: 0:10:00 and wl:0:00:09 is investigated!\n",
      "dataset: WISDM model: GRU dl: 0:10:00 and wl:0:00:10 is investigated!\n",
      "dataset: WISDM model: GRU dl: 0:10:00 and wl:0:00:30 is investigated!\n",
      "dataset: WISDM model: GRU dl: 0:10:00 and wl:0:01:00 is investigated!\n",
      "dataset: WISDM model: GRU dl: 0:10:00 and wl:0:02:00 is investigated!\n",
      "dataset: WISDM model: GRU dl: 0:10:00 and wl:0:03:00 is investigated!\n",
      "dataset: WISDM model: GRU dl: 0:10:00 and wl:0:04:00 is investigated!\n",
      "dataset: WISDM model: GRU dl: 0:10:00 and wl:0:05:00 is investigated!\n",
      "dataset: WISDM model: GRU dl: 0:10:00 and wl:0:06:00 is investigated!\n",
      "dataset: WISDM model: GRU dl: 0:10:00 and wl:0:07:00 is investigated!\n",
      "dataset: WISDM model: LSTM dl: 00:00:04 and wl:00:00:03 is investigated!\n",
      "dataset: WISDM model: LSTM dl: 00:00:05 and wl:00:00:03 is investigated!\n",
      "dataset: WISDM model: LSTM dl: 00:00:06 and wl:00:00:03 is investigated!\n",
      "dataset: WISDM model: LSTM dl: 00:00:06 and wl:00:00:04 is investigated!\n",
      "dataset: WISDM model: LSTM dl: 00:00:07 and wl:00:00:03 is investigated!\n",
      "dataset: WISDM model: LSTM dl: 00:00:07 and wl:00:00:04 is investigated!\n",
      "dataset: WISDM model: LSTM dl: 00:00:07 and wl:00:00:05 is investigated!\n",
      "dataset: WISDM model: LSTM dl: 00:00:08 and wl:00:00:03 is investigated!\n",
      "dataset: WISDM model: LSTM dl: 00:00:08 and wl:00:00:04 is investigated!\n",
      "dataset: WISDM model: LSTM dl: 00:00:08 and wl:00:00:05 is investigated!\n",
      "dataset: WISDM model: LSTM dl: 00:00:08 and wl:00:00:06 is investigated!\n",
      "dataset: WISDM model: LSTM dl: 00:00:09 and wl:00:00:03 is investigated!\n",
      "dataset: WISDM model: LSTM dl: 00:00:09 and wl:00:00:04 is investigated!\n",
      "dataset: WISDM model: LSTM dl: 00:00:09 and wl:00:00:05 is investigated!\n",
      "dataset: WISDM model: LSTM dl: 00:00:09 and wl:00:00:06 is investigated!\n",
      "dataset: WISDM model: LSTM dl: 00:00:10 and wl:00:00:03 is investigated!\n",
      "dataset: WISDM model: LSTM dl: 00:00:10 and wl:00:00:04 is investigated!\n",
      "dl: 0:00:10 and wl: 0:00:05\n",
      "(5, 300)\n",
      "Metal device set to: Apple M1 Pro\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-25 05:04:12.118833: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:306] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2023-03-25 05:04:12.118863: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:272] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 1/10\n",
      "./datasets/WISDM/segments/method rnn_2d/wl 100/wo 75/dl 200/do 0/train [6, 7, 8, 9, 10, 11, 12, 13, 14]/valid [0, 1, 2, 3]/test [4, 5]/\n",
      "Dataset is already existed!\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-25 05:04:12.434370: W tensorflow/tsl/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n",
      "2023-03-25 05:04:13.250728: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n",
      "2023-03-25 05:04:13.341176: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n",
      "2023-03-25 05:04:13.444664: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "237/237 [==============================] - ETA: 0s - loss: 0.9035 - accuracy: 0.6801"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-25 05:04:16.520709: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n",
      "2023-03-25 05:04:16.560477: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "237/237 [==============================] - 5s 16ms/step - loss: 0.9035 - accuracy: 0.6801 - val_loss: 1.3514 - val_accuracy: 0.4900\n",
      "Epoch 2/50\n",
      "237/237 [==============================] - 4s 15ms/step - loss: 0.6258 - accuracy: 0.7668 - val_loss: 1.4270 - val_accuracy: 0.5127\n",
      "Epoch 3/50\n",
      "237/237 [==============================] - 4s 15ms/step - loss: 0.5444 - accuracy: 0.7945 - val_loss: 1.4468 - val_accuracy: 0.5022\n",
      "Epoch 4/50\n",
      "237/237 [==============================] - 3s 15ms/step - loss: 0.4964 - accuracy: 0.8141 - val_loss: 1.4717 - val_accuracy: 0.5168\n",
      "Epoch 5/50\n",
      "237/237 [==============================] - 4s 16ms/step - loss: 0.4577 - accuracy: 0.8193 - val_loss: 1.5429 - val_accuracy: 0.5109\n",
      "Epoch 6/50\n",
      "237/237 [==============================] - 4s 15ms/step - loss: 0.4199 - accuracy: 0.8434 - val_loss: 1.5540 - val_accuracy: 0.5377\n",
      "Epoch 7/50\n",
      "237/237 [==============================] - 3s 15ms/step - loss: 0.4045 - accuracy: 0.8487 - val_loss: 1.6182 - val_accuracy: 0.5001\n",
      "Epoch 8/50\n",
      "237/237 [==============================] - 4s 15ms/step - loss: 0.3774 - accuracy: 0.8601 - val_loss: 1.6109 - val_accuracy: 0.5234\n",
      "Epoch 9/50\n",
      "237/237 [==============================] - 4s 15ms/step - loss: 0.3623 - accuracy: 0.8616 - val_loss: 1.6897 - val_accuracy: 0.5147\n",
      "Epoch 10/50\n",
      "237/237 [==============================] - 4s 15ms/step - loss: 0.3427 - accuracy: 0.8729 - val_loss: 1.7695 - val_accuracy: 0.4930\n",
      "Epoch 11/50\n",
      "237/237 [==============================] - 3s 15ms/step - loss: 0.3350 - accuracy: 0.8712 - val_loss: 1.7763 - val_accuracy: 0.5058\n",
      "Epoch 12/50\n",
      "237/237 [==============================] - 3s 15ms/step - loss: 0.3172 - accuracy: 0.8832 - val_loss: 1.7424 - val_accuracy: 0.5210\n",
      "Epoch 13/50\n",
      "237/237 [==============================] - 3s 15ms/step - loss: 0.3099 - accuracy: 0.8853 - val_loss: 1.7079 - val_accuracy: 0.5368\n",
      "Epoch 14/50\n",
      "237/237 [==============================] - 3s 15ms/step - loss: 0.2956 - accuracy: 0.8911 - val_loss: 1.8503 - val_accuracy: 0.5067\n",
      "Epoch 15/50\n",
      "237/237 [==============================] - 3s 15ms/step - loss: 0.2835 - accuracy: 0.8955 - val_loss: 1.8211 - val_accuracy: 0.5284\n",
      "Epoch 16/50\n",
      "237/237 [==============================] - 4s 15ms/step - loss: 0.2776 - accuracy: 0.8967 - val_loss: 1.9538 - val_accuracy: 0.5314\n",
      "Epoch 17/50\n",
      "237/237 [==============================] - 4s 15ms/step - loss: 0.2758 - accuracy: 0.8982 - val_loss: 1.9124 - val_accuracy: 0.5162\n",
      "Epoch 18/50\n",
      "237/237 [==============================] - 3s 15ms/step - loss: 0.2668 - accuracy: 0.9037 - val_loss: 1.9565 - val_accuracy: 0.5153\n",
      "Epoch 19/50\n",
      "237/237 [==============================] - 4s 15ms/step - loss: 0.2605 - accuracy: 0.9013 - val_loss: 1.9018 - val_accuracy: 0.5356\n",
      "Epoch 20/50\n",
      "237/237 [==============================] - 4s 16ms/step - loss: 0.2439 - accuracy: 0.9108 - val_loss: 2.0062 - val_accuracy: 0.5347\n",
      "Epoch 21/50\n",
      "237/237 [==============================] - 4s 15ms/step - loss: 0.2398 - accuracy: 0.9144 - val_loss: 1.9581 - val_accuracy: 0.5380\n",
      "Epoch 22/50\n",
      "237/237 [==============================] - 4s 15ms/step - loss: 0.2402 - accuracy: 0.9107 - val_loss: 2.0281 - val_accuracy: 0.5133\n",
      "Epoch 23/50\n",
      "237/237 [==============================] - 4s 15ms/step - loss: 0.2325 - accuracy: 0.9150 - val_loss: 2.0587 - val_accuracy: 0.5019\n",
      "Epoch 24/50\n",
      "237/237 [==============================] - 4s 15ms/step - loss: 0.2290 - accuracy: 0.9174 - val_loss: 2.0468 - val_accuracy: 0.5341\n",
      "Epoch 25/50\n",
      "237/237 [==============================] - 4s 15ms/step - loss: 0.2213 - accuracy: 0.9241 - val_loss: 2.0142 - val_accuracy: 0.5332\n",
      "Epoch 26/50\n",
      "237/237 [==============================] - 4s 15ms/step - loss: 0.2159 - accuracy: 0.9194 - val_loss: 2.2631 - val_accuracy: 0.5156\n",
      "Epoch 27/50\n",
      "237/237 [==============================] - 4s 15ms/step - loss: 0.2193 - accuracy: 0.9210 - val_loss: 2.2354 - val_accuracy: 0.5004\n",
      "Epoch 28/50\n",
      "237/237 [==============================] - 4s 15ms/step - loss: 0.2020 - accuracy: 0.9305 - val_loss: 2.2763 - val_accuracy: 0.4903\n",
      "Epoch 29/50\n",
      "237/237 [==============================] - 4s 15ms/step - loss: 0.2050 - accuracy: 0.9311 - val_loss: 2.1245 - val_accuracy: 0.5398\n",
      "Epoch 30/50\n",
      "237/237 [==============================] - 4s 15ms/step - loss: 0.1990 - accuracy: 0.9302 - val_loss: 2.2080 - val_accuracy: 0.4987\n",
      "Epoch 31/50\n",
      "237/237 [==============================] - 4s 15ms/step - loss: 0.2013 - accuracy: 0.9325 - val_loss: 2.1957 - val_accuracy: 0.5308\n",
      "Epoch 32/50\n",
      "237/237 [==============================] - 4s 15ms/step - loss: 0.1971 - accuracy: 0.9319 - val_loss: 2.1950 - val_accuracy: 0.5314\n",
      "Epoch 33/50\n",
      "237/237 [==============================] - 4s 15ms/step - loss: 0.1882 - accuracy: 0.9352 - val_loss: 2.2462 - val_accuracy: 0.5222\n",
      "Epoch 34/50\n",
      "237/237 [==============================] - 4s 15ms/step - loss: 0.1944 - accuracy: 0.9306 - val_loss: 2.2332 - val_accuracy: 0.5579\n",
      "Epoch 35/50\n",
      "237/237 [==============================] - 4s 15ms/step - loss: 0.1767 - accuracy: 0.9380 - val_loss: 2.3904 - val_accuracy: 0.5094\n",
      "Epoch 36/50\n",
      "237/237 [==============================] - 4s 15ms/step - loss: 0.1823 - accuracy: 0.9352 - val_loss: 2.2889 - val_accuracy: 0.5377\n",
      "Epoch 37/50\n",
      "237/237 [==============================] - 4s 15ms/step - loss: 0.1818 - accuracy: 0.9362 - val_loss: 2.3621 - val_accuracy: 0.5165\n",
      "Epoch 38/50\n",
      "237/237 [==============================] - 4s 15ms/step - loss: 0.1773 - accuracy: 0.9392 - val_loss: 2.4399 - val_accuracy: 0.5136\n",
      "Epoch 39/50\n",
      "237/237 [==============================] - 4s 15ms/step - loss: 0.1697 - accuracy: 0.9391 - val_loss: 2.3952 - val_accuracy: 0.5383\n",
      "Epoch 40/50\n",
      "237/237 [==============================] - 4s 15ms/step - loss: 0.1692 - accuracy: 0.9401 - val_loss: 2.4570 - val_accuracy: 0.5156\n",
      "Epoch 41/50\n",
      "237/237 [==============================] - 4s 15ms/step - loss: 0.1680 - accuracy: 0.9399 - val_loss: 2.4048 - val_accuracy: 0.5216\n",
      "Epoch 42/50\n",
      "237/237 [==============================] - 4s 15ms/step - loss: 0.1600 - accuracy: 0.9419 - val_loss: 2.3327 - val_accuracy: 0.5332\n",
      "Epoch 43/50\n",
      "237/237 [==============================] - 4s 15ms/step - loss: 0.1563 - accuracy: 0.9465 - val_loss: 2.3814 - val_accuracy: 0.5267\n",
      "Epoch 44/50\n",
      "237/237 [==============================] - 4s 15ms/step - loss: 0.1584 - accuracy: 0.9437 - val_loss: 2.4133 - val_accuracy: 0.5314\n",
      "Epoch 45/50\n",
      "237/237 [==============================] - 4s 15ms/step - loss: 0.1473 - accuracy: 0.9494 - val_loss: 2.4792 - val_accuracy: 0.5356\n",
      "Epoch 46/50\n",
      "237/237 [==============================] - 4s 15ms/step - loss: 0.1456 - accuracy: 0.9481 - val_loss: 2.3947 - val_accuracy: 0.5430\n",
      "Epoch 47/50\n",
      "237/237 [==============================] - 4s 15ms/step - loss: 0.1442 - accuracy: 0.9496 - val_loss: 2.5213 - val_accuracy: 0.5213\n",
      "Epoch 48/50\n",
      "237/237 [==============================] - 4s 15ms/step - loss: 0.1408 - accuracy: 0.9498 - val_loss: 2.3953 - val_accuracy: 0.5344\n",
      "Epoch 49/50\n",
      "237/237 [==============================] - 4s 15ms/step - loss: 0.1383 - accuracy: 0.9512 - val_loss: 2.4335 - val_accuracy: 0.5439\n",
      "Epoch 50/50\n",
      "237/237 [==============================] - 4s 15ms/step - loss: 0.1438 - accuracy: 0.9498 - val_loss: 2.5182 - val_accuracy: 0.5344\n",
      "35/53 [==================>...........] - ETA: 0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-25 05:07:11.245630: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n",
      "2023-03-25 05:07:11.275299: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53/53 [==============================] - 0s 3ms/step\n",
      "Test(<__main__.LSTM object at 0x168e23610>): 0.60\n",
      "{'mse_loss': [0.06470231531925873], 'accuracy': [0.5958083832335329], 'precision': [0.5834869499529342], 'recall': [0.60031111570675], 'f1': [0.5819035079771774]}\n",
      "iteration: 2/10\n",
      "./datasets/WISDM/segments/method rnn_2d/wl 100/wo 75/dl 200/do 0/train [0, 7, 8, 9, 10, 11, 12, 13, 14]/valid [1, 2, 3, 4]/test [5, 6]/\n",
      "Dataset is already existed!\n",
      "Epoch 1/50\n",
      "237/237 [==============================] - 4s 15ms/step - loss: 1.1554 - accuracy: 0.6916 - val_loss: 1.6903 - val_accuracy: 0.5034\n",
      "Epoch 2/50\n",
      "237/237 [==============================] - 3s 15ms/step - loss: 0.5572 - accuracy: 0.7971 - val_loss: 1.6387 - val_accuracy: 0.5192\n",
      "Epoch 3/50\n",
      "237/237 [==============================] - 3s 15ms/step - loss: 0.4829 - accuracy: 0.8205 - val_loss: 1.6712 - val_accuracy: 0.5267\n",
      "Epoch 4/50\n",
      "237/237 [==============================] - 3s 15ms/step - loss: 0.4369 - accuracy: 0.8398 - val_loss: 1.6271 - val_accuracy: 0.5320\n",
      "Epoch 5/50\n",
      "237/237 [==============================] - 3s 15ms/step - loss: 0.3953 - accuracy: 0.8585 - val_loss: 1.6241 - val_accuracy: 0.5466\n",
      "Epoch 6/50\n",
      "237/237 [==============================] - 3s 15ms/step - loss: 0.3680 - accuracy: 0.8698 - val_loss: 1.7519 - val_accuracy: 0.5341\n",
      "Epoch 7/50\n",
      "237/237 [==============================] - 3s 15ms/step - loss: 0.3453 - accuracy: 0.8816 - val_loss: 1.8218 - val_accuracy: 0.5103\n",
      "Epoch 8/50\n",
      "237/237 [==============================] - 3s 15ms/step - loss: 0.3287 - accuracy: 0.8889 - val_loss: 1.6679 - val_accuracy: 0.5496\n",
      "Epoch 9/50\n",
      "237/237 [==============================] - 3s 15ms/step - loss: 0.3178 - accuracy: 0.8863 - val_loss: 1.7195 - val_accuracy: 0.5279\n",
      "Epoch 10/50\n",
      "237/237 [==============================] - 3s 15ms/step - loss: 0.3038 - accuracy: 0.8968 - val_loss: 1.8036 - val_accuracy: 0.5180\n",
      "Epoch 11/50\n",
      "237/237 [==============================] - 3s 15ms/step - loss: 0.2877 - accuracy: 0.9030 - val_loss: 1.7142 - val_accuracy: 0.5505\n",
      "Epoch 12/50\n",
      "237/237 [==============================] - 3s 15ms/step - loss: 0.2739 - accuracy: 0.9062 - val_loss: 1.8069 - val_accuracy: 0.5136\n",
      "Epoch 13/50\n",
      "237/237 [==============================] - 3s 15ms/step - loss: 0.2584 - accuracy: 0.9128 - val_loss: 1.7575 - val_accuracy: 0.5424\n",
      "Epoch 14/50\n",
      "237/237 [==============================] - 4s 16ms/step - loss: 0.2536 - accuracy: 0.9137 - val_loss: 1.7599 - val_accuracy: 0.5210\n",
      "Epoch 15/50\n",
      "237/237 [==============================] - 4s 15ms/step - loss: 0.2423 - accuracy: 0.9191 - val_loss: 1.8299 - val_accuracy: 0.5293\n",
      "Epoch 16/50\n",
      "237/237 [==============================] - 3s 15ms/step - loss: 0.2331 - accuracy: 0.9218 - val_loss: 1.8977 - val_accuracy: 0.5174\n",
      "Epoch 17/50\n",
      "237/237 [==============================] - 3s 15ms/step - loss: 0.2304 - accuracy: 0.9222 - val_loss: 1.9798 - val_accuracy: 0.4906\n",
      "Epoch 18/50\n",
      "237/237 [==============================] - 3s 15ms/step - loss: 0.2154 - accuracy: 0.9278 - val_loss: 1.8965 - val_accuracy: 0.5159\n",
      "Epoch 19/50\n",
      "237/237 [==============================] - 4s 15ms/step - loss: 0.2129 - accuracy: 0.9285 - val_loss: 1.9609 - val_accuracy: 0.5028\n",
      "Epoch 20/50\n",
      "237/237 [==============================] - 3s 15ms/step - loss: 0.2042 - accuracy: 0.9337 - val_loss: 1.9566 - val_accuracy: 0.5276\n",
      "Epoch 21/50\n",
      "237/237 [==============================] - 3s 15ms/step - loss: 0.1942 - accuracy: 0.9335 - val_loss: 2.0070 - val_accuracy: 0.5195\n",
      "Epoch 22/50\n",
      "237/237 [==============================] - 4s 15ms/step - loss: 0.1933 - accuracy: 0.9355 - val_loss: 2.0326 - val_accuracy: 0.5118\n",
      "Epoch 23/50\n",
      "237/237 [==============================] - 3s 14ms/step - loss: 0.1845 - accuracy: 0.9387 - val_loss: 2.0815 - val_accuracy: 0.5025\n",
      "Epoch 24/50\n",
      "237/237 [==============================] - 3s 14ms/step - loss: 0.1875 - accuracy: 0.9364 - val_loss: 2.0021 - val_accuracy: 0.5147\n",
      "Epoch 25/50\n",
      "237/237 [==============================] - 3s 14ms/step - loss: 0.1776 - accuracy: 0.9424 - val_loss: 2.0700 - val_accuracy: 0.4990\n",
      "Epoch 26/50\n",
      "237/237 [==============================] - 3s 14ms/step - loss: 0.1768 - accuracy: 0.9407 - val_loss: 2.0007 - val_accuracy: 0.5347\n",
      "Epoch 27/50\n",
      "237/237 [==============================] - 3s 14ms/step - loss: 0.1662 - accuracy: 0.9481 - val_loss: 2.0016 - val_accuracy: 0.5279\n",
      "Epoch 28/50\n",
      "237/237 [==============================] - 3s 14ms/step - loss: 0.1625 - accuracy: 0.9481 - val_loss: 2.1339 - val_accuracy: 0.5162\n",
      "Epoch 29/50\n",
      "237/237 [==============================] - 3s 14ms/step - loss: 0.1570 - accuracy: 0.9506 - val_loss: 2.2162 - val_accuracy: 0.5094\n",
      "Epoch 30/50\n",
      "237/237 [==============================] - 3s 14ms/step - loss: 0.1619 - accuracy: 0.9479 - val_loss: 2.1685 - val_accuracy: 0.4984\n",
      "Epoch 31/50\n",
      "237/237 [==============================] - 3s 14ms/step - loss: 0.1531 - accuracy: 0.9496 - val_loss: 2.1110 - val_accuracy: 0.5064\n",
      "Epoch 32/50\n",
      "237/237 [==============================] - 3s 14ms/step - loss: 0.1481 - accuracy: 0.9532 - val_loss: 2.1440 - val_accuracy: 0.5147\n",
      "Epoch 33/50\n",
      "237/237 [==============================] - 3s 14ms/step - loss: 0.1446 - accuracy: 0.9530 - val_loss: 2.1954 - val_accuracy: 0.5055\n",
      "Epoch 34/50\n",
      "237/237 [==============================] - 3s 14ms/step - loss: 0.1372 - accuracy: 0.9559 - val_loss: 2.2030 - val_accuracy: 0.5314\n",
      "Epoch 35/50\n",
      "237/237 [==============================] - 3s 14ms/step - loss: 0.1297 - accuracy: 0.9590 - val_loss: 2.2073 - val_accuracy: 0.5386\n",
      "Epoch 36/50\n",
      "237/237 [==============================] - 3s 14ms/step - loss: 0.1306 - accuracy: 0.9544 - val_loss: 2.2697 - val_accuracy: 0.5037\n",
      "Epoch 37/50\n",
      "237/237 [==============================] - 3s 14ms/step - loss: 0.1296 - accuracy: 0.9581 - val_loss: 2.3776 - val_accuracy: 0.4864\n",
      "Epoch 38/50\n",
      "237/237 [==============================] - 3s 14ms/step - loss: 0.1274 - accuracy: 0.9574 - val_loss: 2.3123 - val_accuracy: 0.5377\n",
      "Epoch 39/50\n",
      "237/237 [==============================] - 3s 14ms/step - loss: 0.1197 - accuracy: 0.9601 - val_loss: 2.2651 - val_accuracy: 0.5118\n",
      "Epoch 40/50\n",
      "237/237 [==============================] - 4s 15ms/step - loss: 0.1159 - accuracy: 0.9643 - val_loss: 2.4713 - val_accuracy: 0.5040\n",
      "Epoch 41/50\n",
      "237/237 [==============================] - 4s 16ms/step - loss: 0.1160 - accuracy: 0.9630 - val_loss: 2.4961 - val_accuracy: 0.4969\n",
      "Epoch 42/50\n",
      "237/237 [==============================] - 3s 14ms/step - loss: 0.1124 - accuracy: 0.9631 - val_loss: 2.3852 - val_accuracy: 0.5097\n",
      "Epoch 43/50\n",
      "237/237 [==============================] - 3s 14ms/step - loss: 0.1084 - accuracy: 0.9654 - val_loss: 2.4321 - val_accuracy: 0.4984\n",
      "Epoch 44/50\n",
      "237/237 [==============================] - 3s 14ms/step - loss: 0.1021 - accuracy: 0.9674 - val_loss: 2.5025 - val_accuracy: 0.4909\n",
      "Epoch 45/50\n",
      "237/237 [==============================] - 3s 14ms/step - loss: 0.1045 - accuracy: 0.9663 - val_loss: 2.5448 - val_accuracy: 0.5156\n",
      "Epoch 46/50\n",
      "237/237 [==============================] - 3s 14ms/step - loss: 0.0988 - accuracy: 0.9656 - val_loss: 2.3659 - val_accuracy: 0.5547\n",
      "Epoch 47/50\n",
      "237/237 [==============================] - 3s 14ms/step - loss: 0.0985 - accuracy: 0.9678 - val_loss: 2.4650 - val_accuracy: 0.5034\n",
      "Epoch 48/50\n",
      "237/237 [==============================] - 3s 14ms/step - loss: 0.0933 - accuracy: 0.9692 - val_loss: 2.3636 - val_accuracy: 0.5043\n",
      "Epoch 49/50\n",
      "237/237 [==============================] - 3s 14ms/step - loss: 0.0991 - accuracy: 0.9670 - val_loss: 2.5990 - val_accuracy: 0.4912\n",
      "Epoch 50/50\n",
      "237/237 [==============================] - 3s 14ms/step - loss: 0.0974 - accuracy: 0.9704 - val_loss: 2.4829 - val_accuracy: 0.5243\n",
      "53/53 [==============================] - 0s 3ms/step\n",
      "Test(<__main__.LSTM object at 0x168e23610>): 0.66\n",
      "{'mse_loss': [0.06470231531925873, 0.056243887930252354], 'accuracy': [0.5958083832335329, 0.6568862275449102], 'precision': [0.5834869499529342, 0.6744893980957647], 'recall': [0.60031111570675, 0.6590199791475445], 'f1': [0.5819035079771774, 0.6561383969387712]}\n",
      "iteration: 3/10\n",
      "./datasets/WISDM/segments/method rnn_2d/wl 100/wo 75/dl 200/do 0/train [0, 1, 8, 9, 10, 11, 12, 13, 14]/valid [2, 3, 4, 5]/test [6, 7]/\n",
      "class: data_1627_accel_phone, data size: 2:09:15, selected data size: 2:09:15\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class: data_1645_accel_phone, data size: 2:13:59, selected data size: 2:13:59\n",
      "class: data_1643_accel_phone, data size: 2:06:31, selected data size: 2:06:31\n",
      "class: data_1648_accel_phone, data size: 2:13:59, selected data size: 2:13:59\n",
      "class: data_1614_accel_phone, data size: 2:16:28, selected data size: 2:16:28\n",
      "class: data_1644_accel_phone, data size: 2:13:58, selected data size: 2:13:58\n",
      "class: data_1650_accel_phone, data size: 2:13:58, selected data size: 2:13:58\n",
      "class: data_1642_accel_phone, data size: 1:59:06, selected data size: 1:59:06\n",
      "class: data_1641_accel_phone, data size: 2:13:59, selected data size: 2:13:59\n",
      "class: data_1646_accel_phone, data size: 2:13:59, selected data size: 2:13:59\n",
      "segmenting data with 946872 points\n",
      "making 37841 segments\n",
      "segmenting data with 420832 points\n",
      "making 16797 segments\n",
      "segmenting data with 210416 points\n",
      "making 8383 segments\n",
      "Epoch 1/50\n",
      "237/237 [==============================] - 3s 15ms/step - loss: 1.3701 - accuracy: 0.6715 - val_loss: 1.6540 - val_accuracy: 0.6092\n",
      "Epoch 2/50\n",
      "237/237 [==============================] - 3s 14ms/step - loss: 0.5287 - accuracy: 0.8253 - val_loss: 1.4813 - val_accuracy: 0.6473\n",
      "Epoch 3/50\n",
      "237/237 [==============================] - 3s 14ms/step - loss: 0.4363 - accuracy: 0.8515 - val_loss: 1.5271 - val_accuracy: 0.6229\n",
      "Epoch 4/50\n",
      "237/237 [==============================] - 3s 14ms/step - loss: 0.3822 - accuracy: 0.8730 - val_loss: 1.5558 - val_accuracy: 0.6008\n",
      "Epoch 5/50\n",
      "237/237 [==============================] - 3s 14ms/step - loss: 0.3404 - accuracy: 0.8903 - val_loss: 1.5019 - val_accuracy: 0.6077\n",
      "Epoch 6/50\n",
      "237/237 [==============================] - 3s 14ms/step - loss: 0.3071 - accuracy: 0.9030 - val_loss: 1.5564 - val_accuracy: 0.6229\n",
      "Epoch 7/50\n",
      "237/237 [==============================] - 3s 14ms/step - loss: 0.2878 - accuracy: 0.9055 - val_loss: 1.6063 - val_accuracy: 0.6199\n",
      "Epoch 8/50\n",
      "237/237 [==============================] - 3s 14ms/step - loss: 0.2677 - accuracy: 0.9140 - val_loss: 1.5423 - val_accuracy: 0.6139\n",
      "Epoch 9/50\n",
      "237/237 [==============================] - 3s 14ms/step - loss: 0.2495 - accuracy: 0.9194 - val_loss: 1.6011 - val_accuracy: 0.6011\n",
      "Epoch 10/50\n",
      "237/237 [==============================] - 3s 14ms/step - loss: 0.2375 - accuracy: 0.9232 - val_loss: 1.7402 - val_accuracy: 0.5707\n",
      "Epoch 11/50\n",
      "237/237 [==============================] - 3s 14ms/step - loss: 0.2281 - accuracy: 0.9263 - val_loss: 1.6236 - val_accuracy: 0.5952\n",
      "Epoch 12/50\n",
      "237/237 [==============================] - 3s 14ms/step - loss: 0.2202 - accuracy: 0.9296 - val_loss: 1.6738 - val_accuracy: 0.5952\n",
      "Epoch 13/50\n",
      "237/237 [==============================] - 3s 14ms/step - loss: 0.2065 - accuracy: 0.9346 - val_loss: 1.6509 - val_accuracy: 0.5979\n",
      "Epoch 14/50\n",
      "237/237 [==============================] - 3s 14ms/step - loss: 0.1990 - accuracy: 0.9387 - val_loss: 1.7686 - val_accuracy: 0.5710\n",
      "Epoch 15/50\n",
      "237/237 [==============================] - 3s 14ms/step - loss: 0.1922 - accuracy: 0.9388 - val_loss: 1.6467 - val_accuracy: 0.6053\n",
      "Epoch 16/50\n",
      "237/237 [==============================] - 3s 14ms/step - loss: 0.1854 - accuracy: 0.9413 - val_loss: 1.7114 - val_accuracy: 0.6005\n",
      "Epoch 17/50\n",
      "237/237 [==============================] - 3s 14ms/step - loss: 0.1792 - accuracy: 0.9436 - val_loss: 1.7218 - val_accuracy: 0.6154\n",
      "Epoch 18/50\n",
      "237/237 [==============================] - 3s 14ms/step - loss: 0.1751 - accuracy: 0.9438 - val_loss: 1.6621 - val_accuracy: 0.5990\n",
      "Epoch 19/50\n",
      "237/237 [==============================] - 3s 14ms/step - loss: 0.1673 - accuracy: 0.9467 - val_loss: 1.6747 - val_accuracy: 0.6062\n",
      "Epoch 20/50\n",
      "237/237 [==============================] - 3s 14ms/step - loss: 0.1615 - accuracy: 0.9473 - val_loss: 1.7821 - val_accuracy: 0.5871\n",
      "Epoch 21/50\n",
      "237/237 [==============================] - 3s 14ms/step - loss: 0.1581 - accuracy: 0.9507 - val_loss: 1.7616 - val_accuracy: 0.6157\n",
      "Epoch 22/50\n",
      "237/237 [==============================] - 3s 14ms/step - loss: 0.1489 - accuracy: 0.9543 - val_loss: 1.7554 - val_accuracy: 0.6059\n",
      "Epoch 23/50\n",
      "237/237 [==============================] - 3s 14ms/step - loss: 0.1444 - accuracy: 0.9553 - val_loss: 1.7355 - val_accuracy: 0.6190\n",
      "Epoch 24/50\n",
      "237/237 [==============================] - 3s 14ms/step - loss: 0.1400 - accuracy: 0.9564 - val_loss: 1.7556 - val_accuracy: 0.5976\n",
      "Epoch 25/50\n",
      "237/237 [==============================] - 3s 14ms/step - loss: 0.1381 - accuracy: 0.9555 - val_loss: 1.6827 - val_accuracy: 0.6312\n",
      "Epoch 26/50\n",
      "237/237 [==============================] - 3s 14ms/step - loss: 0.1387 - accuracy: 0.9540 - val_loss: 1.7447 - val_accuracy: 0.6116\n",
      "Epoch 27/50\n",
      "237/237 [==============================] - 3s 14ms/step - loss: 0.1300 - accuracy: 0.9592 - val_loss: 1.7921 - val_accuracy: 0.6056\n",
      "Epoch 28/50\n",
      "237/237 [==============================] - 3s 14ms/step - loss: 0.1294 - accuracy: 0.9588 - val_loss: 1.8475 - val_accuracy: 0.5996\n",
      "Epoch 29/50\n",
      "237/237 [==============================] - 3s 14ms/step - loss: 0.1222 - accuracy: 0.9621 - val_loss: 1.8197 - val_accuracy: 0.5985\n",
      "Epoch 30/50\n",
      "237/237 [==============================] - 3s 14ms/step - loss: 0.1191 - accuracy: 0.9630 - val_loss: 1.8269 - val_accuracy: 0.5821\n",
      "Epoch 31/50\n",
      "237/237 [==============================] - 3s 14ms/step - loss: 0.1165 - accuracy: 0.9629 - val_loss: 1.7297 - val_accuracy: 0.6279\n",
      "Epoch 32/50\n",
      "237/237 [==============================] - 3s 14ms/step - loss: 0.1105 - accuracy: 0.9660 - val_loss: 1.8450 - val_accuracy: 0.6050\n",
      "Epoch 33/50\n",
      "237/237 [==============================] - 3s 14ms/step - loss: 0.1125 - accuracy: 0.9629 - val_loss: 1.7589 - val_accuracy: 0.6211\n",
      "Epoch 34/50\n",
      "237/237 [==============================] - 3s 14ms/step - loss: 0.1057 - accuracy: 0.9655 - val_loss: 1.8300 - val_accuracy: 0.6157\n",
      "Epoch 35/50\n",
      "237/237 [==============================] - 3s 14ms/step - loss: 0.1009 - accuracy: 0.9676 - val_loss: 1.8720 - val_accuracy: 0.5985\n",
      "Epoch 36/50\n",
      "237/237 [==============================] - 3s 14ms/step - loss: 0.1004 - accuracy: 0.9689 - val_loss: 1.9061 - val_accuracy: 0.6291\n",
      "Epoch 37/50\n",
      "237/237 [==============================] - 3s 14ms/step - loss: 0.0973 - accuracy: 0.9687 - val_loss: 1.9274 - val_accuracy: 0.5937\n",
      "Epoch 38/50\n",
      "237/237 [==============================] - 3s 14ms/step - loss: 0.0944 - accuracy: 0.9703 - val_loss: 1.8981 - val_accuracy: 0.5952\n",
      "Epoch 39/50\n",
      "237/237 [==============================] - 4s 15ms/step - loss: 0.0965 - accuracy: 0.9701 - val_loss: 1.8896 - val_accuracy: 0.6059\n",
      "Epoch 40/50\n",
      "237/237 [==============================] - 3s 14ms/step - loss: 0.0898 - accuracy: 0.9720 - val_loss: 1.9800 - val_accuracy: 0.6250\n",
      "Epoch 41/50\n",
      "237/237 [==============================] - 3s 14ms/step - loss: 0.0901 - accuracy: 0.9734 - val_loss: 1.9169 - val_accuracy: 0.6229\n",
      "Epoch 42/50\n",
      "237/237 [==============================] - 3s 14ms/step - loss: 0.0888 - accuracy: 0.9730 - val_loss: 1.9480 - val_accuracy: 0.6160\n",
      "Epoch 43/50\n",
      "237/237 [==============================] - 3s 14ms/step - loss: 0.0818 - accuracy: 0.9757 - val_loss: 1.8804 - val_accuracy: 0.6122\n",
      "Epoch 44/50\n",
      "237/237 [==============================] - 3s 14ms/step - loss: 0.0804 - accuracy: 0.9753 - val_loss: 1.9842 - val_accuracy: 0.5973\n",
      "Epoch 45/50\n",
      "237/237 [==============================] - 3s 14ms/step - loss: 0.0805 - accuracy: 0.9770 - val_loss: 1.9958 - val_accuracy: 0.5949\n",
      "Epoch 46/50\n",
      "237/237 [==============================] - 3s 14ms/step - loss: 0.0884 - accuracy: 0.9734 - val_loss: 1.9034 - val_accuracy: 0.6157\n",
      "Epoch 47/50\n",
      "237/237 [==============================] - 3s 15ms/step - loss: 0.0740 - accuracy: 0.9778 - val_loss: 1.8726 - val_accuracy: 0.6065\n",
      "Epoch 48/50\n",
      "237/237 [==============================] - 3s 14ms/step - loss: 0.0729 - accuracy: 0.9789 - val_loss: 1.9984 - val_accuracy: 0.6119\n",
      "Epoch 49/50\n",
      "237/237 [==============================] - 3s 14ms/step - loss: 0.0755 - accuracy: 0.9771 - val_loss: 2.0789 - val_accuracy: 0.6038\n",
      "Epoch 50/50\n",
      "237/237 [==============================] - 3s 14ms/step - loss: 0.0724 - accuracy: 0.9782 - val_loss: 2.0460 - val_accuracy: 0.6211\n",
      "53/53 [==============================] - 0s 3ms/step\n",
      "Test(<__main__.LSTM object at 0x168e23610>): 0.66\n",
      "{'mse_loss': [0.06470231531925873, 0.056243887930252354, 0.05487118811614781], 'accuracy': [0.5958083832335329, 0.6568862275449102, 0.6550898203592814], 'precision': [0.5834869499529342, 0.6744893980957647, 0.6488098431275562], 'recall': [0.60031111570675, 0.6590199791475445, 0.6543477206395574], 'f1': [0.5819035079771774, 0.6561383969387712, 0.6264648196209691]}\n",
      "iteration: 4/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./datasets/WISDM/segments/method rnn_2d/wl 100/wo 75/dl 200/do 0/train [0, 1, 2, 9, 10, 11, 12, 13, 14]/valid [3, 4, 5, 6]/test [7, 8]/\n",
      "class: data_1646_accel_phone, data size: 2:13:59, selected data size: 2:13:59\n",
      "class: data_1644_accel_phone, data size: 2:13:58, selected data size: 2:13:58\n",
      "class: data_1645_accel_phone, data size: 2:13:59, selected data size: 2:13:59\n",
      "class: data_1643_accel_phone, data size: 2:06:31, selected data size: 2:06:31\n",
      "class: data_1642_accel_phone, data size: 1:59:06, selected data size: 1:59:06\n",
      "class: data_1614_accel_phone, data size: 2:16:28, selected data size: 2:16:28\n",
      "class: data_1627_accel_phone, data size: 2:09:15, selected data size: 2:09:15\n",
      "class: data_1641_accel_phone, data size: 2:13:59, selected data size: 2:13:59\n",
      "class: data_1648_accel_phone, data size: 2:13:59, selected data size: 2:13:59\n",
      "class: data_1650_accel_phone, data size: 2:13:58, selected data size: 2:13:58\n",
      "segmenting data with 946872 points\n",
      "making 37841 segments\n",
      "segmenting data with 420832 points\n",
      "making 16797 segments\n",
      "segmenting data with 210416 points\n",
      "making 8383 segments\n",
      "Epoch 1/50\n",
      "237/237 [==============================] - 3s 15ms/step - loss: 1.8249 - accuracy: 0.5746 - val_loss: 1.5754 - val_accuracy: 0.4900\n",
      "Epoch 2/50\n",
      "237/237 [==============================] - 3s 14ms/step - loss: 0.7709 - accuracy: 0.7541 - val_loss: 1.2617 - val_accuracy: 0.5615\n",
      "Epoch 3/50\n",
      "237/237 [==============================] - 3s 14ms/step - loss: 0.5994 - accuracy: 0.8081 - val_loss: 1.2657 - val_accuracy: 0.5502\n",
      "Epoch 4/50\n",
      "237/237 [==============================] - 3s 15ms/step - loss: 0.5172 - accuracy: 0.8293 - val_loss: 1.1998 - val_accuracy: 0.5707\n",
      "Epoch 5/50\n",
      "237/237 [==============================] - 3s 14ms/step - loss: 0.4541 - accuracy: 0.8513 - val_loss: 1.3300 - val_accuracy: 0.5308\n",
      "Epoch 6/50\n",
      "237/237 [==============================] - 3s 14ms/step - loss: 0.4160 - accuracy: 0.8665 - val_loss: 1.3663 - val_accuracy: 0.5538\n",
      "Epoch 7/50\n",
      " 96/237 [===========>..................] - ETA: 1s - loss: 0.3800 - accuracy: 0.8851"
     ]
    }
   ],
   "source": [
    "models = ['GRU','LSTM','ConvLSTM','BiLSTM']\n",
    "# 'GRU','LSTM','ConvLSTM','BiLSTM'\n",
    "# 'Power_consumption','PRSA2017','RSSI','User_Identification_From_Walking','WISDM','Motor_Failure_Time'\n",
    "for problem in ['WISDM','Motor_Failure_Time']:\n",
    "    dataset = problems[problem]['dataset']\n",
    "    n_classes = problems[problem]['n_classes']\n",
    "    features = problems[problem]['features']\n",
    "    sample_rate = problems[problem]['sample_rate']\n",
    "    data_length_time = problems[problem]['data_length_time']\n",
    "    n_h_block = problems[problem]['n_h_block']\n",
    "    n_train_h_block = problems[problem]['n_train_h_block']\n",
    "    n_valid_h_block = problems[problem]['n_valid_h_block']\n",
    "    n_test_h_block = problems[problem]['n_test_h_block']\n",
    "    h_moving_step = problems[problem]['h_moving_step']\n",
    "    decision_times = problems[problem]['decision_times']\n",
    "    segments_times = problems[problem]['segments_times']\n",
    "    \n",
    "#     log_dir = f\"./comparisons/log/{problem}/recurrent/\"\n",
    "    log_dir = f\"./benchmark/\"\n",
    "    \n",
    "    for model in models:\n",
    "        for decision_time in decision_times:\n",
    "            for decision_overlap in [0.0]:\n",
    "                for segments_time in segments_times:\n",
    "                    for segments_overlap in [0.75]:\n",
    "                        if float(decision_time*0.75) < float(segments_time):\n",
    "                            continue\n",
    "                        \n",
    "                        if (os.path.exists(log_dir + \"statistics.csv\")):\n",
    "                            is_investigated = False\n",
    "                            saved_statistics = pd.read_csv(log_dir + \"statistics.csv\")\n",
    "                            for index, row in saved_statistics.iterrows():\n",
    "                                if row['dataset'] == str(dataset) and row['inner_classifier'] == str(model):\n",
    "                                    t1 = datetime.strptime(row['segments_time'],\"%H:%M:%S\")\n",
    "                                    t2 = datetime.strptime(row['decision_time'],\"%H:%M:%S\")\n",
    "                                    td1 = timedelta(hours=t1.hour, minutes=t1.minute, seconds=t1.second)\n",
    "                                    td2 = timedelta(hours=t2.hour, minutes=t2.minute, seconds=t2.second)\n",
    "                                    if (td1.total_seconds() == int(segments_time) \n",
    "                                        and td2.total_seconds() == int(decision_time)):\n",
    "                                        print(\"dataset: \" + problem + \" \" +\n",
    "                                              \"model: \" + model + \" \" +\n",
    "                                            \"dl: \" + row['decision_time'] + \" and wl:\" + row[\n",
    "                                            'segments_time'] + \" is investigated!\")\n",
    "                                        is_investigated = True\n",
    "                                        break\n",
    "                            if (is_investigated):\n",
    "                                continue\n",
    "\n",
    "                        print(\"dl: \" + str(timedelta(seconds=int(decision_time))) +\n",
    "                              \" and \" +\n",
    "                              \"wl: \" + str(timedelta(seconds=int(segments_time))))\n",
    "                        \n",
    "                        classifier = eval(model)(classes=n_classes,\n",
    "                                                 n_features=len(features),\n",
    "                                                 segments_size=int(segments_time * sample_rate),\n",
    "                                                 segments_overlap=segments_overlap,\n",
    "                                                 decision_size=int(decision_time * sample_rate),\n",
    "                                                 decision_overlap=decision_overlap)\n",
    "    \n",
    "                        # cross-validation\n",
    "                        start = datetime.now()\n",
    "                        statistics = h_block_analyzer(db_path=dataset,\n",
    "                                                      sample_rate=sample_rate,\n",
    "                                                      features=features,\n",
    "                                                      n_classes=n_classes,\n",
    "                                                      noise_rate=noise_rate,\n",
    "                                                      segments_time=segments_time,\n",
    "                                                      segments_overlap=segments_overlap,\n",
    "                                                      decision_time=decision_time,\n",
    "                                                      decision_overlap=decision_overlap,\n",
    "                                                      classifier=classifier,\n",
    "                                                      epochs=epochs,\n",
    "                                                      batch_size=batch_size,\n",
    "                                                      data_length_time=data_length_time,\n",
    "                                                      n_h_block=n_h_block,\n",
    "                                                      n_train_h_block=n_train_h_block,\n",
    "                                                      n_valid_h_block=n_valid_h_block,\n",
    "                                                      n_test_h_block=n_test_h_block,\n",
    "                                                      h_moving_step=h_moving_step)\n",
    "                        end = datetime.now()\n",
    "                        running_time = end - start\n",
    "    \n",
    "                        # Summarizing the results of cross-validation\n",
    "                        data = {}\n",
    "                        data['dataset'] = dataset\n",
    "                        data['class'] = str(n_classes)\n",
    "                        data['features'] = str(features)\n",
    "                        data['sample_rate'] = str(sample_rate)\n",
    "                        data['noise_rate'] = str(noise_rate)\n",
    "                        data['epochs'] = str(epochs)\n",
    "                        data['batch_size'] = str(batch_size)\n",
    "                        data['data_length_time'] = str(data_length_time)\n",
    "                        data['n_h_block'] = str(n_h_block)\n",
    "                        data['n_train_h_block'] = str(n_train_h_block)\n",
    "                        data['n_valid_h_block'] = str(n_valid_h_block)\n",
    "                        data['n_test_h_block'] = str(n_test_h_block)\n",
    "                        data['h_moving_step'] = str(h_moving_step)\n",
    "                        data['segments_time'] = str(segments_time)\n",
    "                        data['segments_overlap'] = str(segments_overlap)\n",
    "                        data['inner_classifier'] = str(model)\n",
    "                        data['datetime'] = datetime.now().strftime(\"%Y:%m:%d %H:%M:%S\")\n",
    "                        data['running_time'] = str(running_time.seconds) + \" seconds\"\n",
    "                        data['n_params'] = classifier.count_params()\n",
    "                        data['segments_time'] = timedelta(seconds=int(segments_time))\n",
    "                        data['segments_overlap'] = segments_overlap\n",
    "                        data['decision_time'] = timedelta(seconds=int(decision_time))\n",
    "                        data['decision_overlap'] = decision_overlap\n",
    "                        statistics_summary = {}\n",
    "                        for key in statistics.keys():\n",
    "                            statistics_summary[key + '_mean'] = np.average(statistics[key])\n",
    "                            statistics_summary[key + '_std'] = np.std(statistics[key])\n",
    "                            statistics_summary[key + '_max'] = np.max(statistics[key])\n",
    "                            statistics_summary[key + '_min'] = np.min(statistics[key])\n",
    "                        data.update(statistics_summary)\n",
    "                        # Save information\n",
    "                        save_result(log_dir=log_dir, data=data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"End\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # FOR TESTING TENSORFLOW:\n",
    "# import tensorflow as tf\n",
    "# from tensorflow.keras.optimizers.legacy import Adam\n",
    "\n",
    "# cifar = tf.keras.datasets.cifar10\n",
    "# (x_train, y_train), (x_test, y_test) = cifar.load_data()\n",
    "# model = tf.keras.applications.ResNet50(\n",
    "#     include_top=True,\n",
    "#     weights=None,\n",
    "#     input_shape=(32, 32, 3),\n",
    "#     classes=10,)\n",
    "\n",
    "# optimizer = tf.keras.optimizers.legacy.Adam(0.001, 0.5)\n",
    "# loss_fn = tf.keras.losses.Crossentropy()\n",
    "# # loss_fn = tf.keras.losses.SparseCategoricalCrossentropy()\n",
    "# model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=[\"accuracy\"])\n",
    "# model.fit(x_train, y_train, epochs=5, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "ZIm0OmusVgXS",
    "-Ng6rf-7xygn",
    "VbUrAsY1enTC",
    "_w5A0xO9Upk1",
    "2bB8dITyHh5H",
    "caa9SgQ8HmsI",
    "lS6yHM-MHosi",
    "yqRhRm3V3SjM"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "env_tf",
   "language": "python",
   "name": "env_tf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
